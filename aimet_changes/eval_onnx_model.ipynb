{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import torch\n",
    "import json\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from mmcv.transforms import Compose\n",
    "from mmdet.utils import get_test_pipeline_cfg\n",
    "\n",
    "def read_json(json_path):\n",
    "    with open(json_path) as f:\n",
    "        data = json.load(f)\n",
    "    return data\n",
    "\n",
    "def preprocess(test_pipeline, image):\n",
    "    if isinstance(image, np.ndarray):\n",
    "        # Calling this method across libraries will result\n",
    "        # in module unregistered error if not prefixed with mmdet.\n",
    "        test_pipeline[0].type = 'mmdet.LoadImageFromNDArray'\n",
    "    test_pipeline = Compose(test_pipeline)\n",
    "    return test_pipeline(dict(img=image))\n",
    "\n",
    "class CustomImageDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, images_dir, annotations_json_path, transform=None):\n",
    "        self.transform = transform\n",
    "        self.images_dir = images_dir\n",
    "        self.annotations_json = read_json(annotations_json_path)\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.annotations_json['images'])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_dict = self.annotations_json['images'][idx]\n",
    "        image_path = os.path.join(self.images_dir, image_dict['file_name'])\n",
    "        image_id = image_dict['id']\n",
    "\n",
    "        image = cv2.imread(image_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        if self.transform:\n",
    "            transformed_images = self.transform(image)\n",
    "        else:\n",
    "            transformed_images = image\n",
    "\n",
    "        return image_id, image_path, transformed_images\n",
    "\n",
    "\n",
    "# calibrationDataloader = DataLoader(calibrationDataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loads checkpoint by local backend from path: /teamspace/studios/this_studio/mmdetection/rtmdet_tiny_8xb32-300e_coco_20220902_112414-78e30dcc.pth\n",
      "The model and loaded state dict do not match exactly\n",
      "\n",
      "unexpected key in source state_dict: data_preprocessor.mean, data_preprocessor.std\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/mmengine/visualization/visualizer.py:196: UserWarning: Failed to add <class 'mmengine.visualization.vis_backend.LocalVisBackend'>, please provide the `save_dir` argument.\n",
      "  warnings.warn(f'Failed to add {vis_backend.__class__}, '\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from mmdet.apis import DetInferencer\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Resize([640, 640]),  # Resize\n",
    "])\n",
    "\n",
    "DEVICE = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "CONFIG_PATH = '/teamspace/studios/this_studio/mmdetection/rtmdet_tiny_8xb32-300e_coco.py'\n",
    "WEIGHTS_PATH = '/teamspace/studios/this_studio/mmdetection/rtmdet_tiny_8xb32-300e_coco_20220902_112414-78e30dcc.pth'\n",
    "EVAL_DATASET_SIZE = 5000\n",
    "CALIBRATION_DATASET_SIZE = 1000\n",
    "BATCH_SIZE = 1\n",
    "\n",
    "ROOT_DATASET_DIR = '/teamspace/studios/this_studio/COCO'\n",
    "IMAGES_DIR = os.path.join(ROOT_DATASET_DIR, 'images')\n",
    "ANNOTATIONS_JSON_PATH = os.path.join(ROOT_DATASET_DIR, 'annotations/instances_val2017.json')\n",
    "# ANNOTATIONS_JSON_PATH = \"/home/shayaan/Desktop/aimet/my_mmdet/temp.json\"\n",
    "\n",
    "model = DetInferencer(model=CONFIG_PATH, weights=WEIGHTS_PATH, device=DEVICE)\n",
    "evalDataset = CustomImageDataset(images_dir=IMAGES_DIR, annotations_json_path=ANNOTATIONS_JSON_PATH, transform=transform)\n",
    "eval_data_loader = DataLoader(evalDataset, batch_size=BATCH_SIZE)\n",
    "\n",
    "DEVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.32s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "import torch\n",
    "\n",
    "from mmdet.models.utils import samplelist_boxtype2tensor\n",
    "from mmengine.registry import MODELS\n",
    "from mmcv.transforms import Compose\n",
    "\n",
    "test_evaluator = model.cfg.test_evaluator\n",
    "test_evaluator.type = 'mmdet.evaluation.CocoMetric' \n",
    "test_evaluator.dataset_meta = model.model.dataset_meta\n",
    "test_evaluator.ann_file = ANNOTATIONS_JSON_PATH\n",
    "test_evaluator = Compose(test_evaluator)\n",
    "\n",
    "collate_preprocessor = model.preprocess\n",
    "predict_by_feat = model.model.bbox_head.predict_by_feat\n",
    "rescale = True\n",
    "\n",
    "preprocessor = MODELS.build(model.cfg.model.data_preprocessor)\n",
    "def add_pred_to_datasample(data_samples, results_list):\n",
    "    for data_sample, pred_instances in zip(data_samples, results_list):\n",
    "        data_sample.pred_instances = pred_instances\n",
    "    samplelist_boxtype2tensor(data_samples)\n",
    "    return data_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pass_calibration_data(model: torch.nn.Module, samples: int):\n",
    "    data_loader = eval_data_loader\n",
    "    batch_size = data_loader.batch_size\n",
    "    model.eval()\n",
    "    batch_ctr = 0\n",
    "    with torch.no_grad():\n",
    "        for image_id, image_path, _ in tqdm(data_loader):\n",
    "            pre_processed = collate_preprocessor(inputs=image_path, batch_size=batch_size)\n",
    "            _, data = list(pre_processed)[0]\n",
    "            data = preprocessor(data, False)\n",
    "            \n",
    "            preds = model(data['inputs'].to(DEVICE))\n",
    "\n",
    "            batch_ctr += 1\n",
    "            if (batch_ctr * batch_size) > samples:\n",
    "                break  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_PATH = \"/teamspace/studios/this_studio/aimet/exported_models_0.01_2/quant_scheme_W@tf-enhanced _ A@tf_encodings\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnxruntime as ort\n",
    "import numpy as np\n",
    "\n",
    "def onnx_infer(session, data):    \n",
    "    inp_names = [x.name for x in session.get_inputs()]\n",
    "    out_names = [x.name for x in session.get_outputs()]\n",
    "    if type(data) == np.ndarray:\n",
    "        data = [data]\n",
    "    assert len(inp_names) == len(data)\n",
    "\n",
    "    input_dict = dict(zip(inp_names, data))\n",
    "    result = session.run(out_names, input_dict)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 80, 80, 80)\n",
      "(1, 80, 40, 40)\n",
      "(1, 80, 20, 20)\n",
      "(1, 4, 80, 80)\n",
      "(1, 4, 40, 40)\n",
      "(1, 4, 20, 20)\n"
     ]
    }
   ],
   "source": [
    "import os.path as osp\n",
    "model_path = osp.join(BASE_PATH.replace(\"encodings\", \"embedded\"), \"rtm_det_embedded.onnx\")\n",
    "\n",
    "session = ort.InferenceSession(model_path, providers=['CPUExecutionProvider'])\n",
    "results = onnx_infer(session, dummy_input.detach().cpu().numpy())\n",
    "\n",
    "for r in results:\n",
    "    print(r.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "from mmengine.structures import InstanceData\n",
    "import onnxruntime as ort\n",
    "\n",
    "def eval_callback(model_path, save_dir=\"./temp\"):\n",
    "    data_loader = eval_data_loader\n",
    "    new_preds = []\n",
    "    session = ort.InferenceSession(model_path, providers=['CPUExecutionProvider'])\n",
    "    for image_id, image_path, _ in tqdm(data_loader):\n",
    "        pre_processed = collate_preprocessor(inputs=image_path, batch_size=BATCH_SIZE)\n",
    "        _, data = list(pre_processed)[0]\n",
    "        data = preprocessor(data, False)\n",
    "\n",
    "        preds = onnx_infer(session, data['inputs'].detach().cpu().numpy())\n",
    "        preds = tuple([tuple([torch.from_numpy(p) for p in preds[:3]]), tuple([torch.from_numpy(p) for p in preds[3:]])])\n",
    "\n",
    "        batch_img_metas = [\n",
    "        data_samples.metainfo for data_samples in data['data_samples']\n",
    "        ]\n",
    "        preds = predict_by_feat(*preds, batch_img_metas=batch_img_metas, rescale=True)\n",
    "        preds = add_pred_to_datasample(data['data_samples'], preds)\n",
    "        \n",
    "        for img_id, pred in zip(image_id, preds):\n",
    "            pred = pred.pred_instances\n",
    "            new_pred = InstanceData(metainfo={\"img_id\": int(img_id)})\n",
    "            new_pred.bboxes = [np.array(p) for p in pred['bboxes'].cpu()]\n",
    "            new_pred.labels = pred['labels'].cpu()\n",
    "            new_pred.scores = pred['scores'].cpu()\n",
    "            new_preds.append(new_pred)\n",
    "\n",
    "    eval_results = test_evaluator(new_preds)\n",
    "    \n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    num_file = len(glob(f\"{save_dir}/onnx_acc_*\"))\n",
    "    print(\"Saving in this path: \", f\"{save_dir}/onnx_acc_{num_file}.json\")\n",
    "    with open(f\"{save_dir}/onnx_acc_{num_file}.json\", \"w\") as f:\n",
    "        json.dump(eval_results, f, indent=4)\n",
    "    bbox_map = eval_results['bbox_mAP']\n",
    "    return bbox_map\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('/teamspace/studios/this_studio/aimet/exported_models_0.01_2/quant_scheme_W@tf-enhanced _ A@tf_embedded',\n",
       " '/teamspace/studios/this_studio/aimet/exported_models_0.01_2/quant_scheme_W@tf-enhanced _ A@tf_embedded/rtm_det_embedded.onnx')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_dir = BASE_PATH.replace(\"encodings\", \"embedded\")\n",
    "model_path = osp.join(BASE_PATH.replace(\"encodings\", \"embedded\"), \"rtm_det_embedded.onnx\")\n",
    "\n",
    "save_dir, model_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "448eb1c38749450cb30e27f50cf87018",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "08/23 10:54:34 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Evaluating bbox...\n",
      "Loading and preparing results...\n",
      "DONE (t=3.74s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=76.32s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=22.68s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.290\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.431\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.311\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.138\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.328\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.438\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.275\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.467\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.519\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.287\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.575\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.714\n",
      "Saving in this path:  /teamspace/studios/this_studio/aimet/exported_models_0.01_2/quant_scheme_W@tf-enhanced _ A@tf_embedded/onnx_acc_0.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.29"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_callback(model_path, save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
