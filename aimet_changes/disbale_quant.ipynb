{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "import json\n",
    "from mmcv.transforms import Compose\n",
    "import numpy as np\n",
    "from mmdet.utils import get_test_pipeline_cfg\n",
    "\n",
    "def read_json(json_path):\n",
    "    with open(json_path) as f:\n",
    "        data = json.load(f)\n",
    "    return data\n",
    "\n",
    "def preprocess(test_pipeline, image):\n",
    "    if isinstance(image, np.ndarray):\n",
    "        # Calling this method across libraries will result\n",
    "        # in module unregistered error if not prefixed with mmdet.\n",
    "        test_pipeline[0].type = 'mmdet.LoadImageFromNDArray'\n",
    "    test_pipeline = Compose(test_pipeline)\n",
    "    return test_pipeline(dict(img=image))\n",
    "\n",
    "class CustomImageDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, images_dir, annotations_json_path, transform=None):\n",
    "        self.transform = transform\n",
    "        self.images_dir = images_dir\n",
    "        self.annotations_json = read_json(annotations_json_path)\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.annotations_json['images'])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_dict = self.annotations_json['images'][idx]\n",
    "        image_path = os.path.join(self.images_dir, image_dict['file_name'])\n",
    "        image_id = image_dict['id']\n",
    "\n",
    "        image = cv2.imread(image_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        if self.transform:\n",
    "            transformed_images = self.transform(image)\n",
    "        else:\n",
    "            transformed_images = image\n",
    "\n",
    "        return image_id, image_path, transformed_images\n",
    "\n",
    "\n",
    "# calibrationDataloader = DataLoader(calibrationDataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-08-12 07:51:57,307] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "df: /root/.triton/autotune: No such file or directory\n",
      "/bin/ld: cannot find -laio: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93m [WARNING] \u001b[0m async_io requires the dev libaio .so object and headers but these were not found.\n",
      "\u001b[93m [WARNING] \u001b[0m async_io: please install the libaio-dev package with apt\n",
      "\u001b[93m [WARNING] \u001b[0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.\n",
      "\u001b[93m [WARNING] \u001b[0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH\n",
      "\u001b[93m [WARNING] \u001b[0m NVIDIA Inference is only supported on Ampere and newer architectures\n",
      "\u001b[93m [WARNING] \u001b[0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.2\n",
      "\u001b[93m [WARNING] \u001b[0m using untested triton version (2.2.0), only 1.0.0 is known to be compatible\n",
      "Loads checkpoint by local backend from path: /teamspace/studios/this_studio/mmdetection/rtmdet_tiny_8xb32-300e_coco_20220902_112414-78e30dcc.pth\n",
      "The model and loaded state dict do not match exactly\n",
      "\n",
      "unexpected key in source state_dict: data_preprocessor.mean, data_preprocessor.std\n",
      "\n",
      "08/12 07:52:01 - mmengine - \u001b[5m\u001b[4m\u001b[33mWARNING\u001b[0m - Failed to search registry with scope \"mmdet\" in the \"function\" registry tree. As a workaround, the current \"function\" registry in \"mmengine\" is used to build instance. This may cause unexpected failure when running the built modules. Please check whether \"mmdet\" is a correct scope, or whether the registry is initialized.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/mmengine/visualization/visualizer.py:196: UserWarning: Failed to add <class 'mmengine.visualization.vis_backend.LocalVisBackend'>, please provide the `save_dir` argument.\n",
      "  warnings.warn(f'Failed to add {vis_backend.__class__}, '\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from mmdet.apis import DetInferencer\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Resize([640, 640]),  # Resize\n",
    "])\n",
    "\n",
    "DEVICE = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "CONFIG_PATH = '/teamspace/studios/this_studio/mmdetection/rtmdet_tiny_8xb32-300e_coco.py'\n",
    "WEIGHTS_PATH = '/teamspace/studios/this_studio/mmdetection/rtmdet_tiny_8xb32-300e_coco_20220902_112414-78e30dcc.pth'\n",
    "EVAL_DATASET_SIZE = 5000\n",
    "CALIBRATION_DATASET_SIZE = 1000\n",
    "BATCH_SIZE = 80\n",
    "\n",
    "ROOT_DATASET_DIR = '/teamspace/studios/this_studio/COCO'\n",
    "IMAGES_DIR = os.path.join(ROOT_DATASET_DIR, 'images')\n",
    "ANNOTATIONS_JSON_PATH = os.path.join(ROOT_DATASET_DIR, 'annotations/instances_val2017.json')\n",
    "# ANNOTATIONS_JSON_PATH = \"/home/shayaan/Desktop/aimet/my_mmdet/temp.json\"\n",
    "\n",
    "model = DetInferencer(model=CONFIG_PATH, weights=WEIGHTS_PATH, device=DEVICE)\n",
    "evalDataset = CustomImageDataset(images_dir=IMAGES_DIR, annotations_json_path=ANNOTATIONS_JSON_PATH, transform=transform)\n",
    "eval_data_loader = DataLoader(evalDataset, batch_size=BATCH_SIZE)\n",
    "\n",
    "DEVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4.896168, 383)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_params = sum(p.numel() for p in model.model.parameters())\n",
    "total_params / 10 ** 6, len(list(model.model.modules())) - 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done (t=0.67s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "from mmcv.transforms import Compose\n",
    "test_evaluator = model.cfg.test_evaluator\n",
    "test_evaluator.type = 'mmdet.evaluation.CocoMetric' \n",
    "test_evaluator.dataset_meta = model.model.dataset_meta\n",
    "test_evaluator.ann_file = ANNOTATIONS_JSON_PATH\n",
    "test_evaluator = Compose(test_evaluator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from typing import Optional\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from glob import glob\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "\n",
    "from mmengine.structures import InstanceData\n",
    "from mmdet.models.utils import samplelist_boxtype2tensor\n",
    "from mmengine.registry import MODELS\n",
    "\n",
    "collate_preprocessor = model.preprocess\n",
    "predict_by_feat = model.model.bbox_head.predict_by_feat\n",
    "rescale = True\n",
    "\n",
    "preprocessor = MODELS.build(model.cfg.model.data_preprocessor)\n",
    "def add_pred_to_datasample(data_samples, results_list):\n",
    "    for data_sample, pred_instances in zip(data_samples, results_list):\n",
    "        data_sample.pred_instances = pred_instances\n",
    "    samplelist_boxtype2tensor(data_samples)\n",
    "    return data_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_callback(model, use_cuda):\n",
    "    data_loader = eval_data_loader\n",
    "    new_preds = []\n",
    "    for image_id, image_path, _ in tqdm(data_loader):\n",
    "        pre_processed = collate_preprocessor(inputs=image_path, batch_size=BATCH_SIZE)\n",
    "        _, data = list(pre_processed)[0]\n",
    "        data = preprocessor(data, False)\n",
    "        preds = model(data['inputs'].cuda())\n",
    "        batch_img_metas = [\n",
    "        data_samples.metainfo for data_samples in data['data_samples']\n",
    "        ]\n",
    "        preds = predict_by_feat(*preds, batch_img_metas=batch_img_metas, rescale=True)\n",
    "        preds = add_pred_to_datasample(data['data_samples'], preds)\n",
    "        \n",
    "        for img_id, pred in zip(image_id, preds):\n",
    "            pred = pred.pred_instances\n",
    "            new_pred = InstanceData(metainfo={\"img_id\": int(img_id)})\n",
    "            new_pred.bboxes = [np.array(p) for p in pred['bboxes'].cpu()]\n",
    "            new_pred.labels = pred['labels'].cpu()\n",
    "            new_pred.scores = pred['scores'].cpu()\n",
    "            new_preds.append(new_pred)\n",
    "\n",
    "    eval_results = test_evaluator(new_preds)\n",
    "    num_file = len(glob(\"/teamspace/studios/this_studio/aimet/Examples/torch/quantization/eval_stats/eval_acc_*\"))\n",
    "    with open(f\"/teamspace/studios/this_studio/aimet/Examples/torch/quantization/eval_stats/eval_acc_{num_file}.json\", \"w\") as f:\n",
    "        json.dump(eval_results, f, indent=4)\n",
    "    bbox_map = eval_results['bbox_mAP']\n",
    "    return bbox_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pass_calibration_data(model: torch.nn.Module, use_cuda):\n",
    "    data_loader = eval_data_loader\n",
    "    batch_size = data_loader.batch_size\n",
    "    model.eval()\n",
    "    samples = CALIBRATION_DATASET_SIZE\n",
    "    batch_ctr = 0\n",
    "    with torch.no_grad():\n",
    "        for image_id, image_path, _ in tqdm(data_loader):\n",
    "            pre_processed = collate_preprocessor(inputs=image_path, batch_size=BATCH_SIZE)\n",
    "            _, data = list(pre_processed)[0]\n",
    "            data = preprocessor(data, False)\n",
    "            \n",
    "            preds = model(data['inputs'].cuda())\n",
    "\n",
    "            batch_ctr += 1\n",
    "            if (batch_ctr * batch_size) > samples:\n",
    "                break  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "AIMET quantization simulation requires the user's model definition to follow certain guidelines.\n",
    "For example, functionals defined in forward pass should be changed to equivalent torch.nn.Module.\n",
    "AIMET user guide lists all these guidelines.\n",
    "\n",
    "The following **ModelPreparer** API uses new graph transformation feature available in PyTorch 1.9+ version and automates model definition changes required to comply with the above guidelines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-08-09 09:30:21,147 - root - INFO - AIMET\n",
      "2024-08-09 09:30:21,424 - ModelPreparer - INFO - Functional         : Adding new module for node: {backbone.stem.0.bn.module_batch_norm} \n",
      "2024-08-09 09:30:21,425 - ModelPreparer - INFO - Functional         : Adding new module for node: {backbone.stem.1.bn.module_batch_norm_1} \n",
      "2024-08-09 09:30:21,426 - ModelPreparer - INFO - Functional         : Adding new module for node: {backbone.stem.2.bn.module_batch_norm_2} \n",
      "2024-08-09 09:30:21,427 - ModelPreparer - INFO - Functional         : Adding new module for node: {backbone.stage1.0.bn.module_batch_norm_3} \n",
      "2024-08-09 09:30:21,428 - ModelPreparer - INFO - Functional         : Adding new module for node: {backbone.stage1.1.short_conv.bn.module_batch_norm_4} \n",
      "2024-08-09 09:30:21,428 - ModelPreparer - INFO - Functional         : Adding new module for node: {backbone.stage1.1.main_conv.bn.module_batch_norm_5} \n",
      "2024-08-09 09:30:21,429 - ModelPreparer - INFO - Functional         : Adding new module for node: {backbone.stage1.1.blocks.0.conv1.bn.module_batch_norm_6} \n",
      "2024-08-09 09:30:21,430 - ModelPreparer - INFO - Functional         : Adding new module for node: {backbone.stage1.1.blocks.0.conv2.depthwise_conv.bn.module_batch_norm_7} \n",
      "2024-08-09 09:30:21,431 - ModelPreparer - INFO - Functional         : Adding new module for node: {backbone.stage1.1.blocks.0.conv2.pointwise_conv.bn.module_batch_norm_8} \n",
      "2024-08-09 09:30:21,431 - ModelPreparer - INFO - Functional         : Adding new module for node: {backbone.stage1.1.blocks.0.module_add} \n",
      "2024-08-09 09:30:21,432 - ModelPreparer - INFO - Functional         : Adding new module for node: {backbone.stage1.1.module_cat} \n",
      "2024-08-09 09:30:21,432 - ModelPreparer - INFO - Functional         : Adding new module for node: {backbone.stage1.1.attention.module_mul} \n",
      "2024-08-09 09:30:21,433 - ModelPreparer - INFO - Functional         : Adding new module for node: {backbone.stage1.1.final_conv.bn.module_batch_norm_9} \n",
      "2024-08-09 09:30:21,434 - ModelPreparer - INFO - Functional         : Adding new module for node: {backbone.stage2.0.bn.module_batch_norm_10} \n",
      "2024-08-09 09:30:21,434 - ModelPreparer - INFO - Functional         : Adding new module for node: {backbone.stage2.1.short_conv.bn.module_batch_norm_11} \n",
      "2024-08-09 09:30:21,435 - ModelPreparer - INFO - Functional         : Adding new module for node: {backbone.stage2.1.main_conv.bn.module_batch_norm_12} \n",
      "2024-08-09 09:30:21,436 - ModelPreparer - INFO - Functional         : Adding new module for node: {backbone.stage2.1.blocks.0.conv1.bn.module_batch_norm_13} \n",
      "2024-08-09 09:30:21,436 - ModelPreparer - INFO - Functional         : Adding new module for node: {backbone.stage2.1.blocks.0.conv2.depthwise_conv.bn.module_batch_norm_14} \n",
      "2024-08-09 09:30:21,437 - ModelPreparer - INFO - Functional         : Adding new module for node: {backbone.stage2.1.blocks.0.conv2.pointwise_conv.bn.module_batch_norm_15} \n",
      "2024-08-09 09:30:21,439 - ModelPreparer - INFO - Functional         : Adding new module for node: {backbone.stage2.1.blocks.0.module_add_1} \n",
      "2024-08-09 09:30:21,440 - ModelPreparer - INFO - Functional         : Adding new module for node: {backbone.stage2.1.module_cat_1} \n",
      "2024-08-09 09:30:21,440 - ModelPreparer - INFO - Functional         : Adding new module for node: {backbone.stage2.1.attention.module_mul_1} \n",
      "2024-08-09 09:30:21,441 - ModelPreparer - INFO - Functional         : Adding new module for node: {backbone.stage2.1.final_conv.bn.module_batch_norm_16} \n",
      "2024-08-09 09:30:21,442 - ModelPreparer - INFO - Functional         : Adding new module for node: {backbone.stage3.0.bn.module_batch_norm_17} \n",
      "2024-08-09 09:30:21,443 - ModelPreparer - INFO - Functional         : Adding new module for node: {backbone.stage3.1.short_conv.bn.module_batch_norm_18} \n",
      "2024-08-09 09:30:21,443 - ModelPreparer - INFO - Functional         : Adding new module for node: {backbone.stage3.1.main_conv.bn.module_batch_norm_19} \n",
      "2024-08-09 09:30:21,444 - ModelPreparer - INFO - Functional         : Adding new module for node: {backbone.stage3.1.blocks.0.conv1.bn.module_batch_norm_20} \n",
      "2024-08-09 09:30:21,445 - ModelPreparer - INFO - Functional         : Adding new module for node: {backbone.stage3.1.blocks.0.conv2.depthwise_conv.bn.module_batch_norm_21} \n",
      "2024-08-09 09:30:21,446 - ModelPreparer - INFO - Functional         : Adding new module for node: {backbone.stage3.1.blocks.0.conv2.pointwise_conv.bn.module_batch_norm_22} \n",
      "2024-08-09 09:30:21,446 - ModelPreparer - INFO - Functional         : Adding new module for node: {backbone.stage3.1.blocks.0.module_add_2} \n",
      "2024-08-09 09:30:21,447 - ModelPreparer - INFO - Functional         : Adding new module for node: {backbone.stage3.1.module_cat_2} \n",
      "2024-08-09 09:30:21,448 - ModelPreparer - INFO - Functional         : Adding new module for node: {backbone.stage3.1.attention.module_mul_2} \n",
      "2024-08-09 09:30:21,448 - ModelPreparer - INFO - Functional         : Adding new module for node: {backbone.stage3.1.final_conv.bn.module_batch_norm_23} \n",
      "2024-08-09 09:30:21,449 - ModelPreparer - INFO - Functional         : Adding new module for node: {backbone.stage4.0.bn.module_batch_norm_24} \n",
      "2024-08-09 09:30:21,450 - ModelPreparer - INFO - Functional         : Adding new module for node: {backbone.stage4.1.conv1.bn.module_batch_norm_25} \n",
      "2024-08-09 09:30:21,451 - ModelPreparer - INFO - Functional         : Adding new module for node: {backbone.stage4.1.module_cat_3} \n",
      "2024-08-09 09:30:21,451 - ModelPreparer - INFO - Functional         : Adding new module for node: {backbone.stage4.1.conv2.bn.module_batch_norm_26} \n",
      "2024-08-09 09:30:21,452 - ModelPreparer - INFO - Functional         : Adding new module for node: {backbone.stage4.2.short_conv.bn.module_batch_norm_27} \n",
      "2024-08-09 09:30:21,453 - ModelPreparer - INFO - Functional         : Adding new module for node: {backbone.stage4.2.main_conv.bn.module_batch_norm_28} \n",
      "2024-08-09 09:30:21,454 - ModelPreparer - INFO - Functional         : Adding new module for node: {backbone.stage4.2.blocks.0.conv1.bn.module_batch_norm_29} \n",
      "2024-08-09 09:30:21,454 - ModelPreparer - INFO - Functional         : Adding new module for node: {backbone.stage4.2.blocks.0.conv2.depthwise_conv.bn.module_batch_norm_30} \n",
      "2024-08-09 09:30:21,455 - ModelPreparer - INFO - Functional         : Adding new module for node: {backbone.stage4.2.blocks.0.conv2.pointwise_conv.bn.module_batch_norm_31} \n",
      "2024-08-09 09:30:21,457 - ModelPreparer - INFO - Functional         : Adding new module for node: {backbone.stage4.2.module_cat_4} \n",
      "2024-08-09 09:30:21,458 - ModelPreparer - INFO - Functional         : Adding new module for node: {backbone.stage4.2.attention.module_mul_3} \n",
      "2024-08-09 09:30:21,459 - ModelPreparer - INFO - Functional         : Adding new module for node: {backbone.stage4.2.final_conv.bn.module_batch_norm_32} \n",
      "2024-08-09 09:30:21,460 - ModelPreparer - INFO - Functional         : Adding new module for node: {neck.reduce_layers.0.bn.module_batch_norm_33} \n",
      "2024-08-09 09:30:21,461 - ModelPreparer - INFO - Functional         : Adding new module for node: {neck.module_cat_5} \n",
      "2024-08-09 09:30:21,461 - ModelPreparer - INFO - Functional         : Adding new module for node: {neck.top_down_blocks.0.short_conv.bn.module_batch_norm_34} \n",
      "2024-08-09 09:30:21,462 - ModelPreparer - INFO - Functional         : Adding new module for node: {neck.top_down_blocks.0.main_conv.bn.module_batch_norm_35} \n",
      "2024-08-09 09:30:21,463 - ModelPreparer - INFO - Functional         : Adding new module for node: {neck.top_down_blocks.0.blocks.0.conv1.bn.module_batch_norm_36} \n",
      "2024-08-09 09:30:21,464 - ModelPreparer - INFO - Functional         : Adding new module for node: {neck.top_down_blocks.0.blocks.0.conv2.depthwise_conv.bn.module_batch_norm_37} \n",
      "2024-08-09 09:30:21,465 - ModelPreparer - INFO - Functional         : Adding new module for node: {neck.top_down_blocks.0.blocks.0.conv2.pointwise_conv.bn.module_batch_norm_38} \n",
      "2024-08-09 09:30:21,465 - ModelPreparer - INFO - Functional         : Adding new module for node: {neck.top_down_blocks.0.module_cat_6} \n",
      "2024-08-09 09:30:21,466 - ModelPreparer - INFO - Functional         : Adding new module for node: {neck.top_down_blocks.0.final_conv.bn.module_batch_norm_39} \n",
      "2024-08-09 09:30:21,467 - ModelPreparer - INFO - Functional         : Adding new module for node: {neck.reduce_layers.1.bn.module_batch_norm_40} \n",
      "2024-08-09 09:30:21,468 - ModelPreparer - INFO - Reused/Duplicate   : Adding new module for node: {neck.module_upsample_1} \n",
      "2024-08-09 09:30:21,469 - ModelPreparer - INFO - Functional         : Adding new module for node: {neck.module_cat_7} \n",
      "2024-08-09 09:30:21,469 - ModelPreparer - INFO - Functional         : Adding new module for node: {neck.top_down_blocks.1.short_conv.bn.module_batch_norm_41} \n",
      "2024-08-09 09:30:21,470 - ModelPreparer - INFO - Functional         : Adding new module for node: {neck.top_down_blocks.1.main_conv.bn.module_batch_norm_42} \n",
      "2024-08-09 09:30:21,471 - ModelPreparer - INFO - Functional         : Adding new module for node: {neck.top_down_blocks.1.blocks.0.conv1.bn.module_batch_norm_43} \n",
      "2024-08-09 09:30:21,472 - ModelPreparer - INFO - Functional         : Adding new module for node: {neck.top_down_blocks.1.blocks.0.conv2.depthwise_conv.bn.module_batch_norm_44} \n",
      "2024-08-09 09:30:21,472 - ModelPreparer - INFO - Functional         : Adding new module for node: {neck.top_down_blocks.1.blocks.0.conv2.pointwise_conv.bn.module_batch_norm_45} \n",
      "2024-08-09 09:30:21,473 - ModelPreparer - INFO - Functional         : Adding new module for node: {neck.top_down_blocks.1.module_cat_8} \n",
      "2024-08-09 09:30:21,473 - ModelPreparer - INFO - Functional         : Adding new module for node: {neck.top_down_blocks.1.final_conv.bn.module_batch_norm_46} \n",
      "2024-08-09 09:30:21,474 - ModelPreparer - INFO - Functional         : Adding new module for node: {neck.downsamples.0.bn.module_batch_norm_47} \n",
      "2024-08-09 09:30:21,475 - ModelPreparer - INFO - Functional         : Adding new module for node: {neck.module_cat_9} \n",
      "2024-08-09 09:30:21,475 - ModelPreparer - INFO - Functional         : Adding new module for node: {neck.bottom_up_blocks.0.short_conv.bn.module_batch_norm_48} \n",
      "2024-08-09 09:30:21,476 - ModelPreparer - INFO - Functional         : Adding new module for node: {neck.bottom_up_blocks.0.main_conv.bn.module_batch_norm_49} \n",
      "2024-08-09 09:30:21,477 - ModelPreparer - INFO - Functional         : Adding new module for node: {neck.bottom_up_blocks.0.blocks.0.conv1.bn.module_batch_norm_50} \n",
      "2024-08-09 09:30:21,477 - ModelPreparer - INFO - Functional         : Adding new module for node: {neck.bottom_up_blocks.0.blocks.0.conv2.depthwise_conv.bn.module_batch_norm_51} \n",
      "2024-08-09 09:30:21,478 - ModelPreparer - INFO - Functional         : Adding new module for node: {neck.bottom_up_blocks.0.blocks.0.conv2.pointwise_conv.bn.module_batch_norm_52} \n",
      "2024-08-09 09:30:21,479 - ModelPreparer - INFO - Functional         : Adding new module for node: {neck.bottom_up_blocks.0.module_cat_10} \n",
      "2024-08-09 09:30:21,479 - ModelPreparer - INFO - Functional         : Adding new module for node: {neck.bottom_up_blocks.0.final_conv.bn.module_batch_norm_53} \n",
      "2024-08-09 09:30:21,480 - ModelPreparer - INFO - Functional         : Adding new module for node: {neck.downsamples.1.bn.module_batch_norm_54} \n",
      "2024-08-09 09:30:21,481 - ModelPreparer - INFO - Functional         : Adding new module for node: {neck.module_cat_11} \n",
      "2024-08-09 09:30:21,481 - ModelPreparer - INFO - Functional         : Adding new module for node: {neck.bottom_up_blocks.1.short_conv.bn.module_batch_norm_55} \n",
      "2024-08-09 09:30:21,482 - ModelPreparer - INFO - Functional         : Adding new module for node: {neck.bottom_up_blocks.1.main_conv.bn.module_batch_norm_56} \n",
      "2024-08-09 09:30:21,483 - ModelPreparer - INFO - Functional         : Adding new module for node: {neck.bottom_up_blocks.1.blocks.0.conv1.bn.module_batch_norm_57} \n",
      "2024-08-09 09:30:21,483 - ModelPreparer - INFO - Functional         : Adding new module for node: {neck.bottom_up_blocks.1.blocks.0.conv2.depthwise_conv.bn.module_batch_norm_58} \n",
      "2024-08-09 09:30:21,489 - ModelPreparer - INFO - Functional         : Adding new module for node: {neck.bottom_up_blocks.1.blocks.0.conv2.pointwise_conv.bn.module_batch_norm_59} \n",
      "2024-08-09 09:30:21,489 - ModelPreparer - INFO - Functional         : Adding new module for node: {neck.bottom_up_blocks.1.module_cat_12} \n",
      "2024-08-09 09:30:21,490 - ModelPreparer - INFO - Functional         : Adding new module for node: {neck.bottom_up_blocks.1.final_conv.bn.module_batch_norm_60} \n",
      "2024-08-09 09:30:21,491 - ModelPreparer - INFO - Functional         : Adding new module for node: {neck.out_convs.0.bn.module_batch_norm_61} \n",
      "2024-08-09 09:30:21,491 - ModelPreparer - INFO - Functional         : Adding new module for node: {neck.out_convs.1.bn.module_batch_norm_62} \n",
      "2024-08-09 09:30:21,492 - ModelPreparer - INFO - Functional         : Adding new module for node: {neck.out_convs.2.bn.module_batch_norm_63} \n",
      "2024-08-09 09:30:21,493 - ModelPreparer - INFO - Functional         : Adding new module for node: {bbox_head.cls_convs.0.0.bn.module_batch_norm_64} \n",
      "2024-08-09 09:30:21,494 - ModelPreparer - INFO - Functional         : Adding new module for node: {bbox_head.cls_convs.0.1.bn.module_batch_norm_65} \n",
      "2024-08-09 09:30:21,494 - ModelPreparer - INFO - Functional         : Adding new module for node: {bbox_head.reg_convs.0.0.bn.module_batch_norm_66} \n",
      "2024-08-09 09:30:21,495 - ModelPreparer - INFO - Functional         : Adding new module for node: {bbox_head.reg_convs.0.1.bn.module_batch_norm_67} \n",
      "2024-08-09 09:30:21,495 - ModelPreparer - INFO - Functional         : Adding new module for node: {bbox_head.module_mul_4} \n",
      "2024-08-09 09:30:21,497 - ModelPreparer - INFO - Reused/Duplicate   : Adding new module for node: {bbox_head.cls_convs.0.0.module_conv_1} \n",
      "2024-08-09 09:30:21,498 - ModelPreparer - INFO - Functional         : Adding new module for node: {bbox_head.cls_convs.1.0.bn.module_batch_norm_68} \n",
      "2024-08-09 09:30:21,499 - ModelPreparer - INFO - Reused/Duplicate   : Adding new module for node: {bbox_head.cls_convs.0.1.module_conv_1} \n",
      "2024-08-09 09:30:21,500 - ModelPreparer - INFO - Functional         : Adding new module for node: {bbox_head.cls_convs.1.1.bn.module_batch_norm_69} \n",
      "2024-08-09 09:30:21,501 - ModelPreparer - INFO - Reused/Duplicate   : Adding new module for node: {bbox_head.reg_convs.0.0.module_conv_1} \n",
      "2024-08-09 09:30:21,501 - ModelPreparer - INFO - Functional         : Adding new module for node: {bbox_head.reg_convs.1.0.bn.module_batch_norm_70} \n",
      "2024-08-09 09:30:21,502 - ModelPreparer - INFO - Reused/Duplicate   : Adding new module for node: {bbox_head.reg_convs.0.1.module_conv_1} \n",
      "2024-08-09 09:30:21,503 - ModelPreparer - INFO - Functional         : Adding new module for node: {bbox_head.reg_convs.1.1.bn.module_batch_norm_71} \n",
      "2024-08-09 09:30:21,503 - ModelPreparer - INFO - Functional         : Adding new module for node: {bbox_head.module_mul_5} \n",
      "2024-08-09 09:30:21,505 - ModelPreparer - INFO - Reused/Duplicate   : Adding new module for node: {bbox_head.cls_convs.0.0.module_conv_2} \n",
      "2024-08-09 09:30:21,505 - ModelPreparer - INFO - Functional         : Adding new module for node: {bbox_head.cls_convs.2.0.bn.module_batch_norm_72} \n",
      "2024-08-09 09:30:21,506 - ModelPreparer - INFO - Reused/Duplicate   : Adding new module for node: {bbox_head.cls_convs.0.1.module_conv_2} \n",
      "2024-08-09 09:30:21,507 - ModelPreparer - INFO - Functional         : Adding new module for node: {bbox_head.cls_convs.2.1.bn.module_batch_norm_73} \n",
      "2024-08-09 09:30:21,508 - ModelPreparer - INFO - Reused/Duplicate   : Adding new module for node: {bbox_head.reg_convs.0.0.module_conv_2} \n",
      "2024-08-09 09:30:21,508 - ModelPreparer - INFO - Functional         : Adding new module for node: {bbox_head.reg_convs.2.0.bn.module_batch_norm_74} \n",
      "2024-08-09 09:30:21,509 - ModelPreparer - INFO - Reused/Duplicate   : Adding new module for node: {bbox_head.reg_convs.0.1.module_conv_2} \n",
      "2024-08-09 09:30:21,510 - ModelPreparer - INFO - Functional         : Adding new module for node: {bbox_head.reg_convs.2.1.bn.module_batch_norm_75} \n",
      "2024-08-09 09:30:21,511 - ModelPreparer - INFO - Functional         : Adding new module for node: {bbox_head.module_mul_6} \n"
     ]
    }
   ],
   "source": [
    "from aimet_torch.model_preparer import prepare_model\n",
    "\n",
    "model = prepare_model(model.model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "---\n",
    "We should decide whether to place the model on a CPU or CUDA device.\n",
    "This example code will use CUDA if available in your current execution environment.\n",
    "You can change this logic and force a device placement if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"------------FP 32 MODEL MODULES ------------\")\n",
    "# print(dict(model.named_modules()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "use_cuda = False\n",
    "if torch.cuda.is_available():\n",
    "    use_cuda = True\n",
    "    model.to(torch.device('cuda'))\n",
    "use_cuda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "---\n",
    "\n",
    "## 3. Apply QuantAnalyzer to the model\n",
    "\n",
    "QuantAnalyzer requires two functions to be defined by the user for passing data through the model:\n",
    "\n",
    "**Forward pass callback**\n",
    "\n",
    "One function will be used to pass representative data through a quantized version of the model to calibrate quantization parameters.\n",
    "This function should be fairly simple - use the existing train or validation data loader to extract some samples and pass them to the model.\n",
    "We don't need to compute any loss metrics, so we can just ignore the model output.\n",
    "\n",
    "The function **must** take two arguments, the first of which will be the model to run the forward pass on.\n",
    "The second argument can be anything additional which the function requires to run, and can be in the form of a single item or a tuple of items.\n",
    "\n",
    "If no additional argument is needed, the user can specify a dummy \"_\" parameter for the function.\n",
    "\n",
    "A few pointers regarding the forward pass data samples:\n",
    "\n",
    "- In practice, we need a very small percentage of the overall data samples for computing encodings.\n",
    "  For example, the training dataset for ImageNet has 1M samples. For computing encodings we only need 500 to 1000 samples.\n",
    "- It may be beneficial if the samples used for computing encoding are well distributed.\n",
    "  It's not necessary that all classes need to be covered since we are only looking at the range of values at every layer activation.\n",
    "  However, we definitely want to avoid an extreme scenario like all 'dark' or 'light' samples are used - e.g. only using pictures captured at night might not give ideal results.\n",
    "\n",
    "The following shows an example of a routine that passes unlabeled samples through the model for computing encodings.\n",
    "This routine can be written in many ways; this is just an example.\n",
    "This function only requires unlabeled data as no loss or other evaluation metric is needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "In order to pass this function to QuantAnalyzer, we need to wrap it in a CallbackFunc object, as shown below.\n",
    "The CallbackFunc takes two arguments: the callback function itself, and the inputs to pass into the callback function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from aimet_torch.quant_analyzer import CallbackFunc\n",
    "\n",
    "forward_pass_callback = CallbackFunc(pass_calibration_data, use_cuda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "---\n",
    "\n",
    "**Evaluation callback**\n",
    "\n",
    "The second function will be used to evaluate the model, and needs to return an accuracy metric.\n",
    "In here, the user should pass any amount of data through the model which they would like when evaluating their model for accuracy.\n",
    "\n",
    "Like the forward pass callback, this function also must take exactly two arguments: the model to evaluate, and any additional argument needed for the function to work.\n",
    "The second argument can be a tuple of items in case multiple items are needed.\n",
    "\n",
    "We will be using the ImageNetDataPipeline's evaluate defined above for this purpose.\n",
    "Like the forward pass callback, we need to wrap the evaluation callback in a CallbackFunc object as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "eval_callback = CallbackFunc(eval_callback, use_cuda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "---\n",
    "\n",
    "**Enabling MSE loss per layer analysis**\n",
    "\n",
    "An optional analysis step in QuantAnalyzer calculates the MSE loss per layer in the model, comparing the layer outputs from the original FP32 model vs. a quantized model.\n",
    "To perform this step, the user needs to also provide an unlabeled DataLoader to QuantAnalyzer.\n",
    "\n",
    "We will demonstrate this step by using the ImageNetDataLoader imported above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_loader = eval_data_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "---\n",
    "\n",
    "QuantAnalyzer also requires a dummy input to the model.\n",
    "This dummy input does not need to be representative of the dataset.\n",
    "All that matters is that the input shape is correct for the model to run on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dummy_input = torch.rand(1, 3, 640, 640).cuda()    # Shape for each ImageNet sample is (3 channels) x (224 height) x (224 width)\n",
    "if use_cuda:\n",
    "    dummy_input = dummy_input.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "---\n",
    "We are now ready to apply QuantAnalyzer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[BatchNorm(),\n",
       " BatchNorm(),\n",
       " Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False),\n",
       " BatchNorm(),\n",
       " BatchNorm(),\n",
       " BatchNorm(),\n",
       " BatchNorm(),\n",
       " BatchNorm(),\n",
       " BatchNorm()]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "module_names = dict(model.named_modules())\n",
    "modules_to_ignore = ['backbone.stage2.1.blocks.0.conv2.depthwise_conv.bn.module_batch_norm_14', 'backbone.stage1.1.blocks.0.conv2.depthwise_conv.bn.module_batch_norm_7', 'backbone.stage2.1.blocks.0.conv2.pointwise_conv.conv', 'backbone.stage3.1.blocks.0.conv2.depthwise_conv.bn.module_batch_norm_21', 'backbone.stage4.2.blocks.0.conv2.depthwise_conv.bn.module_batch_norm_30', 'neck.top_down_blocks.0.blocks.0.conv2.depthwise_conv.bn.module_batch_norm_37', 'neck.top_down_blocks.1.blocks.0.conv2.depthwise_conv.bn.module_batch_norm_44', 'neck.bottom_up_blocks.0.blocks.0.conv2.depthwise_conv.bn.module_batch_norm_51', 'neck.bottom_up_blocks.1.blocks.0.conv2.depthwise_conv.bn.module_batch_norm_58']\n",
    "modules_to_ignore = [module_names[m] for m in modules_to_ignore]\n",
    "\n",
    "modules_to_ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from aimet_torch.v2.quant_analyzer import QuantAnalyzer\n",
    "\n",
    "quant_analyzer = QuantAnalyzer(model, dummy_input, forward_pass_callback, eval_callback, modules_to_ignore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-08-09 09:30:33,909 - BatchNormFolding - INFO - 0 BatchNorms' weights got converted\n",
      "2024-08-09 09:30:39,861 - Quant - INFO - No config file provided, defaulting to config file at /usr/local/lib/python3.10/dist-packages/aimet_common/quantsim_config/default_config.json\n",
      "2024-08-09 09:30:39,893 - Quant - INFO - Unsupported op type Squeeze\n",
      "2024-08-09 09:30:39,893 - Quant - INFO - Unsupported op type Mean\n",
      "2024-08-09 09:30:39,907 - Quant - INFO - Selecting DefaultOpInstanceConfigGenerator to compute the specialized config. hw_version:default\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▉        | 12/63 [07:55<33:41, 39.65s/it]\n",
      "  0%|          | 0/63 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/functional.py:507: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3549.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
      "100%|██████████| 63/63 [06:33<00:00,  6.24s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "08/09 09:45:45 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Evaluating bbox...\n",
      "Loading and preparing results...\n",
      "DONE (t=4.27s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=80.21s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=26.10s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.411\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.579\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.447\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.210\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.455\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.583\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.334\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.554\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.605\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.381\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.677\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.797\n",
      "2024-08-09 09:47:49,594 - QuantAnalyzer - INFO - FP32 eval score (W32A32): 0.411000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 63/63 [06:38<00:00,  6.33s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "08/09 09:55:01 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Evaluating bbox...\n",
      "Loading and preparing results...\n",
      "DONE (t=5.77s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=77.50s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=28.01s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.385\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.551\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.419\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.202\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.426\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.549\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.322\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.535\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.588\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.367\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.657\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.777\n",
      "2024-08-09 09:57:04,966 - QuantAnalyzer - INFO - Weight-quantized eval score (W8A32): 0.385000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 63/63 [09:15<00:00,  8.81s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "08/09 10:06:51 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Evaluating bbox...\n",
      "Loading and preparing results...\n",
      "DONE (t=4.32s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=80.84s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=27.77s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.282\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.423\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.303\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.141\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.324\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.402\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.269\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.458\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.509\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.281\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.563\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.688\n",
      "2024-08-09 10:08:57,549 - QuantAnalyzer - INFO - Activation-quantized eval score (W32A8): 0.282000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 63/63 [09:16<00:00,  8.84s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "08/09 10:18:47 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Evaluating bbox...\n",
      "Loading and preparing results...\n",
      "DONE (t=5.41s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=80.47s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=27.76s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.272\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.410\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.293\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.133\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.308\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.401\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.265\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.452\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.505\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.272\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.561\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.695\n",
      "2024-08-09 10:20:54,175 - QuantAnalyzer - INFO - Activation&Weight-quantized eval score (W8A8): 0.272\n"
     ]
    }
   ],
   "source": [
    "from aimet_common.defs import QuantScheme\n",
    "\n",
    "quant_analyzer.analyze(quant_scheme=QuantScheme.post_training_tf_enhanced,\n",
    "                       default_param_bw=8,\n",
    "                       default_output_bw=8,\n",
    "                       config_file=None,\n",
    "                       results_dir=\"./tmp/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-08-09 07:06:29,906 - BatchNormFolding - INFO - 0 BatchNorms' weights got converted\n",
      "2024-08-09 07:06:36,145 - Quant - INFO - No config file provided, defaulting to config file at /usr/local/lib/python3.10/dist-packages/aimet_common/quantsim_config/default_config.json\n",
      "2024-08-09 07:06:36,177 - Quant - INFO - Unsupported op type Squeeze\n",
      "2024-08-09 07:06:36,178 - Quant - INFO - Unsupported op type Mean\n",
      "2024-08-09 07:06:36,191 - Quant - INFO - Selecting DefaultOpInstanceConfigGenerator to compute the specialized config. hw_version:default\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▉        | 12/63 [08:04<34:18, 40.36s/it]\n"
     ]
    }
   ],
   "source": [
    "from aimet_common.defs import QuantScheme\n",
    "\n",
    "sim = quant_analyzer._create_quantsim_and_encodings(quant_scheme=QuantScheme.post_training_tf_enhanced,\n",
    "                       default_param_bw=8,\n",
    "                       default_output_bw=8,\n",
    "                       config_file=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'': GraphModule(\n",
       "   (backbone): Module(\n",
       "     (stem): Module(\n",
       "       (0): Module(\n",
       "         (conv): QuantizedConv2d(\n",
       "           3, 12, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False\n",
       "           (param_quantizers): ModuleDict(\n",
       "             (weight): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=True)\n",
       "           )\n",
       "           (input_quantizers): ModuleList(\n",
       "             (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "           )\n",
       "           (output_quantizers): ModuleList(\n",
       "             (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "           )\n",
       "         )\n",
       "         (bn): Module(\n",
       "           (module_batch_norm): FakeQuantizedBatchNorm(\n",
       "             (param_quantizers): ModuleDict()\n",
       "             (input_quantizers): ModuleList(\n",
       "               (0-4): 5 x QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "             )\n",
       "             (output_quantizers): ModuleList(\n",
       "               (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "             )\n",
       "           )\n",
       "         )\n",
       "         (activate): CustomSiLU(\n",
       "           (sigmoid): QuantizedSigmoid(\n",
       "             (param_quantizers): ModuleDict()\n",
       "             (input_quantizers): ModuleList(\n",
       "               (0): None\n",
       "             )\n",
       "             (output_quantizers): ModuleList(\n",
       "               (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "             )\n",
       "           )\n",
       "           (mul): QuantizedMultiply(\n",
       "             (param_quantizers): ModuleDict()\n",
       "             (input_quantizers): ModuleList(\n",
       "               (0-1): 2 x None\n",
       "             )\n",
       "             (output_quantizers): ModuleList(\n",
       "               (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "             )\n",
       "           )\n",
       "         )\n",
       "       )\n",
       "       (1): Module(\n",
       "         (conv): QuantizedConv2d(\n",
       "           12, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "           (param_quantizers): ModuleDict(\n",
       "             (weight): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=True)\n",
       "           )\n",
       "           (input_quantizers): ModuleList(\n",
       "             (0): None\n",
       "           )\n",
       "           (output_quantizers): ModuleList(\n",
       "             (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "           )\n",
       "         )\n",
       "         (bn): Module(\n",
       "           (module_batch_norm_1): FakeQuantizedBatchNorm(\n",
       "             (param_quantizers): ModuleDict()\n",
       "             (input_quantizers): ModuleList(\n",
       "               (0-4): 5 x QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "             )\n",
       "             (output_quantizers): ModuleList(\n",
       "               (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "             )\n",
       "           )\n",
       "         )\n",
       "         (activate): CustomSiLU(\n",
       "           (sigmoid): QuantizedSigmoid(\n",
       "             (param_quantizers): ModuleDict()\n",
       "             (input_quantizers): ModuleList(\n",
       "               (0): None\n",
       "             )\n",
       "             (output_quantizers): ModuleList(\n",
       "               (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "             )\n",
       "           )\n",
       "           (mul): QuantizedMultiply(\n",
       "             (param_quantizers): ModuleDict()\n",
       "             (input_quantizers): ModuleList(\n",
       "               (0-1): 2 x None\n",
       "             )\n",
       "             (output_quantizers): ModuleList(\n",
       "               (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "             )\n",
       "           )\n",
       "         )\n",
       "       )\n",
       "       (2): Module(\n",
       "         (conv): QuantizedConv2d(\n",
       "           12, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "           (param_quantizers): ModuleDict(\n",
       "             (weight): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=True)\n",
       "           )\n",
       "           (input_quantizers): ModuleList(\n",
       "             (0): None\n",
       "           )\n",
       "           (output_quantizers): ModuleList(\n",
       "             (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "           )\n",
       "         )\n",
       "         (bn): Module(\n",
       "           (module_batch_norm_2): FakeQuantizedBatchNorm(\n",
       "             (param_quantizers): ModuleDict()\n",
       "             (input_quantizers): ModuleList(\n",
       "               (0-4): 5 x QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "             )\n",
       "             (output_quantizers): ModuleList(\n",
       "               (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "             )\n",
       "           )\n",
       "         )\n",
       "         (activate): CustomSiLU(\n",
       "           (sigmoid): QuantizedSigmoid(\n",
       "             (param_quantizers): ModuleDict()\n",
       "             (input_quantizers): ModuleList(\n",
       "               (0): None\n",
       "             )\n",
       "             (output_quantizers): ModuleList(\n",
       "               (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "             )\n",
       "           )\n",
       "           (mul): QuantizedMultiply(\n",
       "             (param_quantizers): ModuleDict()\n",
       "             (input_quantizers): ModuleList(\n",
       "               (0-1): 2 x None\n",
       "             )\n",
       "             (output_quantizers): ModuleList(\n",
       "               (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "             )\n",
       "           )\n",
       "         )\n",
       "       )\n",
       "     )\n",
       "     (stage1): Module(\n",
       "       (0): Module(\n",
       "         (conv): QuantizedConv2d(\n",
       "           24, 48, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False\n",
       "           (param_quantizers): ModuleDict(\n",
       "             (weight): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=True)\n",
       "           )\n",
       "           (input_quantizers): ModuleList(\n",
       "             (0): None\n",
       "           )\n",
       "           (output_quantizers): ModuleList(\n",
       "             (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "           )\n",
       "         )\n",
       "         (bn): Module(\n",
       "           (module_batch_norm_3): FakeQuantizedBatchNorm(\n",
       "             (param_quantizers): ModuleDict()\n",
       "             (input_quantizers): ModuleList(\n",
       "               (0-4): 5 x QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "             )\n",
       "             (output_quantizers): ModuleList(\n",
       "               (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "             )\n",
       "           )\n",
       "         )\n",
       "         (activate): CustomSiLU(\n",
       "           (sigmoid): QuantizedSigmoid(\n",
       "             (param_quantizers): ModuleDict()\n",
       "             (input_quantizers): ModuleList(\n",
       "               (0): None\n",
       "             )\n",
       "             (output_quantizers): ModuleList(\n",
       "               (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "             )\n",
       "           )\n",
       "           (mul): QuantizedMultiply(\n",
       "             (param_quantizers): ModuleDict()\n",
       "             (input_quantizers): ModuleList(\n",
       "               (0-1): 2 x None\n",
       "             )\n",
       "             (output_quantizers): ModuleList(\n",
       "               (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "             )\n",
       "           )\n",
       "         )\n",
       "       )\n",
       "       (1): Module(\n",
       "         (short_conv): Module(\n",
       "           (conv): QuantizedConv2d(\n",
       "             48, 24, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "             (param_quantizers): ModuleDict(\n",
       "               (weight): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=True)\n",
       "             )\n",
       "             (input_quantizers): ModuleList(\n",
       "               (0): None\n",
       "             )\n",
       "             (output_quantizers): ModuleList(\n",
       "               (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "             )\n",
       "           )\n",
       "           (bn): Module(\n",
       "             (module_batch_norm_4): FakeQuantizedBatchNorm(\n",
       "               (param_quantizers): ModuleDict()\n",
       "               (input_quantizers): ModuleList(\n",
       "                 (0-4): 5 x QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "               )\n",
       "               (output_quantizers): ModuleList(\n",
       "                 (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "               )\n",
       "             )\n",
       "           )\n",
       "           (activate): CustomSiLU(\n",
       "             (sigmoid): QuantizedSigmoid(\n",
       "               (param_quantizers): ModuleDict()\n",
       "               (input_quantizers): ModuleList(\n",
       "                 (0): None\n",
       "               )\n",
       "               (output_quantizers): ModuleList(\n",
       "                 (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "               )\n",
       "             )\n",
       "             (mul): QuantizedMultiply(\n",
       "               (param_quantizers): ModuleDict()\n",
       "               (input_quantizers): ModuleList(\n",
       "                 (0-1): 2 x None\n",
       "               )\n",
       "               (output_quantizers): ModuleList(\n",
       "                 (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "               )\n",
       "             )\n",
       "           )\n",
       "         )\n",
       "         (main_conv): Module(\n",
       "           (conv): QuantizedConv2d(\n",
       "             48, 24, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "             (param_quantizers): ModuleDict(\n",
       "               (weight): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=True)\n",
       "             )\n",
       "             (input_quantizers): ModuleList(\n",
       "               (0): None\n",
       "             )\n",
       "             (output_quantizers): ModuleList(\n",
       "               (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "             )\n",
       "           )\n",
       "           (bn): Module(\n",
       "             (module_batch_norm_5): FakeQuantizedBatchNorm(\n",
       "               (param_quantizers): ModuleDict()\n",
       "               (input_quantizers): ModuleList(\n",
       "                 (0-4): 5 x QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "               )\n",
       "               (output_quantizers): ModuleList(\n",
       "                 (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "               )\n",
       "             )\n",
       "           )\n",
       "           (activate): CustomSiLU(\n",
       "             (sigmoid): QuantizedSigmoid(\n",
       "               (param_quantizers): ModuleDict()\n",
       "               (input_quantizers): ModuleList(\n",
       "                 (0): None\n",
       "               )\n",
       "               (output_quantizers): ModuleList(\n",
       "                 (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "               )\n",
       "             )\n",
       "             (mul): QuantizedMultiply(\n",
       "               (param_quantizers): ModuleDict()\n",
       "               (input_quantizers): ModuleList(\n",
       "                 (0-1): 2 x None\n",
       "               )\n",
       "               (output_quantizers): ModuleList(\n",
       "                 (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "               )\n",
       "             )\n",
       "           )\n",
       "         )\n",
       "         (blocks): Module(\n",
       "           (0): Module(\n",
       "             (conv1): Module(\n",
       "               (conv): QuantizedConv2d(\n",
       "                 24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "                 (param_quantizers): ModuleDict(\n",
       "                   (weight): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=True)\n",
       "                 )\n",
       "                 (input_quantizers): ModuleList(\n",
       "                   (0): None\n",
       "                 )\n",
       "                 (output_quantizers): ModuleList(\n",
       "                   (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "                 )\n",
       "               )\n",
       "               (bn): Module(\n",
       "                 (module_batch_norm_6): FakeQuantizedBatchNorm(\n",
       "                   (param_quantizers): ModuleDict()\n",
       "                   (input_quantizers): ModuleList(\n",
       "                     (0-4): 5 x QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "                   )\n",
       "                   (output_quantizers): ModuleList(\n",
       "                     (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "                   )\n",
       "                 )\n",
       "               )\n",
       "               (activate): CustomSiLU(\n",
       "                 (sigmoid): QuantizedSigmoid(\n",
       "                   (param_quantizers): ModuleDict()\n",
       "                   (input_quantizers): ModuleList(\n",
       "                     (0): None\n",
       "                   )\n",
       "                   (output_quantizers): ModuleList(\n",
       "                     (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "                   )\n",
       "                 )\n",
       "                 (mul): QuantizedMultiply(\n",
       "                   (param_quantizers): ModuleDict()\n",
       "                   (input_quantizers): ModuleList(\n",
       "                     (0-1): 2 x None\n",
       "                   )\n",
       "                   (output_quantizers): ModuleList(\n",
       "                     (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "                   )\n",
       "                 )\n",
       "               )\n",
       "             )\n",
       "             (conv2): Module(\n",
       "               (depthwise_conv): Module(\n",
       "                 (conv): QuantizedConv2d(\n",
       "                   24, 24, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=24, bias=False\n",
       "                   (param_quantizers): ModuleDict(\n",
       "                     (weight): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=True)\n",
       "                   )\n",
       "                   (input_quantizers): ModuleList(\n",
       "                     (0): None\n",
       "                   )\n",
       "                   (output_quantizers): ModuleList(\n",
       "                     (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "                   )\n",
       "                 )\n",
       "                 (bn): Module(\n",
       "                   (module_batch_norm_7): BatchNorm()\n",
       "                 )\n",
       "                 (activate): CustomSiLU(\n",
       "                   (sigmoid): QuantizedSigmoid(\n",
       "                     (param_quantizers): ModuleDict()\n",
       "                     (input_quantizers): ModuleList(\n",
       "                       (0): None\n",
       "                     )\n",
       "                     (output_quantizers): ModuleList(\n",
       "                       (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "                     )\n",
       "                   )\n",
       "                   (mul): QuantizedMultiply(\n",
       "                     (param_quantizers): ModuleDict()\n",
       "                     (input_quantizers): ModuleList(\n",
       "                       (0-1): 2 x None\n",
       "                     )\n",
       "                     (output_quantizers): ModuleList(\n",
       "                       (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "                     )\n",
       "                   )\n",
       "                 )\n",
       "               )\n",
       "               (pointwise_conv): Module(\n",
       "                 (conv): QuantizedConv2d(\n",
       "                   24, 24, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "                   (param_quantizers): ModuleDict(\n",
       "                     (weight): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=True)\n",
       "                   )\n",
       "                   (input_quantizers): ModuleList(\n",
       "                     (0): None\n",
       "                   )\n",
       "                   (output_quantizers): ModuleList(\n",
       "                     (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "                   )\n",
       "                 )\n",
       "                 (bn): Module(\n",
       "                   (module_batch_norm_8): FakeQuantizedBatchNorm(\n",
       "                     (param_quantizers): ModuleDict()\n",
       "                     (input_quantizers): ModuleList(\n",
       "                       (0-4): 5 x QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "                     )\n",
       "                     (output_quantizers): ModuleList(\n",
       "                       (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "                     )\n",
       "                   )\n",
       "                 )\n",
       "                 (activate): CustomSiLU(\n",
       "                   (sigmoid): QuantizedSigmoid(\n",
       "                     (param_quantizers): ModuleDict()\n",
       "                     (input_quantizers): ModuleList(\n",
       "                       (0): None\n",
       "                     )\n",
       "                     (output_quantizers): ModuleList(\n",
       "                       (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "                     )\n",
       "                   )\n",
       "                   (mul): QuantizedMultiply(\n",
       "                     (param_quantizers): ModuleDict()\n",
       "                     (input_quantizers): ModuleList(\n",
       "                       (0-1): 2 x None\n",
       "                     )\n",
       "                     (output_quantizers): ModuleList(\n",
       "                       (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "                     )\n",
       "                   )\n",
       "                 )\n",
       "               )\n",
       "             )\n",
       "             (module_add): QuantizedAdd(\n",
       "               (param_quantizers): ModuleDict()\n",
       "               (input_quantizers): ModuleList(\n",
       "                 (0-1): 2 x None\n",
       "               )\n",
       "               (output_quantizers): ModuleList(\n",
       "                 (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "               )\n",
       "             )\n",
       "           )\n",
       "         )\n",
       "         (attention): Module(\n",
       "           (global_avgpool): FakeQuantizedAdaptiveAvgPool2d(\n",
       "             output_size=1\n",
       "             (param_quantizers): ModuleDict()\n",
       "             (input_quantizers): ModuleList(\n",
       "               (0): None\n",
       "             )\n",
       "             (output_quantizers): ModuleList(\n",
       "               (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "             )\n",
       "           )\n",
       "           (fc): QuantizedConv2d(\n",
       "             48, 48, kernel_size=(1, 1), stride=(1, 1)\n",
       "             (param_quantizers): ModuleDict(\n",
       "               (weight): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=True)\n",
       "               (bias): None\n",
       "             )\n",
       "             (input_quantizers): ModuleList(\n",
       "               (0): None\n",
       "             )\n",
       "             (output_quantizers): ModuleList(\n",
       "               (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "             )\n",
       "           )\n",
       "           (act): FakeQuantizedHardsigmoid(\n",
       "             (param_quantizers): ModuleDict()\n",
       "             (input_quantizers): ModuleList(\n",
       "               (0): None\n",
       "             )\n",
       "             (output_quantizers): ModuleList(\n",
       "               (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "             )\n",
       "           )\n",
       "           (module_mul): QuantizedMultiply(\n",
       "             (param_quantizers): ModuleDict()\n",
       "             (input_quantizers): ModuleList(\n",
       "               (0-1): 2 x None\n",
       "             )\n",
       "             (output_quantizers): ModuleList(\n",
       "               (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "             )\n",
       "           )\n",
       "         )\n",
       "         (final_conv): Module(\n",
       "           (conv): QuantizedConv2d(\n",
       "             48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "             (param_quantizers): ModuleDict(\n",
       "               (weight): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=True)\n",
       "             )\n",
       "             (input_quantizers): ModuleList(\n",
       "               (0): None\n",
       "             )\n",
       "             (output_quantizers): ModuleList(\n",
       "               (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "             )\n",
       "           )\n",
       "           (bn): Module(\n",
       "             (module_batch_norm_9): FakeQuantizedBatchNorm(\n",
       "               (param_quantizers): ModuleDict()\n",
       "               (input_quantizers): ModuleList(\n",
       "                 (0-4): 5 x QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "               )\n",
       "               (output_quantizers): ModuleList(\n",
       "                 (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "               )\n",
       "             )\n",
       "           )\n",
       "           (activate): CustomSiLU(\n",
       "             (sigmoid): QuantizedSigmoid(\n",
       "               (param_quantizers): ModuleDict()\n",
       "               (input_quantizers): ModuleList(\n",
       "                 (0): None\n",
       "               )\n",
       "               (output_quantizers): ModuleList(\n",
       "                 (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "               )\n",
       "             )\n",
       "             (mul): QuantizedMultiply(\n",
       "               (param_quantizers): ModuleDict()\n",
       "               (input_quantizers): ModuleList(\n",
       "                 (0-1): 2 x None\n",
       "               )\n",
       "               (output_quantizers): ModuleList(\n",
       "                 (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "               )\n",
       "             )\n",
       "           )\n",
       "         )\n",
       "         (module_cat): FakeQuantizedConcat(\n",
       "           (param_quantizers): ModuleDict()\n",
       "           (input_quantizers): ModuleList(\n",
       "             (0): None\n",
       "           )\n",
       "           (output_quantizers): ModuleList(\n",
       "             (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "           )\n",
       "         )\n",
       "       )\n",
       "     )\n",
       "     (stage2): Module(\n",
       "       (0): Module(\n",
       "         (conv): QuantizedConv2d(\n",
       "           48, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False\n",
       "           (param_quantizers): ModuleDict(\n",
       "             (weight): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=True)\n",
       "           )\n",
       "           (input_quantizers): ModuleList(\n",
       "             (0): None\n",
       "           )\n",
       "           (output_quantizers): ModuleList(\n",
       "             (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "           )\n",
       "         )\n",
       "         (bn): Module(\n",
       "           (module_batch_norm_10): FakeQuantizedBatchNorm(\n",
       "             (param_quantizers): ModuleDict()\n",
       "             (input_quantizers): ModuleList(\n",
       "               (0-4): 5 x QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "             )\n",
       "             (output_quantizers): ModuleList(\n",
       "               (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "             )\n",
       "           )\n",
       "         )\n",
       "         (activate): CustomSiLU(\n",
       "           (sigmoid): QuantizedSigmoid(\n",
       "             (param_quantizers): ModuleDict()\n",
       "             (input_quantizers): ModuleList(\n",
       "               (0): None\n",
       "             )\n",
       "             (output_quantizers): ModuleList(\n",
       "               (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "             )\n",
       "           )\n",
       "           (mul): QuantizedMultiply(\n",
       "             (param_quantizers): ModuleDict()\n",
       "             (input_quantizers): ModuleList(\n",
       "               (0-1): 2 x None\n",
       "             )\n",
       "             (output_quantizers): ModuleList(\n",
       "               (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "             )\n",
       "           )\n",
       "         )\n",
       "       )\n",
       "       (1): Module(\n",
       "         (short_conv): Module(\n",
       "           (conv): QuantizedConv2d(\n",
       "             96, 48, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "             (param_quantizers): ModuleDict(\n",
       "               (weight): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=True)\n",
       "             )\n",
       "             (input_quantizers): ModuleList(\n",
       "               (0): None\n",
       "             )\n",
       "             (output_quantizers): ModuleList(\n",
       "               (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "             )\n",
       "           )\n",
       "           (bn): Module(\n",
       "             (module_batch_norm_11): FakeQuantizedBatchNorm(\n",
       "               (param_quantizers): ModuleDict()\n",
       "               (input_quantizers): ModuleList(\n",
       "                 (0-4): 5 x QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "               )\n",
       "               (output_quantizers): ModuleList(\n",
       "                 (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "               )\n",
       "             )\n",
       "           )\n",
       "           (activate): CustomSiLU(\n",
       "             (sigmoid): QuantizedSigmoid(\n",
       "               (param_quantizers): ModuleDict()\n",
       "               (input_quantizers): ModuleList(\n",
       "                 (0): None\n",
       "               )\n",
       "               (output_quantizers): ModuleList(\n",
       "                 (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "               )\n",
       "             )\n",
       "             (mul): QuantizedMultiply(\n",
       "               (param_quantizers): ModuleDict()\n",
       "               (input_quantizers): ModuleList(\n",
       "                 (0-1): 2 x None\n",
       "               )\n",
       "               (output_quantizers): ModuleList(\n",
       "                 (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "               )\n",
       "             )\n",
       "           )\n",
       "         )\n",
       "         (main_conv): Module(\n",
       "           (conv): QuantizedConv2d(\n",
       "             96, 48, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "             (param_quantizers): ModuleDict(\n",
       "               (weight): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=True)\n",
       "             )\n",
       "             (input_quantizers): ModuleList(\n",
       "               (0): None\n",
       "             )\n",
       "             (output_quantizers): ModuleList(\n",
       "               (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "             )\n",
       "           )\n",
       "           (bn): Module(\n",
       "             (module_batch_norm_12): FakeQuantizedBatchNorm(\n",
       "               (param_quantizers): ModuleDict()\n",
       "               (input_quantizers): ModuleList(\n",
       "                 (0-4): 5 x QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "               )\n",
       "               (output_quantizers): ModuleList(\n",
       "                 (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "               )\n",
       "             )\n",
       "           )\n",
       "           (activate): CustomSiLU(\n",
       "             (sigmoid): QuantizedSigmoid(\n",
       "               (param_quantizers): ModuleDict()\n",
       "               (input_quantizers): ModuleList(\n",
       "                 (0): None\n",
       "               )\n",
       "               (output_quantizers): ModuleList(\n",
       "                 (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "               )\n",
       "             )\n",
       "             (mul): QuantizedMultiply(\n",
       "               (param_quantizers): ModuleDict()\n",
       "               (input_quantizers): ModuleList(\n",
       "                 (0-1): 2 x None\n",
       "               )\n",
       "               (output_quantizers): ModuleList(\n",
       "                 (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "               )\n",
       "             )\n",
       "           )\n",
       "         )\n",
       "         (blocks): Module(\n",
       "           (0): Module(\n",
       "             (conv1): Module(\n",
       "               (conv): QuantizedConv2d(\n",
       "                 48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "                 (param_quantizers): ModuleDict(\n",
       "                   (weight): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=True)\n",
       "                 )\n",
       "                 (input_quantizers): ModuleList(\n",
       "                   (0): None\n",
       "                 )\n",
       "                 (output_quantizers): ModuleList(\n",
       "                   (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "                 )\n",
       "               )\n",
       "               (bn): Module(\n",
       "                 (module_batch_norm_13): FakeQuantizedBatchNorm(\n",
       "                   (param_quantizers): ModuleDict()\n",
       "                   (input_quantizers): ModuleList(\n",
       "                     (0-4): 5 x QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "                   )\n",
       "                   (output_quantizers): ModuleList(\n",
       "                     (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "                   )\n",
       "                 )\n",
       "               )\n",
       "               (activate): CustomSiLU(\n",
       "                 (sigmoid): QuantizedSigmoid(\n",
       "                   (param_quantizers): ModuleDict()\n",
       "                   (input_quantizers): ModuleList(\n",
       "                     (0): None\n",
       "                   )\n",
       "                   (output_quantizers): ModuleList(\n",
       "                     (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "                   )\n",
       "                 )\n",
       "                 (mul): QuantizedMultiply(\n",
       "                   (param_quantizers): ModuleDict()\n",
       "                   (input_quantizers): ModuleList(\n",
       "                     (0-1): 2 x None\n",
       "                   )\n",
       "                   (output_quantizers): ModuleList(\n",
       "                     (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "                   )\n",
       "                 )\n",
       "               )\n",
       "             )\n",
       "             (conv2): Module(\n",
       "               (depthwise_conv): Module(\n",
       "                 (conv): QuantizedConv2d(\n",
       "                   48, 48, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=48, bias=False\n",
       "                   (param_quantizers): ModuleDict(\n",
       "                     (weight): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=True)\n",
       "                   )\n",
       "                   (input_quantizers): ModuleList(\n",
       "                     (0): None\n",
       "                   )\n",
       "                   (output_quantizers): ModuleList(\n",
       "                     (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "                   )\n",
       "                 )\n",
       "                 (bn): Module(\n",
       "                   (module_batch_norm_14): BatchNorm()\n",
       "                 )\n",
       "                 (activate): CustomSiLU(\n",
       "                   (sigmoid): QuantizedSigmoid(\n",
       "                     (param_quantizers): ModuleDict()\n",
       "                     (input_quantizers): ModuleList(\n",
       "                       (0): None\n",
       "                     )\n",
       "                     (output_quantizers): ModuleList(\n",
       "                       (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "                     )\n",
       "                   )\n",
       "                   (mul): QuantizedMultiply(\n",
       "                     (param_quantizers): ModuleDict()\n",
       "                     (input_quantizers): ModuleList(\n",
       "                       (0-1): 2 x None\n",
       "                     )\n",
       "                     (output_quantizers): ModuleList(\n",
       "                       (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "                     )\n",
       "                   )\n",
       "                 )\n",
       "               )\n",
       "               (pointwise_conv): Module(\n",
       "                 (conv): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                 (bn): Module(\n",
       "                   (module_batch_norm_15): FakeQuantizedBatchNorm(\n",
       "                     (param_quantizers): ModuleDict()\n",
       "                     (input_quantizers): ModuleList(\n",
       "                       (0-4): 5 x QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "                     )\n",
       "                     (output_quantizers): ModuleList(\n",
       "                       (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "                     )\n",
       "                   )\n",
       "                 )\n",
       "                 (activate): CustomSiLU(\n",
       "                   (sigmoid): QuantizedSigmoid(\n",
       "                     (param_quantizers): ModuleDict()\n",
       "                     (input_quantizers): ModuleList(\n",
       "                       (0): None\n",
       "                     )\n",
       "                     (output_quantizers): ModuleList(\n",
       "                       (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "                     )\n",
       "                   )\n",
       "                   (mul): QuantizedMultiply(\n",
       "                     (param_quantizers): ModuleDict()\n",
       "                     (input_quantizers): ModuleList(\n",
       "                       (0-1): 2 x None\n",
       "                     )\n",
       "                     (output_quantizers): ModuleList(\n",
       "                       (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "                     )\n",
       "                   )\n",
       "                 )\n",
       "               )\n",
       "             )\n",
       "             (module_add_1): QuantizedAdd(\n",
       "               (param_quantizers): ModuleDict()\n",
       "               (input_quantizers): ModuleList(\n",
       "                 (0-1): 2 x None\n",
       "               )\n",
       "               (output_quantizers): ModuleList(\n",
       "                 (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "               )\n",
       "             )\n",
       "           )\n",
       "         )\n",
       "         (attention): Module(\n",
       "           (global_avgpool): FakeQuantizedAdaptiveAvgPool2d(\n",
       "             output_size=1\n",
       "             (param_quantizers): ModuleDict()\n",
       "             (input_quantizers): ModuleList(\n",
       "               (0): None\n",
       "             )\n",
       "             (output_quantizers): ModuleList(\n",
       "               (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "             )\n",
       "           )\n",
       "           (fc): QuantizedConv2d(\n",
       "             96, 96, kernel_size=(1, 1), stride=(1, 1)\n",
       "             (param_quantizers): ModuleDict(\n",
       "               (weight): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=True)\n",
       "               (bias): None\n",
       "             )\n",
       "             (input_quantizers): ModuleList(\n",
       "               (0): None\n",
       "             )\n",
       "             (output_quantizers): ModuleList(\n",
       "               (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "             )\n",
       "           )\n",
       "           (act): FakeQuantizedHardsigmoid(\n",
       "             (param_quantizers): ModuleDict()\n",
       "             (input_quantizers): ModuleList(\n",
       "               (0): None\n",
       "             )\n",
       "             (output_quantizers): ModuleList(\n",
       "               (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "             )\n",
       "           )\n",
       "           (module_mul_1): QuantizedMultiply(\n",
       "             (param_quantizers): ModuleDict()\n",
       "             (input_quantizers): ModuleList(\n",
       "               (0-1): 2 x None\n",
       "             )\n",
       "             (output_quantizers): ModuleList(\n",
       "               (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "             )\n",
       "           )\n",
       "         )\n",
       "         (final_conv): Module(\n",
       "           (conv): QuantizedConv2d(\n",
       "             96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "             (param_quantizers): ModuleDict(\n",
       "               (weight): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=True)\n",
       "             )\n",
       "             (input_quantizers): ModuleList(\n",
       "               (0): None\n",
       "             )\n",
       "             (output_quantizers): ModuleList(\n",
       "               (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "             )\n",
       "           )\n",
       "           (bn): Module(\n",
       "             (module_batch_norm_16): FakeQuantizedBatchNorm(\n",
       "               (param_quantizers): ModuleDict()\n",
       "               (input_quantizers): ModuleList(\n",
       "                 (0-4): 5 x QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "               )\n",
       "               (output_quantizers): ModuleList(\n",
       "                 (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "               )\n",
       "             )\n",
       "           )\n",
       "           (activate): CustomSiLU(\n",
       "             (sigmoid): QuantizedSigmoid(\n",
       "               (param_quantizers): ModuleDict()\n",
       "               (input_quantizers): ModuleList(\n",
       "                 (0): None\n",
       "               )\n",
       "               (output_quantizers): ModuleList(\n",
       "                 (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "               )\n",
       "             )\n",
       "             (mul): QuantizedMultiply(\n",
       "               (param_quantizers): ModuleDict()\n",
       "               (input_quantizers): ModuleList(\n",
       "                 (0-1): 2 x None\n",
       "               )\n",
       "               (output_quantizers): ModuleList(\n",
       "                 (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "               )\n",
       "             )\n",
       "           )\n",
       "         )\n",
       "         (module_cat_1): FakeQuantizedConcat(\n",
       "           (param_quantizers): ModuleDict()\n",
       "           (input_quantizers): ModuleList(\n",
       "             (0): None\n",
       "           )\n",
       "           (output_quantizers): ModuleList(\n",
       "             (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "           )\n",
       "         )\n",
       "       )\n",
       "     )\n",
       "     (stage3): Module(\n",
       "       (0): Module(\n",
       "         (conv): QuantizedConv2d(\n",
       "           96, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False\n",
       "           (param_quantizers): ModuleDict(\n",
       "             (weight): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=True)\n",
       "           )\n",
       "           (input_quantizers): ModuleList(\n",
       "             (0): None\n",
       "           )\n",
       "           (output_quantizers): ModuleList(\n",
       "             (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "           )\n",
       "         )\n",
       "         (bn): Module(\n",
       "           (module_batch_norm_17): FakeQuantizedBatchNorm(\n",
       "             (param_quantizers): ModuleDict()\n",
       "             (input_quantizers): ModuleList(\n",
       "               (0-4): 5 x QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "             )\n",
       "             (output_quantizers): ModuleList(\n",
       "               (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "             )\n",
       "           )\n",
       "         )\n",
       "         (activate): CustomSiLU(\n",
       "           (sigmoid): QuantizedSigmoid(\n",
       "             (param_quantizers): ModuleDict()\n",
       "             (input_quantizers): ModuleList(\n",
       "               (0): None\n",
       "             )\n",
       "             (output_quantizers): ModuleList(\n",
       "               (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "             )\n",
       "           )\n",
       "           (mul): QuantizedMultiply(\n",
       "             (param_quantizers): ModuleDict()\n",
       "             (input_quantizers): ModuleList(\n",
       "               (0-1): 2 x None\n",
       "             )\n",
       "             (output_quantizers): ModuleList(\n",
       "               (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "             )\n",
       "           )\n",
       "         )\n",
       "       )\n",
       "       (1): Module(\n",
       "         (short_conv): Module(\n",
       "           (conv): QuantizedConv2d(\n",
       "             192, 96, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "             (param_quantizers): ModuleDict(\n",
       "               (weight): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=True)\n",
       "             )\n",
       "             (input_quantizers): ModuleList(\n",
       "               (0): None\n",
       "             )\n",
       "             (output_quantizers): ModuleList(\n",
       "               (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "             )\n",
       "           )\n",
       "           (bn): Module(\n",
       "             (module_batch_norm_18): FakeQuantizedBatchNorm(\n",
       "               (param_quantizers): ModuleDict()\n",
       "               (input_quantizers): ModuleList(\n",
       "                 (0-4): 5 x QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "               )\n",
       "               (output_quantizers): ModuleList(\n",
       "                 (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "               )\n",
       "             )\n",
       "           )\n",
       "           (activate): CustomSiLU(\n",
       "             (sigmoid): QuantizedSigmoid(\n",
       "               (param_quantizers): ModuleDict()\n",
       "               (input_quantizers): ModuleList(\n",
       "                 (0): None\n",
       "               )\n",
       "               (output_quantizers): ModuleList(\n",
       "                 (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "               )\n",
       "             )\n",
       "             (mul): QuantizedMultiply(\n",
       "               (param_quantizers): ModuleDict()\n",
       "               (input_quantizers): ModuleList(\n",
       "                 (0-1): 2 x None\n",
       "               )\n",
       "               (output_quantizers): ModuleList(\n",
       "                 (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "               )\n",
       "             )\n",
       "           )\n",
       "         )\n",
       "         (main_conv): Module(\n",
       "           (conv): QuantizedConv2d(\n",
       "             192, 96, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "             (param_quantizers): ModuleDict(\n",
       "               (weight): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=True)\n",
       "             )\n",
       "             (input_quantizers): ModuleList(\n",
       "               (0): None\n",
       "             )\n",
       "             (output_quantizers): ModuleList(\n",
       "               (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "             )\n",
       "           )\n",
       "           (bn): Module(\n",
       "             (module_batch_norm_19): FakeQuantizedBatchNorm(\n",
       "               (param_quantizers): ModuleDict()\n",
       "               (input_quantizers): ModuleList(\n",
       "                 (0-4): 5 x QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "               )\n",
       "               (output_quantizers): ModuleList(\n",
       "                 (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "               )\n",
       "             )\n",
       "           )\n",
       "           (activate): CustomSiLU(\n",
       "             (sigmoid): QuantizedSigmoid(\n",
       "               (param_quantizers): ModuleDict()\n",
       "               (input_quantizers): ModuleList(\n",
       "                 (0): None\n",
       "               )\n",
       "               (output_quantizers): ModuleList(\n",
       "                 (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "               )\n",
       "             )\n",
       "             (mul): QuantizedMultiply(\n",
       "               (param_quantizers): ModuleDict()\n",
       "               (input_quantizers): ModuleList(\n",
       "                 (0-1): 2 x None\n",
       "               )\n",
       "               (output_quantizers): ModuleList(\n",
       "                 (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "               )\n",
       "             )\n",
       "           )\n",
       "         )\n",
       "         (blocks): Module(\n",
       "           (0): Module(\n",
       "             (conv1): Module(\n",
       "               (conv): QuantizedConv2d(\n",
       "                 96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "                 (param_quantizers): ModuleDict(\n",
       "                   (weight): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=True)\n",
       "                 )\n",
       "                 (input_quantizers): ModuleList(\n",
       "                   (0): None\n",
       "                 )\n",
       "                 (output_quantizers): ModuleList(\n",
       "                   (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "                 )\n",
       "               )\n",
       "               (bn): Module(\n",
       "                 (module_batch_norm_20): FakeQuantizedBatchNorm(\n",
       "                   (param_quantizers): ModuleDict()\n",
       "                   (input_quantizers): ModuleList(\n",
       "                     (0-4): 5 x QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "                   )\n",
       "                   (output_quantizers): ModuleList(\n",
       "                     (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "                   )\n",
       "                 )\n",
       "               )\n",
       "               (activate): CustomSiLU(\n",
       "                 (sigmoid): QuantizedSigmoid(\n",
       "                   (param_quantizers): ModuleDict()\n",
       "                   (input_quantizers): ModuleList(\n",
       "                     (0): None\n",
       "                   )\n",
       "                   (output_quantizers): ModuleList(\n",
       "                     (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "                   )\n",
       "                 )\n",
       "                 (mul): QuantizedMultiply(\n",
       "                   (param_quantizers): ModuleDict()\n",
       "                   (input_quantizers): ModuleList(\n",
       "                     (0-1): 2 x None\n",
       "                   )\n",
       "                   (output_quantizers): ModuleList(\n",
       "                     (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "                   )\n",
       "                 )\n",
       "               )\n",
       "             )\n",
       "             (conv2): Module(\n",
       "               (depthwise_conv): Module(\n",
       "                 (conv): QuantizedConv2d(\n",
       "                   96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96, bias=False\n",
       "                   (param_quantizers): ModuleDict(\n",
       "                     (weight): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=True)\n",
       "                   )\n",
       "                   (input_quantizers): ModuleList(\n",
       "                     (0): None\n",
       "                   )\n",
       "                   (output_quantizers): ModuleList(\n",
       "                     (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "                   )\n",
       "                 )\n",
       "                 (bn): Module(\n",
       "                   (module_batch_norm_21): BatchNorm()\n",
       "                 )\n",
       "                 (activate): CustomSiLU(\n",
       "                   (sigmoid): QuantizedSigmoid(\n",
       "                     (param_quantizers): ModuleDict()\n",
       "                     (input_quantizers): ModuleList(\n",
       "                       (0): None\n",
       "                     )\n",
       "                     (output_quantizers): ModuleList(\n",
       "                       (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "                     )\n",
       "                   )\n",
       "                   (mul): QuantizedMultiply(\n",
       "                     (param_quantizers): ModuleDict()\n",
       "                     (input_quantizers): ModuleList(\n",
       "                       (0-1): 2 x None\n",
       "                     )\n",
       "                     (output_quantizers): ModuleList(\n",
       "                       (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "                     )\n",
       "                   )\n",
       "                 )\n",
       "               )\n",
       "               (pointwise_conv): Module(\n",
       "                 (conv): QuantizedConv2d(\n",
       "                   96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "                   (param_quantizers): ModuleDict(\n",
       "                     (weight): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=True)\n",
       "                   )\n",
       "                   (input_quantizers): ModuleList(\n",
       "                     (0): None\n",
       "                   )\n",
       "                   (output_quantizers): ModuleList(\n",
       "                     (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "                   )\n",
       "                 )\n",
       "                 (bn): Module(\n",
       "                   (module_batch_norm_22): FakeQuantizedBatchNorm(\n",
       "                     (param_quantizers): ModuleDict()\n",
       "                     (input_quantizers): ModuleList(\n",
       "                       (0-4): 5 x QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "                     )\n",
       "                     (output_quantizers): ModuleList(\n",
       "                       (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "                     )\n",
       "                   )\n",
       "                 )\n",
       "                 (activate): CustomSiLU(\n",
       "                   (sigmoid): QuantizedSigmoid(\n",
       "                     (param_quantizers): ModuleDict()\n",
       "                     (input_quantizers): ModuleList(\n",
       "                       (0): None\n",
       "                     )\n",
       "                     (output_quantizers): ModuleList(\n",
       "                       (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "                     )\n",
       "                   )\n",
       "                   (mul): QuantizedMultiply(\n",
       "                     (param_quantizers): ModuleDict()\n",
       "                     (input_quantizers): ModuleList(\n",
       "                       (0-1): 2 x None\n",
       "                     )\n",
       "                     (output_quantizers): ModuleList(\n",
       "                       (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "                     )\n",
       "                   )\n",
       "                 )\n",
       "               )\n",
       "             )\n",
       "             (module_add_2): QuantizedAdd(\n",
       "               (param_quantizers): ModuleDict()\n",
       "               (input_quantizers): ModuleList(\n",
       "                 (0-1): 2 x None\n",
       "               )\n",
       "               (output_quantizers): ModuleList(\n",
       "                 (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "               )\n",
       "             )\n",
       "           )\n",
       "         )\n",
       "         (attention): Module(\n",
       "           (global_avgpool): FakeQuantizedAdaptiveAvgPool2d(\n",
       "             output_size=1\n",
       "             (param_quantizers): ModuleDict()\n",
       "             (input_quantizers): ModuleList(\n",
       "               (0): None\n",
       "             )\n",
       "             (output_quantizers): ModuleList(\n",
       "               (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "             )\n",
       "           )\n",
       "           (fc): QuantizedConv2d(\n",
       "             192, 192, kernel_size=(1, 1), stride=(1, 1)\n",
       "             (param_quantizers): ModuleDict(\n",
       "               (weight): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=True)\n",
       "               (bias): None\n",
       "             )\n",
       "             (input_quantizers): ModuleList(\n",
       "               (0): None\n",
       "             )\n",
       "             (output_quantizers): ModuleList(\n",
       "               (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "             )\n",
       "           )\n",
       "           (act): FakeQuantizedHardsigmoid(\n",
       "             (param_quantizers): ModuleDict()\n",
       "             (input_quantizers): ModuleList(\n",
       "               (0): None\n",
       "             )\n",
       "             (output_quantizers): ModuleList(\n",
       "               (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "             )\n",
       "           )\n",
       "           (module_mul_2): QuantizedMultiply(\n",
       "             (param_quantizers): ModuleDict()\n",
       "             (input_quantizers): ModuleList(\n",
       "               (0-1): 2 x None\n",
       "             )\n",
       "             (output_quantizers): ModuleList(\n",
       "               (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "             )\n",
       "           )\n",
       "         )\n",
       "         (final_conv): Module(\n",
       "           (conv): QuantizedConv2d(\n",
       "             192, 192, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "             (param_quantizers): ModuleDict(\n",
       "               (weight): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=True)\n",
       "             )\n",
       "             (input_quantizers): ModuleList(\n",
       "               (0): None\n",
       "             )\n",
       "             (output_quantizers): ModuleList(\n",
       "               (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "             )\n",
       "           )\n",
       "           (bn): Module(\n",
       "             (module_batch_norm_23): FakeQuantizedBatchNorm(\n",
       "               (param_quantizers): ModuleDict()\n",
       "               (input_quantizers): ModuleList(\n",
       "                 (0-4): 5 x QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "               )\n",
       "               (output_quantizers): ModuleList(\n",
       "                 (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "               )\n",
       "             )\n",
       "           )\n",
       "           (activate): CustomSiLU(\n",
       "             (sigmoid): QuantizedSigmoid(\n",
       "               (param_quantizers): ModuleDict()\n",
       "               (input_quantizers): ModuleList(\n",
       "                 (0): None\n",
       "               )\n",
       "               (output_quantizers): ModuleList(\n",
       "                 (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "               )\n",
       "             )\n",
       "             (mul): QuantizedMultiply(\n",
       "               (param_quantizers): ModuleDict()\n",
       "               (input_quantizers): ModuleList(\n",
       "                 (0-1): 2 x None\n",
       "               )\n",
       "               (output_quantizers): ModuleList(\n",
       "                 (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "               )\n",
       "             )\n",
       "           )\n",
       "         )\n",
       "         (module_cat_2): FakeQuantizedConcat(\n",
       "           (param_quantizers): ModuleDict()\n",
       "           (input_quantizers): ModuleList(\n",
       "             (0): None\n",
       "           )\n",
       "           (output_quantizers): ModuleList(\n",
       "             (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "           )\n",
       "         )\n",
       "       )\n",
       "     )\n",
       "     (stage4): Module(\n",
       "       (0): Module(\n",
       "         (conv): QuantizedConv2d(\n",
       "           192, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False\n",
       "           (param_quantizers): ModuleDict(\n",
       "             (weight): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=True)\n",
       "           )\n",
       "           (input_quantizers): ModuleList(\n",
       "             (0): None\n",
       "           )\n",
       "           (output_quantizers): ModuleList(\n",
       "             (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "           )\n",
       "         )\n",
       "         (bn): Module(\n",
       "           (module_batch_norm_24): FakeQuantizedBatchNorm(\n",
       "             (param_quantizers): ModuleDict()\n",
       "             (input_quantizers): ModuleList(\n",
       "               (0-4): 5 x QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "             )\n",
       "             (output_quantizers): ModuleList(\n",
       "               (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "             )\n",
       "           )\n",
       "         )\n",
       "         (activate): CustomSiLU(\n",
       "           (sigmoid): QuantizedSigmoid(\n",
       "             (param_quantizers): ModuleDict()\n",
       "             (input_quantizers): ModuleList(\n",
       "               (0): None\n",
       "             )\n",
       "             (output_quantizers): ModuleList(\n",
       "               (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "             )\n",
       "           )\n",
       "           (mul): QuantizedMultiply(\n",
       "             (param_quantizers): ModuleDict()\n",
       "             (input_quantizers): ModuleList(\n",
       "               (0-1): 2 x None\n",
       "             )\n",
       "             (output_quantizers): ModuleList(\n",
       "               (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "             )\n",
       "           )\n",
       "         )\n",
       "       )\n",
       "       (1): Module(\n",
       "         (conv1): Module(\n",
       "           (conv): QuantizedConv2d(\n",
       "             384, 192, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "             (param_quantizers): ModuleDict(\n",
       "               (weight): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=True)\n",
       "             )\n",
       "             (input_quantizers): ModuleList(\n",
       "               (0): None\n",
       "             )\n",
       "             (output_quantizers): ModuleList(\n",
       "               (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "             )\n",
       "           )\n",
       "           (bn): Module(\n",
       "             (module_batch_norm_25): FakeQuantizedBatchNorm(\n",
       "               (param_quantizers): ModuleDict()\n",
       "               (input_quantizers): ModuleList(\n",
       "                 (0-4): 5 x QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "               )\n",
       "               (output_quantizers): ModuleList(\n",
       "                 (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "               )\n",
       "             )\n",
       "           )\n",
       "           (activate): CustomSiLU(\n",
       "             (sigmoid): QuantizedSigmoid(\n",
       "               (param_quantizers): ModuleDict()\n",
       "               (input_quantizers): ModuleList(\n",
       "                 (0): None\n",
       "               )\n",
       "               (output_quantizers): ModuleList(\n",
       "                 (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "               )\n",
       "             )\n",
       "             (mul): QuantizedMultiply(\n",
       "               (param_quantizers): ModuleDict()\n",
       "               (input_quantizers): ModuleList(\n",
       "                 (0-1): 2 x None\n",
       "               )\n",
       "               (output_quantizers): ModuleList(\n",
       "                 (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "               )\n",
       "             )\n",
       "           )\n",
       "         )\n",
       "         (poolings): Module(\n",
       "           (0): FakeQuantizedMaxPool2d(\n",
       "             kernel_size=5, stride=1, padding=2, dilation=1, ceil_mode=False\n",
       "             (param_quantizers): ModuleDict()\n",
       "             (input_quantizers): ModuleList(\n",
       "               (0): None\n",
       "             )\n",
       "             (output_quantizers): ModuleList(\n",
       "               (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "             )\n",
       "           )\n",
       "           (1): FakeQuantizedMaxPool2d(\n",
       "             kernel_size=9, stride=1, padding=4, dilation=1, ceil_mode=False\n",
       "             (param_quantizers): ModuleDict()\n",
       "             (input_quantizers): ModuleList(\n",
       "               (0): None\n",
       "             )\n",
       "             (output_quantizers): ModuleList(\n",
       "               (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "             )\n",
       "           )\n",
       "           (2): FakeQuantizedMaxPool2d(\n",
       "             kernel_size=13, stride=1, padding=6, dilation=1, ceil_mode=False\n",
       "             (param_quantizers): ModuleDict()\n",
       "             (input_quantizers): ModuleList(\n",
       "               (0): None\n",
       "             )\n",
       "             (output_quantizers): ModuleList(\n",
       "               (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "             )\n",
       "           )\n",
       "         )\n",
       "         (conv2): Module(\n",
       "           (conv): QuantizedConv2d(\n",
       "             768, 384, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "             (param_quantizers): ModuleDict(\n",
       "               (weight): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=True)\n",
       "             )\n",
       "             (input_quantizers): ModuleList(\n",
       "               (0): None\n",
       "             )\n",
       "             (output_quantizers): ModuleList(\n",
       "               (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "             )\n",
       "           )\n",
       "           (bn): Module(\n",
       "             (module_batch_norm_26): FakeQuantizedBatchNorm(\n",
       "               (param_quantizers): ModuleDict()\n",
       "               (input_quantizers): ModuleList(\n",
       "                 (0-4): 5 x QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "               )\n",
       "               (output_quantizers): ModuleList(\n",
       "                 (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "               )\n",
       "             )\n",
       "           )\n",
       "           (activate): CustomSiLU(\n",
       "             (sigmoid): QuantizedSigmoid(\n",
       "               (param_quantizers): ModuleDict()\n",
       "               (input_quantizers): ModuleList(\n",
       "                 (0): None\n",
       "               )\n",
       "               (output_quantizers): ModuleList(\n",
       "                 (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "               )\n",
       "             )\n",
       "             (mul): QuantizedMultiply(\n",
       "               (param_quantizers): ModuleDict()\n",
       "               (input_quantizers): ModuleList(\n",
       "                 (0-1): 2 x None\n",
       "               )\n",
       "               (output_quantizers): ModuleList(\n",
       "                 (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "               )\n",
       "             )\n",
       "           )\n",
       "         )\n",
       "         (module_cat_3): FakeQuantizedConcat(\n",
       "           (param_quantizers): ModuleDict()\n",
       "           (input_quantizers): ModuleList(\n",
       "             (0): None\n",
       "           )\n",
       "           (output_quantizers): ModuleList(\n",
       "             (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "           )\n",
       "         )\n",
       "       )\n",
       "       (2): Module(\n",
       "         (short_conv): Module(\n",
       "           (conv): QuantizedConv2d(\n",
       "             384, 192, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "             (param_quantizers): ModuleDict(\n",
       "               (weight): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=True)\n",
       "             )\n",
       "             (input_quantizers): ModuleList(\n",
       "               (0): None\n",
       "             )\n",
       "             (output_quantizers): ModuleList(\n",
       "               (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "             )\n",
       "           )\n",
       "           (bn): Module(\n",
       "             (module_batch_norm_27): FakeQuantizedBatchNorm(\n",
       "               (param_quantizers): ModuleDict()\n",
       "               (input_quantizers): ModuleList(\n",
       "                 (0-4): 5 x QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "               )\n",
       "               (output_quantizers): ModuleList(\n",
       "                 (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "               )\n",
       "             )\n",
       "           )\n",
       "           (activate): CustomSiLU(\n",
       "             (sigmoid): QuantizedSigmoid(\n",
       "               (param_quantizers): ModuleDict()\n",
       "               (input_quantizers): ModuleList(\n",
       "                 (0): None\n",
       "               )\n",
       "               (output_quantizers): ModuleList(\n",
       "                 (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "               )\n",
       "             )\n",
       "             (mul): QuantizedMultiply(\n",
       "               (param_quantizers): ModuleDict()\n",
       "               (input_quantizers): ModuleList(\n",
       "                 (0-1): 2 x None\n",
       "               )\n",
       "               (output_quantizers): ModuleList(\n",
       "                 (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "               )\n",
       "             )\n",
       "           )\n",
       "         )\n",
       "         (main_conv): Module(\n",
       "           (conv): QuantizedConv2d(\n",
       "             384, 192, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "             (param_quantizers): ModuleDict(\n",
       "               (weight): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=True)\n",
       "             )\n",
       "             (input_quantizers): ModuleList(\n",
       "               (0): None\n",
       "             )\n",
       "             (output_quantizers): ModuleList(\n",
       "               (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "             )\n",
       "           )\n",
       "           (bn): Module(\n",
       "             (module_batch_norm_28): FakeQuantizedBatchNorm(\n",
       "               (param_quantizers): ModuleDict()\n",
       "               (input_quantizers): ModuleList(\n",
       "                 (0-4): 5 x QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "               )\n",
       "               (output_quantizers): ModuleList(\n",
       "                 (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "               )\n",
       "             )\n",
       "           )\n",
       "           (activate): CustomSiLU(\n",
       "             (sigmoid): QuantizedSigmoid(\n",
       "               (param_quantizers): ModuleDict()\n",
       "               (input_quantizers): ModuleList(\n",
       "                 (0): None\n",
       "               )\n",
       "               (output_quantizers): ModuleList(\n",
       "                 (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "               )\n",
       "             )\n",
       "             (mul): QuantizedMultiply(\n",
       "               (param_quantizers): ModuleDict()\n",
       "               (input_quantizers): ModuleList(\n",
       "                 (0-1): 2 x None\n",
       "               )\n",
       "               (output_quantizers): ModuleList(\n",
       "                 (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "               )\n",
       "             )\n",
       "           )\n",
       "         )\n",
       "         (blocks): Module(\n",
       "           (0): Module(\n",
       "             (conv1): Module(\n",
       "               (conv): QuantizedConv2d(\n",
       "                 192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "                 (param_quantizers): ModuleDict(\n",
       "                   (weight): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=True)\n",
       "                 )\n",
       "                 (input_quantizers): ModuleList(\n",
       "                   (0): None\n",
       "                 )\n",
       "                 (output_quantizers): ModuleList(\n",
       "                   (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "                 )\n",
       "               )\n",
       "               (bn): Module(\n",
       "                 (module_batch_norm_29): FakeQuantizedBatchNorm(\n",
       "                   (param_quantizers): ModuleDict()\n",
       "                   (input_quantizers): ModuleList(\n",
       "                     (0-4): 5 x QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "                   )\n",
       "                   (output_quantizers): ModuleList(\n",
       "                     (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "                   )\n",
       "                 )\n",
       "               )\n",
       "               (activate): CustomSiLU(\n",
       "                 (sigmoid): QuantizedSigmoid(\n",
       "                   (param_quantizers): ModuleDict()\n",
       "                   (input_quantizers): ModuleList(\n",
       "                     (0): None\n",
       "                   )\n",
       "                   (output_quantizers): ModuleList(\n",
       "                     (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "                   )\n",
       "                 )\n",
       "                 (mul): QuantizedMultiply(\n",
       "                   (param_quantizers): ModuleDict()\n",
       "                   (input_quantizers): ModuleList(\n",
       "                     (0-1): 2 x None\n",
       "                   )\n",
       "                   (output_quantizers): ModuleList(\n",
       "                     (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "                   )\n",
       "                 )\n",
       "               )\n",
       "             )\n",
       "             (conv2): Module(\n",
       "               (depthwise_conv): Module(\n",
       "                 (conv): QuantizedConv2d(\n",
       "                   192, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=192, bias=False\n",
       "                   (param_quantizers): ModuleDict(\n",
       "                     (weight): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=True)\n",
       "                   )\n",
       "                   (input_quantizers): ModuleList(\n",
       "                     (0): None\n",
       "                   )\n",
       "                   (output_quantizers): ModuleList(\n",
       "                     (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "                   )\n",
       "                 )\n",
       "                 (bn): Module(\n",
       "                   (module_batch_norm_30): BatchNorm()\n",
       "                 )\n",
       "                 (activate): CustomSiLU(\n",
       "                   (sigmoid): QuantizedSigmoid(\n",
       "                     (param_quantizers): ModuleDict()\n",
       "                     (input_quantizers): ModuleList(\n",
       "                       (0): None\n",
       "                     )\n",
       "                     (output_quantizers): ModuleList(\n",
       "                       (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "                     )\n",
       "                   )\n",
       "                   (mul): QuantizedMultiply(\n",
       "                     (param_quantizers): ModuleDict()\n",
       "                     (input_quantizers): ModuleList(\n",
       "                       (0-1): 2 x None\n",
       "                     )\n",
       "                     (output_quantizers): ModuleList(\n",
       "                       (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "                     )\n",
       "                   )\n",
       "                 )\n",
       "               )\n",
       "               (pointwise_conv): Module(\n",
       "                 (conv): QuantizedConv2d(\n",
       "                   192, 192, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "                   (param_quantizers): ModuleDict(\n",
       "                     (weight): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=True)\n",
       "                   )\n",
       "                   (input_quantizers): ModuleList(\n",
       "                     (0): None\n",
       "                   )\n",
       "                   (output_quantizers): ModuleList(\n",
       "                     (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "                   )\n",
       "                 )\n",
       "                 (bn): Module(\n",
       "                   (module_batch_norm_31): FakeQuantizedBatchNorm(\n",
       "                     (param_quantizers): ModuleDict()\n",
       "                     (input_quantizers): ModuleList(\n",
       "                       (0-4): 5 x QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "                     )\n",
       "                     (output_quantizers): ModuleList(\n",
       "                       (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "                     )\n",
       "                   )\n",
       "                 )\n",
       "                 (activate): CustomSiLU(\n",
       "                   (sigmoid): QuantizedSigmoid(\n",
       "                     (param_quantizers): ModuleDict()\n",
       "                     (input_quantizers): ModuleList(\n",
       "                       (0): None\n",
       "                     )\n",
       "                     (output_quantizers): ModuleList(\n",
       "                       (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "                     )\n",
       "                   )\n",
       "                   (mul): QuantizedMultiply(\n",
       "                     (param_quantizers): ModuleDict()\n",
       "                     (input_quantizers): ModuleList(\n",
       "                       (0-1): 2 x None\n",
       "                     )\n",
       "                     (output_quantizers): ModuleList(\n",
       "                       (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "                     )\n",
       "                   )\n",
       "                 )\n",
       "               )\n",
       "             )\n",
       "           )\n",
       "         )\n",
       "         (attention): Module(\n",
       "           (global_avgpool): FakeQuantizedAdaptiveAvgPool2d(\n",
       "             output_size=1\n",
       "             (param_quantizers): ModuleDict()\n",
       "             (input_quantizers): ModuleList(\n",
       "               (0): None\n",
       "             )\n",
       "             (output_quantizers): ModuleList(\n",
       "               (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "             )\n",
       "           )\n",
       "           (fc): QuantizedConv2d(\n",
       "             384, 384, kernel_size=(1, 1), stride=(1, 1)\n",
       "             (param_quantizers): ModuleDict(\n",
       "               (weight): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=True)\n",
       "               (bias): None\n",
       "             )\n",
       "             (input_quantizers): ModuleList(\n",
       "               (0): None\n",
       "             )\n",
       "             (output_quantizers): ModuleList(\n",
       "               (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "             )\n",
       "           )\n",
       "           (act): FakeQuantizedHardsigmoid(\n",
       "             (param_quantizers): ModuleDict()\n",
       "             (input_quantizers): ModuleList(\n",
       "               (0): None\n",
       "             )\n",
       "             (output_quantizers): ModuleList(\n",
       "               (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "             )\n",
       "           )\n",
       "           (module_mul_3): QuantizedMultiply(\n",
       "             (param_quantizers): ModuleDict()\n",
       "             (input_quantizers): ModuleList(\n",
       "               (0-1): 2 x None\n",
       "             )\n",
       "             (output_quantizers): ModuleList(\n",
       "               (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "             )\n",
       "           )\n",
       "         )\n",
       "         (final_conv): Module(\n",
       "           (conv): QuantizedConv2d(\n",
       "             384, 384, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "             (param_quantizers): ModuleDict(\n",
       "               (weight): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=True)\n",
       "             )\n",
       "             (input_quantizers): ModuleList(\n",
       "               (0): None\n",
       "             )\n",
       "             (output_quantizers): ModuleList(\n",
       "               (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "             )\n",
       "           )\n",
       "           (bn): Module(\n",
       "             (module_batch_norm_32): FakeQuantizedBatchNorm(\n",
       "               (param_quantizers): ModuleDict()\n",
       "               (input_quantizers): ModuleList(\n",
       "                 (0-4): 5 x QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "               )\n",
       "               (output_quantizers): ModuleList(\n",
       "                 (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "               )\n",
       "             )\n",
       "           )\n",
       "           (activate): CustomSiLU(\n",
       "             (sigmoid): QuantizedSigmoid(\n",
       "               (param_quantizers): ModuleDict()\n",
       "               (input_quantizers): ModuleList(\n",
       "                 (0): None\n",
       "               )\n",
       "               (output_quantizers): ModuleList(\n",
       "                 (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "               )\n",
       "             )\n",
       "             (mul): QuantizedMultiply(\n",
       "               (param_quantizers): ModuleDict()\n",
       "               (input_quantizers): ModuleList(\n",
       "                 (0-1): 2 x None\n",
       "               )\n",
       "               (output_quantizers): ModuleList(\n",
       "                 (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "               )\n",
       "             )\n",
       "           )\n",
       "         )\n",
       "         (module_cat_4): FakeQuantizedConcat(\n",
       "           (param_quantizers): ModuleDict()\n",
       "           (input_quantizers): ModuleList(\n",
       "             (0): None\n",
       "           )\n",
       "           (output_quantizers): ModuleList(\n",
       "             (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "           )\n",
       "         )\n",
       "       )\n",
       "     )\n",
       "   )\n",
       "   (neck): Module(\n",
       "     (reduce_layers): Module(\n",
       "       (0): Module(\n",
       "         (conv): QuantizedConv2d(\n",
       "           384, 192, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "           (param_quantizers): ModuleDict(\n",
       "             (weight): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=True)\n",
       "           )\n",
       "           (input_quantizers): ModuleList(\n",
       "             (0): None\n",
       "           )\n",
       "           (output_quantizers): ModuleList(\n",
       "             (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "           )\n",
       "         )\n",
       "         (bn): Module(\n",
       "           (module_batch_norm_33): FakeQuantizedBatchNorm(\n",
       "             (param_quantizers): ModuleDict()\n",
       "             (input_quantizers): ModuleList(\n",
       "               (0-4): 5 x QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "             )\n",
       "             (output_quantizers): ModuleList(\n",
       "               (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "             )\n",
       "           )\n",
       "         )\n",
       "         (activate): CustomSiLU(\n",
       "           (sigmoid): QuantizedSigmoid(\n",
       "             (param_quantizers): ModuleDict()\n",
       "             (input_quantizers): ModuleList(\n",
       "               (0): None\n",
       "             )\n",
       "             (output_quantizers): ModuleList(\n",
       "               (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "             )\n",
       "           )\n",
       "           (mul): QuantizedMultiply(\n",
       "             (param_quantizers): ModuleDict()\n",
       "             (input_quantizers): ModuleList(\n",
       "               (0-1): 2 x None\n",
       "             )\n",
       "             (output_quantizers): ModuleList(\n",
       "               (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "             )\n",
       "           )\n",
       "         )\n",
       "       )\n",
       "       (1): Module(\n",
       "         (conv): QuantizedConv2d(\n",
       "           192, 96, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "           (param_quantizers): ModuleDict(\n",
       "             (weight): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=True)\n",
       "           )\n",
       "           (input_quantizers): ModuleList(\n",
       "             (0): None\n",
       "           )\n",
       "           (output_quantizers): ModuleList(\n",
       "             (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "           )\n",
       "         )\n",
       "         (bn): Module(\n",
       "           (module_batch_norm_40): FakeQuantizedBatchNorm(\n",
       "             (param_quantizers): ModuleDict()\n",
       "             (input_quantizers): ModuleList(\n",
       "               (0-4): 5 x QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "             )\n",
       "             (output_quantizers): ModuleList(\n",
       "               (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "             )\n",
       "           )\n",
       "         )\n",
       "         (activate): CustomSiLU(\n",
       "           (sigmoid): QuantizedSigmoid(\n",
       "             (param_quantizers): ModuleDict()\n",
       "             (input_quantizers): ModuleList(\n",
       "               (0): None\n",
       "             )\n",
       "             (output_quantizers): ModuleList(\n",
       "               (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "             )\n",
       "           )\n",
       "           (mul): QuantizedMultiply(\n",
       "             (param_quantizers): ModuleDict()\n",
       "             (input_quantizers): ModuleList(\n",
       "               (0-1): 2 x None\n",
       "             )\n",
       "             (output_quantizers): ModuleList(\n",
       "               (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "             )\n",
       "           )\n",
       "         )\n",
       "       )\n",
       "     )\n",
       "     (upsample): FakeQuantizedUpsample(\n",
       "       scale_factor=2.0, mode='nearest'\n",
       "       (param_quantizers): ModuleDict()\n",
       "       (input_quantizers): ModuleList(\n",
       "         (0): None\n",
       "       )\n",
       "       (output_quantizers): ModuleList(\n",
       "         (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "       )\n",
       "     )\n",
       "     (top_down_blocks): Module(\n",
       "       (0): Module(\n",
       "         (short_conv): Module(\n",
       "           (conv): QuantizedConv2d(\n",
       "             384, 96, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "             (param_quantizers): ModuleDict(\n",
       "               (weight): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=True)\n",
       "             )\n",
       "             (input_quantizers): ModuleList(\n",
       "               (0): None\n",
       "             )\n",
       "             (output_quantizers): ModuleList(\n",
       "               (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "             )\n",
       "           )\n",
       "           (bn): Module(\n",
       "             (module_batch_norm_34): FakeQuantizedBatchNorm(\n",
       "               (param_quantizers): ModuleDict()\n",
       "               (input_quantizers): ModuleList(\n",
       "                 (0-4): 5 x QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "               )\n",
       "               (output_quantizers): ModuleList(\n",
       "                 (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "               )\n",
       "             )\n",
       "           )\n",
       "           (activate): CustomSiLU(\n",
       "             (sigmoid): QuantizedSigmoid(\n",
       "               (param_quantizers): ModuleDict()\n",
       "               (input_quantizers): ModuleList(\n",
       "                 (0): None\n",
       "               )\n",
       "               (output_quantizers): ModuleList(\n",
       "                 (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "               )\n",
       "             )\n",
       "             (mul): QuantizedMultiply(\n",
       "               (param_quantizers): ModuleDict()\n",
       "               (input_quantizers): ModuleList(\n",
       "                 (0-1): 2 x None\n",
       "               )\n",
       "               (output_quantizers): ModuleList(\n",
       "                 (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "               )\n",
       "             )\n",
       "           )\n",
       "         )\n",
       "         (main_conv): Module(\n",
       "           (conv): QuantizedConv2d(\n",
       "             384, 96, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "             (param_quantizers): ModuleDict(\n",
       "               (weight): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=True)\n",
       "             )\n",
       "             (input_quantizers): ModuleList(\n",
       "               (0): None\n",
       "             )\n",
       "             (output_quantizers): ModuleList(\n",
       "               (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "             )\n",
       "           )\n",
       "           (bn): Module(\n",
       "             (module_batch_norm_35): FakeQuantizedBatchNorm(\n",
       "               (param_quantizers): ModuleDict()\n",
       "               (input_quantizers): ModuleList(\n",
       "                 (0-4): 5 x QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "               )\n",
       "               (output_quantizers): ModuleList(\n",
       "                 (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "               )\n",
       "             )\n",
       "           )\n",
       "           (activate): CustomSiLU(\n",
       "             (sigmoid): QuantizedSigmoid(\n",
       "               (param_quantizers): ModuleDict()\n",
       "               (input_quantizers): ModuleList(\n",
       "                 (0): None\n",
       "               )\n",
       "               (output_quantizers): ModuleList(\n",
       "                 (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "               )\n",
       "             )\n",
       "             (mul): QuantizedMultiply(\n",
       "               (param_quantizers): ModuleDict()\n",
       "               (input_quantizers): ModuleList(\n",
       "                 (0-1): 2 x None\n",
       "               )\n",
       "               (output_quantizers): ModuleList(\n",
       "                 (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "               )\n",
       "             )\n",
       "           )\n",
       "         )\n",
       "         (blocks): Module(\n",
       "           (0): Module(\n",
       "             (conv1): Module(\n",
       "               (conv): QuantizedConv2d(\n",
       "                 96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "                 (param_quantizers): ModuleDict(\n",
       "                   (weight): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=True)\n",
       "                 )\n",
       "                 (input_quantizers): ModuleList(\n",
       "                   (0): None\n",
       "                 )\n",
       "                 (output_quantizers): ModuleList(\n",
       "                   (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "                 )\n",
       "               )\n",
       "               (bn): Module(\n",
       "                 (module_batch_norm_36): FakeQuantizedBatchNorm(\n",
       "                   (param_quantizers): ModuleDict()\n",
       "                   (input_quantizers): ModuleList(\n",
       "                     (0-4): 5 x QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "                   )\n",
       "                   (output_quantizers): ModuleList(\n",
       "                     (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "                   )\n",
       "                 )\n",
       "               )\n",
       "               (activate): CustomSiLU(\n",
       "                 (sigmoid): QuantizedSigmoid(\n",
       "                   (param_quantizers): ModuleDict()\n",
       "                   (input_quantizers): ModuleList(\n",
       "                     (0): None\n",
       "                   )\n",
       "                   (output_quantizers): ModuleList(\n",
       "                     (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "                   )\n",
       "                 )\n",
       "                 (mul): QuantizedMultiply(\n",
       "                   (param_quantizers): ModuleDict()\n",
       "                   (input_quantizers): ModuleList(\n",
       "                     (0-1): 2 x None\n",
       "                   )\n",
       "                   (output_quantizers): ModuleList(\n",
       "                     (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "                   )\n",
       "                 )\n",
       "               )\n",
       "             )\n",
       "             (conv2): Module(\n",
       "               (depthwise_conv): Module(\n",
       "                 (conv): QuantizedConv2d(\n",
       "                   96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96, bias=False\n",
       "                   (param_quantizers): ModuleDict(\n",
       "                     (weight): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=True)\n",
       "                   )\n",
       "                   (input_quantizers): ModuleList(\n",
       "                     (0): None\n",
       "                   )\n",
       "                   (output_quantizers): ModuleList(\n",
       "                     (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "                   )\n",
       "                 )\n",
       "                 (bn): Module(\n",
       "                   (module_batch_norm_37): BatchNorm()\n",
       "                 )\n",
       "                 (activate): CustomSiLU(\n",
       "                   (sigmoid): QuantizedSigmoid(\n",
       "                     (param_quantizers): ModuleDict()\n",
       "                     (input_quantizers): ModuleList(\n",
       "                       (0): None\n",
       "                     )\n",
       "                     (output_quantizers): ModuleList(\n",
       "                       (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "                     )\n",
       "                   )\n",
       "                   (mul): QuantizedMultiply(\n",
       "                     (param_quantizers): ModuleDict()\n",
       "                     (input_quantizers): ModuleList(\n",
       "                       (0-1): 2 x None\n",
       "                     )\n",
       "                     (output_quantizers): ModuleList(\n",
       "                       (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "                     )\n",
       "                   )\n",
       "                 )\n",
       "               )\n",
       "               (pointwise_conv): Module(\n",
       "                 (conv): QuantizedConv2d(\n",
       "                   96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "                   (param_quantizers): ModuleDict(\n",
       "                     (weight): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=True)\n",
       "                   )\n",
       "                   (input_quantizers): ModuleList(\n",
       "                     (0): None\n",
       "                   )\n",
       "                   (output_quantizers): ModuleList(\n",
       "                     (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "                   )\n",
       "                 )\n",
       "                 (bn): Module(\n",
       "                   (module_batch_norm_38): FakeQuantizedBatchNorm(\n",
       "                     (param_quantizers): ModuleDict()\n",
       "                     (input_quantizers): ModuleList(\n",
       "                       (0-4): 5 x QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "                     )\n",
       "                     (output_quantizers): ModuleList(\n",
       "                       (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "                     )\n",
       "                   )\n",
       "                 )\n",
       "                 (activate): CustomSiLU(\n",
       "                   (sigmoid): QuantizedSigmoid(\n",
       "                     (param_quantizers): ModuleDict()\n",
       "                     (input_quantizers): ModuleList(\n",
       "                       (0): None\n",
       "                     )\n",
       "                     (output_quantizers): ModuleList(\n",
       "                       (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "                     )\n",
       "                   )\n",
       "                   (mul): QuantizedMultiply(\n",
       "                     (param_quantizers): ModuleDict()\n",
       "                     (input_quantizers): ModuleList(\n",
       "                       (0-1): 2 x None\n",
       "                     )\n",
       "                     (output_quantizers): ModuleList(\n",
       "                       (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "                     )\n",
       "                   )\n",
       "                 )\n",
       "               )\n",
       "             )\n",
       "           )\n",
       "         )\n",
       "         (final_conv): Module(\n",
       "           (conv): QuantizedConv2d(\n",
       "             192, 192, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "             (param_quantizers): ModuleDict(\n",
       "               (weight): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=True)\n",
       "             )\n",
       "             (input_quantizers): ModuleList(\n",
       "               (0): None\n",
       "             )\n",
       "             (output_quantizers): ModuleList(\n",
       "               (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "             )\n",
       "           )\n",
       "           (bn): Module(\n",
       "             (module_batch_norm_39): FakeQuantizedBatchNorm(\n",
       "               (param_quantizers): ModuleDict()\n",
       "               (input_quantizers): ModuleList(\n",
       "                 (0-4): 5 x QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "               )\n",
       "               (output_quantizers): ModuleList(\n",
       "                 (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "               )\n",
       "             )\n",
       "           )\n",
       "           (activate): CustomSiLU(\n",
       "             (sigmoid): QuantizedSigmoid(\n",
       "               (param_quantizers): ModuleDict()\n",
       "               (input_quantizers): ModuleList(\n",
       "                 (0): None\n",
       "               )\n",
       "               (output_quantizers): ModuleList(\n",
       "                 (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "               )\n",
       "             )\n",
       "             (mul): QuantizedMultiply(\n",
       "               (param_quantizers): ModuleDict()\n",
       "               (input_quantizers): ModuleList(\n",
       "                 (0-1): 2 x None\n",
       "               )\n",
       "               (output_quantizers): ModuleList(\n",
       "                 (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "               )\n",
       "             )\n",
       "           )\n",
       "         )\n",
       "         (module_cat_6): FakeQuantizedConcat(\n",
       "           (param_quantizers): ModuleDict()\n",
       "           (input_quantizers): ModuleList(\n",
       "             (0): None\n",
       "           )\n",
       "           (output_quantizers): ModuleList(\n",
       "             (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "           )\n",
       "         )\n",
       "       )\n",
       "       (1): Module(\n",
       "         (short_conv): Module(\n",
       "           (conv): QuantizedConv2d(\n",
       "             192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "             (param_quantizers): ModuleDict(\n",
       "               (weight): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=True)\n",
       "             )\n",
       "             (input_quantizers): ModuleList(\n",
       "               (0): None\n",
       "             )\n",
       "             (output_quantizers): ModuleList(\n",
       "               (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "             )\n",
       "           )\n",
       "           (bn): Module(\n",
       "             (module_batch_norm_41): FakeQuantizedBatchNorm(\n",
       "               (param_quantizers): ModuleDict()\n",
       "               (input_quantizers): ModuleList(\n",
       "                 (0-4): 5 x QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "               )\n",
       "               (output_quantizers): ModuleList(\n",
       "                 (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "               )\n",
       "             )\n",
       "           )\n",
       "           (activate): CustomSiLU(\n",
       "             (sigmoid): QuantizedSigmoid(\n",
       "               (param_quantizers): ModuleDict()\n",
       "               (input_quantizers): ModuleList(\n",
       "                 (0): None\n",
       "               )\n",
       "               (output_quantizers): ModuleList(\n",
       "                 (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "               )\n",
       "             )\n",
       "             (mul): QuantizedMultiply(\n",
       "               (param_quantizers): ModuleDict()\n",
       "               (input_quantizers): ModuleList(\n",
       "                 (0-1): 2 x None\n",
       "               )\n",
       "               (output_quantizers): ModuleList(\n",
       "                 (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "               )\n",
       "             )\n",
       "           )\n",
       "         )\n",
       "         (main_conv): Module(\n",
       "           (conv): QuantizedConv2d(\n",
       "             192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "             (param_quantizers): ModuleDict(\n",
       "               (weight): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=True)\n",
       "             )\n",
       "             (input_quantizers): ModuleList(\n",
       "               (0): None\n",
       "             )\n",
       "             (output_quantizers): ModuleList(\n",
       "               (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "             )\n",
       "           )\n",
       "           (bn): Module(\n",
       "             (module_batch_norm_42): FakeQuantizedBatchNorm(\n",
       "               (param_quantizers): ModuleDict()\n",
       "               (input_quantizers): ModuleList(\n",
       "                 (0-4): 5 x QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "               )\n",
       "               (output_quantizers): ModuleList(\n",
       "                 (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "               )\n",
       "             )\n",
       "           )\n",
       "           (activate): CustomSiLU(\n",
       "             (sigmoid): QuantizedSigmoid(\n",
       "               (param_quantizers): ModuleDict()\n",
       "               (input_quantizers): ModuleList(\n",
       "                 (0): None\n",
       "               )\n",
       "               (output_quantizers): ModuleList(\n",
       "                 (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "               )\n",
       "             )\n",
       "             (mul): QuantizedMultiply(\n",
       "               (param_quantizers): ModuleDict()\n",
       "               (input_quantizers): ModuleList(\n",
       "                 (0-1): 2 x None\n",
       "               )\n",
       "               (output_quantizers): ModuleList(\n",
       "                 (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "               )\n",
       "             )\n",
       "           )\n",
       "         )\n",
       "         (blocks): Module(\n",
       "           (0): Module(\n",
       "             (conv1): Module(\n",
       "               (conv): QuantizedConv2d(\n",
       "                 48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "                 (param_quantizers): ModuleDict(\n",
       "                   (weight): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=True)\n",
       "                 )\n",
       "                 (input_quantizers): ModuleList(\n",
       "                   (0): None\n",
       "                 )\n",
       "                 (output_quantizers): ModuleList(\n",
       "                   (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "                 )\n",
       "               )\n",
       "               (bn): Module(\n",
       "                 (module_batch_norm_43): FakeQuantizedBatchNorm(\n",
       "                   (param_quantizers): ModuleDict()\n",
       "                   (input_quantizers): ModuleList(\n",
       "                     (0-4): 5 x QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "                   )\n",
       "                   (output_quantizers): ModuleList(\n",
       "                     (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "                   )\n",
       "                 )\n",
       "               )\n",
       "               (activate): CustomSiLU(\n",
       "                 (sigmoid): QuantizedSigmoid(\n",
       "                   (param_quantizers): ModuleDict()\n",
       "                   (input_quantizers): ModuleList(\n",
       "                     (0): None\n",
       "                   )\n",
       "                   (output_quantizers): ModuleList(\n",
       "                     (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "                   )\n",
       "                 )\n",
       "                 (mul): QuantizedMultiply(\n",
       "                   (param_quantizers): ModuleDict()\n",
       "                   (input_quantizers): ModuleList(\n",
       "                     (0-1): 2 x None\n",
       "                   )\n",
       "                   (output_quantizers): ModuleList(\n",
       "                     (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "                   )\n",
       "                 )\n",
       "               )\n",
       "             )\n",
       "             (conv2): Module(\n",
       "               (depthwise_conv): Module(\n",
       "                 (conv): QuantizedConv2d(\n",
       "                   48, 48, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=48, bias=False\n",
       "                   (param_quantizers): ModuleDict(\n",
       "                     (weight): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=True)\n",
       "                   )\n",
       "                   (input_quantizers): ModuleList(\n",
       "                     (0): None\n",
       "                   )\n",
       "                   (output_quantizers): ModuleList(\n",
       "                     (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "                   )\n",
       "                 )\n",
       "                 (bn): Module(\n",
       "                   (module_batch_norm_44): BatchNorm()\n",
       "                 )\n",
       "                 (activate): CustomSiLU(\n",
       "                   (sigmoid): QuantizedSigmoid(\n",
       "                     (param_quantizers): ModuleDict()\n",
       "                     (input_quantizers): ModuleList(\n",
       "                       (0): None\n",
       "                     )\n",
       "                     (output_quantizers): ModuleList(\n",
       "                       (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "                     )\n",
       "                   )\n",
       "                   (mul): QuantizedMultiply(\n",
       "                     (param_quantizers): ModuleDict()\n",
       "                     (input_quantizers): ModuleList(\n",
       "                       (0-1): 2 x None\n",
       "                     )\n",
       "                     (output_quantizers): ModuleList(\n",
       "                       (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "                     )\n",
       "                   )\n",
       "                 )\n",
       "               )\n",
       "               (pointwise_conv): Module(\n",
       "                 (conv): QuantizedConv2d(\n",
       "                   48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "                   (param_quantizers): ModuleDict(\n",
       "                     (weight): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=True)\n",
       "                   )\n",
       "                   (input_quantizers): ModuleList(\n",
       "                     (0): None\n",
       "                   )\n",
       "                   (output_quantizers): ModuleList(\n",
       "                     (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "                   )\n",
       "                 )\n",
       "                 (bn): Module(\n",
       "                   (module_batch_norm_45): FakeQuantizedBatchNorm(\n",
       "                     (param_quantizers): ModuleDict()\n",
       "                     (input_quantizers): ModuleList(\n",
       "                       (0-4): 5 x QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "                     )\n",
       "                     (output_quantizers): ModuleList(\n",
       "                       (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "                     )\n",
       "                   )\n",
       "                 )\n",
       "                 (activate): CustomSiLU(\n",
       "                   (sigmoid): QuantizedSigmoid(\n",
       "                     (param_quantizers): ModuleDict()\n",
       "                     (input_quantizers): ModuleList(\n",
       "                       (0): None\n",
       "                     )\n",
       "                     (output_quantizers): ModuleList(\n",
       "                       (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "                     )\n",
       "                   )\n",
       "                   (mul): QuantizedMultiply(\n",
       "                     (param_quantizers): ModuleDict()\n",
       "                     (input_quantizers): ModuleList(\n",
       "                       (0-1): 2 x None\n",
       "                     )\n",
       "                     (output_quantizers): ModuleList(\n",
       "                       (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "                     )\n",
       "                   )\n",
       "                 )\n",
       "               )\n",
       "             )\n",
       "           )\n",
       "         )\n",
       "         (final_conv): Module(\n",
       "           (conv): QuantizedConv2d(\n",
       "             96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "             (param_quantizers): ModuleDict(\n",
       "               (weight): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=True)\n",
       "             )\n",
       "             (input_quantizers): ModuleList(\n",
       "               (0): None\n",
       "             )\n",
       "             (output_quantizers): ModuleList(\n",
       "               (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "             )\n",
       "           )\n",
       "           (bn): Module(\n",
       "             (module_batch_norm_46): FakeQuantizedBatchNorm(\n",
       "               (param_quantizers): ModuleDict()\n",
       "               (input_quantizers): ModuleList(\n",
       "                 (0-4): 5 x QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "               )\n",
       "               (output_quantizers): ModuleList(\n",
       "                 (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "               )\n",
       "             )\n",
       "           )\n",
       "           (activate): CustomSiLU(\n",
       "             (sigmoid): QuantizedSigmoid(\n",
       "               (param_quantizers): ModuleDict()\n",
       "               (input_quantizers): ModuleList(\n",
       "                 (0): None\n",
       "               )\n",
       "               (output_quantizers): ModuleList(\n",
       "                 (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "               )\n",
       "             )\n",
       "             (mul): QuantizedMultiply(\n",
       "               (param_quantizers): ModuleDict()\n",
       "               (input_quantizers): ModuleList(\n",
       "                 (0-1): 2 x None\n",
       "               )\n",
       "               (output_quantizers): ModuleList(\n",
       "                 (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "               )\n",
       "             )\n",
       "           )\n",
       "         )\n",
       "         (module_cat_8): FakeQuantizedConcat(\n",
       "           (param_quantizers): ModuleDict()\n",
       "           (input_quantizers): ModuleList(\n",
       "             (0): None\n",
       "           )\n",
       "           (output_quantizers): ModuleList(\n",
       "             (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "           )\n",
       "         )\n",
       "       )\n",
       "     )\n",
       "     (downsamples): Module(\n",
       "       (0): Module(\n",
       "         (conv): QuantizedConv2d(\n",
       "           96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False\n",
       "           (param_quantizers): ModuleDict(\n",
       "             (weight): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=True)\n",
       "           )\n",
       "           (input_quantizers): ModuleList(\n",
       "             (0): None\n",
       "           )\n",
       "           (output_quantizers): ModuleList(\n",
       "             (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "           )\n",
       "         )\n",
       "         (bn): Module(\n",
       "           (module_batch_norm_47): FakeQuantizedBatchNorm(\n",
       "             (param_quantizers): ModuleDict()\n",
       "             (input_quantizers): ModuleList(\n",
       "               (0-4): 5 x QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "             )\n",
       "             (output_quantizers): ModuleList(\n",
       "               (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "             )\n",
       "           )\n",
       "         )\n",
       "         (activate): CustomSiLU(\n",
       "           (sigmoid): QuantizedSigmoid(\n",
       "             (param_quantizers): ModuleDict()\n",
       "             (input_quantizers): ModuleList(\n",
       "               (0): None\n",
       "             )\n",
       "             (output_quantizers): ModuleList(\n",
       "               (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "             )\n",
       "           )\n",
       "           (mul): QuantizedMultiply(\n",
       "             (param_quantizers): ModuleDict()\n",
       "             (input_quantizers): ModuleList(\n",
       "               (0-1): 2 x None\n",
       "             )\n",
       "             (output_quantizers): ModuleList(\n",
       "               (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "             )\n",
       "           )\n",
       "         )\n",
       "       )\n",
       "       (1): Module(\n",
       "         (conv): QuantizedConv2d(\n",
       "           192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False\n",
       "           (param_quantizers): ModuleDict(\n",
       "             (weight): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=True)\n",
       "           )\n",
       "           (input_quantizers): ModuleList(\n",
       "             (0): None\n",
       "           )\n",
       "           (output_quantizers): ModuleList(\n",
       "             (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "           )\n",
       "         )\n",
       "         (bn): Module(\n",
       "           (module_batch_norm_54): FakeQuantizedBatchNorm(\n",
       "             (param_quantizers): ModuleDict()\n",
       "             (input_quantizers): ModuleList(\n",
       "               (0-4): 5 x QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "             )\n",
       "             (output_quantizers): ModuleList(\n",
       "               (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "             )\n",
       "           )\n",
       "         )\n",
       "         (activate): CustomSiLU(\n",
       "           (sigmoid): QuantizedSigmoid(\n",
       "             (param_quantizers): ModuleDict()\n",
       "             (input_quantizers): ModuleList(\n",
       "               (0): None\n",
       "             )\n",
       "             (output_quantizers): ModuleList(\n",
       "               (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "             )\n",
       "           )\n",
       "           (mul): QuantizedMultiply(\n",
       "             (param_quantizers): ModuleDict()\n",
       "             (input_quantizers): ModuleList(\n",
       "               (0-1): 2 x None\n",
       "             )\n",
       "             (output_quantizers): ModuleList(\n",
       "               (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "             )\n",
       "           )\n",
       "         )\n",
       "       )\n",
       "     )\n",
       "     (bottom_up_blocks): Module(\n",
       "       (0): Module(\n",
       "         (short_conv): Module(\n",
       "           (conv): QuantizedConv2d(\n",
       "             192, 96, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "             (param_quantizers): ModuleDict(\n",
       "               (weight): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=True)\n",
       "             )\n",
       "             (input_quantizers): ModuleList(\n",
       "               (0): None\n",
       "             )\n",
       "             (output_quantizers): ModuleList(\n",
       "               (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "             )\n",
       "           )\n",
       "           (bn): Module(\n",
       "             (module_batch_norm_48): FakeQuantizedBatchNorm(\n",
       "               (param_quantizers): ModuleDict()\n",
       "               (input_quantizers): ModuleList(\n",
       "                 (0-4): 5 x QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "               )\n",
       "               (output_quantizers): ModuleList(\n",
       "                 (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "               )\n",
       "             )\n",
       "           )\n",
       "           (activate): CustomSiLU(\n",
       "             (sigmoid): QuantizedSigmoid(\n",
       "               (param_quantizers): ModuleDict()\n",
       "               (input_quantizers): ModuleList(\n",
       "                 (0): None\n",
       "               )\n",
       "               (output_quantizers): ModuleList(\n",
       "                 (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "               )\n",
       "             )\n",
       "             (mul): QuantizedMultiply(\n",
       "               (param_quantizers): ModuleDict()\n",
       "               (input_quantizers): ModuleList(\n",
       "                 (0-1): 2 x None\n",
       "               )\n",
       "               (output_quantizers): ModuleList(\n",
       "                 (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "               )\n",
       "             )\n",
       "           )\n",
       "         )\n",
       "         (main_conv): Module(\n",
       "           (conv): QuantizedConv2d(\n",
       "             192, 96, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "             (param_quantizers): ModuleDict(\n",
       "               (weight): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=True)\n",
       "             )\n",
       "             (input_quantizers): ModuleList(\n",
       "               (0): None\n",
       "             )\n",
       "             (output_quantizers): ModuleList(\n",
       "               (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "             )\n",
       "           )\n",
       "           (bn): Module(\n",
       "             (module_batch_norm_49): FakeQuantizedBatchNorm(\n",
       "               (param_quantizers): ModuleDict()\n",
       "               (input_quantizers): ModuleList(\n",
       "                 (0-4): 5 x QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "               )\n",
       "               (output_quantizers): ModuleList(\n",
       "                 (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "               )\n",
       "             )\n",
       "           )\n",
       "           (activate): CustomSiLU(\n",
       "             (sigmoid): QuantizedSigmoid(\n",
       "               (param_quantizers): ModuleDict()\n",
       "               (input_quantizers): ModuleList(\n",
       "                 (0): None\n",
       "               )\n",
       "               (output_quantizers): ModuleList(\n",
       "                 (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "               )\n",
       "             )\n",
       "             (mul): QuantizedMultiply(\n",
       "               (param_quantizers): ModuleDict()\n",
       "               (input_quantizers): ModuleList(\n",
       "                 (0-1): 2 x None\n",
       "               )\n",
       "               (output_quantizers): ModuleList(\n",
       "                 (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "               )\n",
       "             )\n",
       "           )\n",
       "         )\n",
       "         (blocks): Module(\n",
       "           (0): Module(\n",
       "             (conv1): Module(\n",
       "               (conv): QuantizedConv2d(\n",
       "                 96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "                 (param_quantizers): ModuleDict(\n",
       "                   (weight): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=True)\n",
       "                 )\n",
       "                 (input_quantizers): ModuleList(\n",
       "                   (0): None\n",
       "                 )\n",
       "                 (output_quantizers): ModuleList(\n",
       "                   (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "                 )\n",
       "               )\n",
       "               (bn): Module(\n",
       "                 (module_batch_norm_50): FakeQuantizedBatchNorm(\n",
       "                   (param_quantizers): ModuleDict()\n",
       "                   (input_quantizers): ModuleList(\n",
       "                     (0-4): 5 x QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "                   )\n",
       "                   (output_quantizers): ModuleList(\n",
       "                     (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "                   )\n",
       "                 )\n",
       "               )\n",
       "               (activate): CustomSiLU(\n",
       "                 (sigmoid): QuantizedSigmoid(\n",
       "                   (param_quantizers): ModuleDict()\n",
       "                   (input_quantizers): ModuleList(\n",
       "                     (0): None\n",
       "                   )\n",
       "                   (output_quantizers): ModuleList(\n",
       "                     (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "                   )\n",
       "                 )\n",
       "                 (mul): QuantizedMultiply(\n",
       "                   (param_quantizers): ModuleDict()\n",
       "                   (input_quantizers): ModuleList(\n",
       "                     (0-1): 2 x None\n",
       "                   )\n",
       "                   (output_quantizers): ModuleList(\n",
       "                     (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "                   )\n",
       "                 )\n",
       "               )\n",
       "             )\n",
       "             (conv2): Module(\n",
       "               (depthwise_conv): Module(\n",
       "                 (conv): QuantizedConv2d(\n",
       "                   96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96, bias=False\n",
       "                   (param_quantizers): ModuleDict(\n",
       "                     (weight): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=True)\n",
       "                   )\n",
       "                   (input_quantizers): ModuleList(\n",
       "                     (0): None\n",
       "                   )\n",
       "                   (output_quantizers): ModuleList(\n",
       "                     (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "                   )\n",
       "                 )\n",
       "                 (bn): Module(\n",
       "                   (module_batch_norm_51): BatchNorm()\n",
       "                 )\n",
       "                 (activate): CustomSiLU(\n",
       "                   (sigmoid): QuantizedSigmoid(\n",
       "                     (param_quantizers): ModuleDict()\n",
       "                     (input_quantizers): ModuleList(\n",
       "                       (0): None\n",
       "                     )\n",
       "                     (output_quantizers): ModuleList(\n",
       "                       (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "                     )\n",
       "                   )\n",
       "                   (mul): QuantizedMultiply(\n",
       "                     (param_quantizers): ModuleDict()\n",
       "                     (input_quantizers): ModuleList(\n",
       "                       (0-1): 2 x None\n",
       "                     )\n",
       "                     (output_quantizers): ModuleList(\n",
       "                       (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "                     )\n",
       "                   )\n",
       "                 )\n",
       "               )\n",
       "               (pointwise_conv): Module(\n",
       "                 (conv): QuantizedConv2d(\n",
       "                   96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "                   (param_quantizers): ModuleDict(\n",
       "                     (weight): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=True)\n",
       "                   )\n",
       "                   (input_quantizers): ModuleList(\n",
       "                     (0): None\n",
       "                   )\n",
       "                   (output_quantizers): ModuleList(\n",
       "                     (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "                   )\n",
       "                 )\n",
       "                 (bn): Module(\n",
       "                   (module_batch_norm_52): FakeQuantizedBatchNorm(\n",
       "                     (param_quantizers): ModuleDict()\n",
       "                     (input_quantizers): ModuleList(\n",
       "                       (0-4): 5 x QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "                     )\n",
       "                     (output_quantizers): ModuleList(\n",
       "                       (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "                     )\n",
       "                   )\n",
       "                 )\n",
       "                 (activate): CustomSiLU(\n",
       "                   (sigmoid): QuantizedSigmoid(\n",
       "                     (param_quantizers): ModuleDict()\n",
       "                     (input_quantizers): ModuleList(\n",
       "                       (0): None\n",
       "                     )\n",
       "                     (output_quantizers): ModuleList(\n",
       "                       (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "                     )\n",
       "                   )\n",
       "                   (mul): QuantizedMultiply(\n",
       "                     (param_quantizers): ModuleDict()\n",
       "                     (input_quantizers): ModuleList(\n",
       "                       (0-1): 2 x None\n",
       "                     )\n",
       "                     (output_quantizers): ModuleList(\n",
       "                       (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "                     )\n",
       "                   )\n",
       "                 )\n",
       "               )\n",
       "             )\n",
       "           )\n",
       "         )\n",
       "         (final_conv): Module(\n",
       "           (conv): QuantizedConv2d(\n",
       "             192, 192, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "             (param_quantizers): ModuleDict(\n",
       "               (weight): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=True)\n",
       "             )\n",
       "             (input_quantizers): ModuleList(\n",
       "               (0): None\n",
       "             )\n",
       "             (output_quantizers): ModuleList(\n",
       "               (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "             )\n",
       "           )\n",
       "           (bn): Module(\n",
       "             (module_batch_norm_53): FakeQuantizedBatchNorm(\n",
       "               (param_quantizers): ModuleDict()\n",
       "               (input_quantizers): ModuleList(\n",
       "                 (0-4): 5 x QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "               )\n",
       "               (output_quantizers): ModuleList(\n",
       "                 (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "               )\n",
       "             )\n",
       "           )\n",
       "           (activate): CustomSiLU(\n",
       "             (sigmoid): QuantizedSigmoid(\n",
       "               (param_quantizers): ModuleDict()\n",
       "               (input_quantizers): ModuleList(\n",
       "                 (0): None\n",
       "               )\n",
       "               (output_quantizers): ModuleList(\n",
       "                 (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "               )\n",
       "             )\n",
       "             (mul): QuantizedMultiply(\n",
       "               (param_quantizers): ModuleDict()\n",
       "               (input_quantizers): ModuleList(\n",
       "                 (0-1): 2 x None\n",
       "               )\n",
       "               (output_quantizers): ModuleList(\n",
       "                 (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "               )\n",
       "             )\n",
       "           )\n",
       "         )\n",
       "         (module_cat_10): FakeQuantizedConcat(\n",
       "           (param_quantizers): ModuleDict()\n",
       "           (input_quantizers): ModuleList(\n",
       "             (0): None\n",
       "           )\n",
       "           (output_quantizers): ModuleList(\n",
       "             (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "           )\n",
       "         )\n",
       "       )\n",
       "       (1): Module(\n",
       "         (short_conv): Module(\n",
       "           (conv): QuantizedConv2d(\n",
       "             384, 192, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "             (param_quantizers): ModuleDict(\n",
       "               (weight): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=True)\n",
       "             )\n",
       "             (input_quantizers): ModuleList(\n",
       "               (0): None\n",
       "             )\n",
       "             (output_quantizers): ModuleList(\n",
       "               (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "             )\n",
       "           )\n",
       "           (bn): Module(\n",
       "             (module_batch_norm_55): FakeQuantizedBatchNorm(\n",
       "               (param_quantizers): ModuleDict()\n",
       "               (input_quantizers): ModuleList(\n",
       "                 (0-4): 5 x QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "               )\n",
       "               (output_quantizers): ModuleList(\n",
       "                 (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "               )\n",
       "             )\n",
       "           )\n",
       "           (activate): CustomSiLU(\n",
       "             (sigmoid): QuantizedSigmoid(\n",
       "               (param_quantizers): ModuleDict()\n",
       "               (input_quantizers): ModuleList(\n",
       "                 (0): None\n",
       "               )\n",
       "               (output_quantizers): ModuleList(\n",
       "                 (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "               )\n",
       "             )\n",
       "             (mul): QuantizedMultiply(\n",
       "               (param_quantizers): ModuleDict()\n",
       "               (input_quantizers): ModuleList(\n",
       "                 (0-1): 2 x None\n",
       "               )\n",
       "               (output_quantizers): ModuleList(\n",
       "                 (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "               )\n",
       "             )\n",
       "           )\n",
       "         )\n",
       "         (main_conv): Module(\n",
       "           (conv): QuantizedConv2d(\n",
       "             384, 192, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "             (param_quantizers): ModuleDict(\n",
       "               (weight): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=True)\n",
       "             )\n",
       "             (input_quantizers): ModuleList(\n",
       "               (0): None\n",
       "             )\n",
       "             (output_quantizers): ModuleList(\n",
       "               (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "             )\n",
       "           )\n",
       "           (bn): Module(\n",
       "             (module_batch_norm_56): FakeQuantizedBatchNorm(\n",
       "               (param_quantizers): ModuleDict()\n",
       "               (input_quantizers): ModuleList(\n",
       "                 (0-4): 5 x QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "               )\n",
       "               (output_quantizers): ModuleList(\n",
       "                 (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "               )\n",
       "             )\n",
       "           )\n",
       "           (activate): CustomSiLU(\n",
       "             (sigmoid): QuantizedSigmoid(\n",
       "               (param_quantizers): ModuleDict()\n",
       "               (input_quantizers): ModuleList(\n",
       "                 (0): None\n",
       "               )\n",
       "               (output_quantizers): ModuleList(\n",
       "                 (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "               )\n",
       "             )\n",
       "             (mul): QuantizedMultiply(\n",
       "               (param_quantizers): ModuleDict()\n",
       "               (input_quantizers): ModuleList(\n",
       "                 (0-1): 2 x None\n",
       "               )\n",
       "               (output_quantizers): ModuleList(\n",
       "                 (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "               )\n",
       "             )\n",
       "           )\n",
       "         )\n",
       "         (blocks): Module(\n",
       "           (0): Module(\n",
       "             (conv1): Module(\n",
       "               (conv): QuantizedConv2d(\n",
       "                 192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "                 (param_quantizers): ModuleDict(\n",
       "                   (weight): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=True)\n",
       "                 )\n",
       "                 (input_quantizers): ModuleList(\n",
       "                   (0): None\n",
       "                 )\n",
       "                 (output_quantizers): ModuleList(\n",
       "                   (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "                 )\n",
       "               )\n",
       "               (bn): Module(\n",
       "                 (module_batch_norm_57): FakeQuantizedBatchNorm(\n",
       "                   (param_quantizers): ModuleDict()\n",
       "                   (input_quantizers): ModuleList(\n",
       "                     (0-4): 5 x QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "                   )\n",
       "                   (output_quantizers): ModuleList(\n",
       "                     (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "                   )\n",
       "                 )\n",
       "               )\n",
       "               (activate): CustomSiLU(\n",
       "                 (sigmoid): QuantizedSigmoid(\n",
       "                   (param_quantizers): ModuleDict()\n",
       "                   (input_quantizers): ModuleList(\n",
       "                     (0): None\n",
       "                   )\n",
       "                   (output_quantizers): ModuleList(\n",
       "                     (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "                   )\n",
       "                 )\n",
       "                 (mul): QuantizedMultiply(\n",
       "                   (param_quantizers): ModuleDict()\n",
       "                   (input_quantizers): ModuleList(\n",
       "                     (0-1): 2 x None\n",
       "                   )\n",
       "                   (output_quantizers): ModuleList(\n",
       "                     (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "                   )\n",
       "                 )\n",
       "               )\n",
       "             )\n",
       "             (conv2): Module(\n",
       "               (depthwise_conv): Module(\n",
       "                 (conv): QuantizedConv2d(\n",
       "                   192, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=192, bias=False\n",
       "                   (param_quantizers): ModuleDict(\n",
       "                     (weight): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=True)\n",
       "                   )\n",
       "                   (input_quantizers): ModuleList(\n",
       "                     (0): None\n",
       "                   )\n",
       "                   (output_quantizers): ModuleList(\n",
       "                     (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "                   )\n",
       "                 )\n",
       "                 (bn): Module(\n",
       "                   (module_batch_norm_58): BatchNorm()\n",
       "                 )\n",
       "                 (activate): CustomSiLU(\n",
       "                   (sigmoid): QuantizedSigmoid(\n",
       "                     (param_quantizers): ModuleDict()\n",
       "                     (input_quantizers): ModuleList(\n",
       "                       (0): None\n",
       "                     )\n",
       "                     (output_quantizers): ModuleList(\n",
       "                       (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "                     )\n",
       "                   )\n",
       "                   (mul): QuantizedMultiply(\n",
       "                     (param_quantizers): ModuleDict()\n",
       "                     (input_quantizers): ModuleList(\n",
       "                       (0-1): 2 x None\n",
       "                     )\n",
       "                     (output_quantizers): ModuleList(\n",
       "                       (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "                     )\n",
       "                   )\n",
       "                 )\n",
       "               )\n",
       "               (pointwise_conv): Module(\n",
       "                 (conv): QuantizedConv2d(\n",
       "                   192, 192, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "                   (param_quantizers): ModuleDict(\n",
       "                     (weight): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=True)\n",
       "                   )\n",
       "                   (input_quantizers): ModuleList(\n",
       "                     (0): None\n",
       "                   )\n",
       "                   (output_quantizers): ModuleList(\n",
       "                     (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "                   )\n",
       "                 )\n",
       "                 (bn): Module(\n",
       "                   (module_batch_norm_59): FakeQuantizedBatchNorm(\n",
       "                     (param_quantizers): ModuleDict()\n",
       "                     (input_quantizers): ModuleList(\n",
       "                       (0-4): 5 x QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "                     )\n",
       "                     (output_quantizers): ModuleList(\n",
       "                       (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "                     )\n",
       "                   )\n",
       "                 )\n",
       "                 (activate): CustomSiLU(\n",
       "                   (sigmoid): QuantizedSigmoid(\n",
       "                     (param_quantizers): ModuleDict()\n",
       "                     (input_quantizers): ModuleList(\n",
       "                       (0): None\n",
       "                     )\n",
       "                     (output_quantizers): ModuleList(\n",
       "                       (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "                     )\n",
       "                   )\n",
       "                   (mul): QuantizedMultiply(\n",
       "                     (param_quantizers): ModuleDict()\n",
       "                     (input_quantizers): ModuleList(\n",
       "                       (0-1): 2 x None\n",
       "                     )\n",
       "                     (output_quantizers): ModuleList(\n",
       "                       (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "                     )\n",
       "                   )\n",
       "                 )\n",
       "               )\n",
       "             )\n",
       "           )\n",
       "         )\n",
       "         (final_conv): Module(\n",
       "           (conv): QuantizedConv2d(\n",
       "             384, 384, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "             (param_quantizers): ModuleDict(\n",
       "               (weight): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=True)\n",
       "             )\n",
       "             (input_quantizers): ModuleList(\n",
       "               (0): None\n",
       "             )\n",
       "             (output_quantizers): ModuleList(\n",
       "               (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "             )\n",
       "           )\n",
       "           (bn): Module(\n",
       "             (module_batch_norm_60): FakeQuantizedBatchNorm(\n",
       "               (param_quantizers): ModuleDict()\n",
       "               (input_quantizers): ModuleList(\n",
       "                 (0-4): 5 x QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "               )\n",
       "               (output_quantizers): ModuleList(\n",
       "                 (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "               )\n",
       "             )\n",
       "           )\n",
       "           (activate): CustomSiLU(\n",
       "             (sigmoid): QuantizedSigmoid(\n",
       "               (param_quantizers): ModuleDict()\n",
       "               (input_quantizers): ModuleList(\n",
       "                 (0): None\n",
       "               )\n",
       "               (output_quantizers): ModuleList(\n",
       "                 (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "               )\n",
       "             )\n",
       "             (mul): QuantizedMultiply(\n",
       "               (param_quantizers): ModuleDict()\n",
       "               (input_quantizers): ModuleList(\n",
       "                 (0-1): 2 x None\n",
       "               )\n",
       "               (output_quantizers): ModuleList(\n",
       "                 (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "               )\n",
       "             )\n",
       "           )\n",
       "         )\n",
       "         (module_cat_12): FakeQuantizedConcat(\n",
       "           (param_quantizers): ModuleDict()\n",
       "           (input_quantizers): ModuleList(\n",
       "             (0): None\n",
       "           )\n",
       "           (output_quantizers): ModuleList(\n",
       "             (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "           )\n",
       "         )\n",
       "       )\n",
       "     )\n",
       "     (out_convs): Module(\n",
       "       (0): Module(\n",
       "         (conv): QuantizedConv2d(\n",
       "           96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "           (param_quantizers): ModuleDict(\n",
       "             (weight): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=True)\n",
       "           )\n",
       "           (input_quantizers): ModuleList(\n",
       "             (0): None\n",
       "           )\n",
       "           (output_quantizers): ModuleList(\n",
       "             (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "           )\n",
       "         )\n",
       "         (bn): Module(\n",
       "           (module_batch_norm_61): FakeQuantizedBatchNorm(\n",
       "             (param_quantizers): ModuleDict()\n",
       "             (input_quantizers): ModuleList(\n",
       "               (0-4): 5 x QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "             )\n",
       "             (output_quantizers): ModuleList(\n",
       "               (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "             )\n",
       "           )\n",
       "         )\n",
       "         (activate): CustomSiLU(\n",
       "           (sigmoid): QuantizedSigmoid(\n",
       "             (param_quantizers): ModuleDict()\n",
       "             (input_quantizers): ModuleList(\n",
       "               (0): None\n",
       "             )\n",
       "             (output_quantizers): ModuleList(\n",
       "               (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "             )\n",
       "           )\n",
       "           (mul): QuantizedMultiply(\n",
       "             (param_quantizers): ModuleDict()\n",
       "             (input_quantizers): ModuleList(\n",
       "               (0-1): 2 x None\n",
       "             )\n",
       "             (output_quantizers): ModuleList(\n",
       "               (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "             )\n",
       "           )\n",
       "         )\n",
       "       )\n",
       "       (1): Module(\n",
       "         (conv): QuantizedConv2d(\n",
       "           192, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "           (param_quantizers): ModuleDict(\n",
       "             (weight): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=True)\n",
       "           )\n",
       "           (input_quantizers): ModuleList(\n",
       "             (0): None\n",
       "           )\n",
       "           (output_quantizers): ModuleList(\n",
       "             (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "           )\n",
       "         )\n",
       "         (bn): Module(\n",
       "           (module_batch_norm_62): FakeQuantizedBatchNorm(\n",
       "             (param_quantizers): ModuleDict()\n",
       "             (input_quantizers): ModuleList(\n",
       "               (0-4): 5 x QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "             )\n",
       "             (output_quantizers): ModuleList(\n",
       "               (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "             )\n",
       "           )\n",
       "         )\n",
       "         (activate): CustomSiLU(\n",
       "           (sigmoid): QuantizedSigmoid(\n",
       "             (param_quantizers): ModuleDict()\n",
       "             (input_quantizers): ModuleList(\n",
       "               (0): None\n",
       "             )\n",
       "             (output_quantizers): ModuleList(\n",
       "               (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "             )\n",
       "           )\n",
       "           (mul): QuantizedMultiply(\n",
       "             (param_quantizers): ModuleDict()\n",
       "             (input_quantizers): ModuleList(\n",
       "               (0-1): 2 x None\n",
       "             )\n",
       "             (output_quantizers): ModuleList(\n",
       "               (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "             )\n",
       "           )\n",
       "         )\n",
       "       )\n",
       "       (2): Module(\n",
       "         (conv): QuantizedConv2d(\n",
       "           384, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "           (param_quantizers): ModuleDict(\n",
       "             (weight): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=True)\n",
       "           )\n",
       "           (input_quantizers): ModuleList(\n",
       "             (0): None\n",
       "           )\n",
       "           (output_quantizers): ModuleList(\n",
       "             (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "           )\n",
       "         )\n",
       "         (bn): Module(\n",
       "           (module_batch_norm_63): FakeQuantizedBatchNorm(\n",
       "             (param_quantizers): ModuleDict()\n",
       "             (input_quantizers): ModuleList(\n",
       "               (0-4): 5 x QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "             )\n",
       "             (output_quantizers): ModuleList(\n",
       "               (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "             )\n",
       "           )\n",
       "         )\n",
       "         (activate): CustomSiLU(\n",
       "           (sigmoid): QuantizedSigmoid(\n",
       "             (param_quantizers): ModuleDict()\n",
       "             (input_quantizers): ModuleList(\n",
       "               (0): None\n",
       "             )\n",
       "             (output_quantizers): ModuleList(\n",
       "               (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "             )\n",
       "           )\n",
       "           (mul): QuantizedMultiply(\n",
       "             (param_quantizers): ModuleDict()\n",
       "             (input_quantizers): ModuleList(\n",
       "               (0-1): 2 x None\n",
       "             )\n",
       "             (output_quantizers): ModuleList(\n",
       "               (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "             )\n",
       "           )\n",
       "         )\n",
       "       )\n",
       "     )\n",
       "     (module_cat_5): FakeQuantizedConcat(\n",
       "       (param_quantizers): ModuleDict()\n",
       "       (input_quantizers): ModuleList(\n",
       "         (0): None\n",
       "       )\n",
       "       (output_quantizers): ModuleList(\n",
       "         (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "       )\n",
       "     )\n",
       "     (module_upsample_1): FakeQuantizedUpsample(\n",
       "       scale_factor=2.0, mode='nearest'\n",
       "       (param_quantizers): ModuleDict()\n",
       "       (input_quantizers): ModuleList(\n",
       "         (0): None\n",
       "       )\n",
       "       (output_quantizers): ModuleList(\n",
       "         (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "       )\n",
       "     )\n",
       "     (module_cat_7): FakeQuantizedConcat(\n",
       "       (param_quantizers): ModuleDict()\n",
       "       (input_quantizers): ModuleList(\n",
       "         (0): None\n",
       "       )\n",
       "       (output_quantizers): ModuleList(\n",
       "         (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "       )\n",
       "     )\n",
       "     (module_cat_9): FakeQuantizedConcat(\n",
       "       (param_quantizers): ModuleDict()\n",
       "       (input_quantizers): ModuleList(\n",
       "         (0): None\n",
       "       )\n",
       "       (output_quantizers): ModuleList(\n",
       "         (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "       )\n",
       "     )\n",
       "     (module_cat_11): FakeQuantizedConcat(\n",
       "       (param_quantizers): ModuleDict()\n",
       "       (input_quantizers): ModuleList(\n",
       "         (0): None\n",
       "       )\n",
       "       (output_quantizers): ModuleList(\n",
       "         (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "       )\n",
       "     )\n",
       "   )\n",
       "   (bbox_head): Module(\n",
       "     (cls_convs): Module(\n",
       "       (0): Module(\n",
       "         (0): Module(\n",
       "           (conv): QuantizedConv2d(\n",
       "             96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "             (param_quantizers): ModuleDict(\n",
       "               (weight): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=True)\n",
       "             )\n",
       "             (input_quantizers): ModuleList(\n",
       "               (0): None\n",
       "             )\n",
       "             (output_quantizers): ModuleList(\n",
       "               (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "             )\n",
       "           )\n",
       "           (bn): Module(\n",
       "             (module_batch_norm_64): FakeQuantizedBatchNorm(\n",
       "               (param_quantizers): ModuleDict()\n",
       "               (input_quantizers): ModuleList(\n",
       "                 (0-4): 5 x QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "               )\n",
       "               (output_quantizers): ModuleList(\n",
       "                 (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "               )\n",
       "             )\n",
       "           )\n",
       "           (activate): CustomSiLU(\n",
       "             (sigmoid): QuantizedSigmoid(\n",
       "               (param_quantizers): ModuleDict()\n",
       "               (input_quantizers): ModuleList(\n",
       "                 (0): None\n",
       "               )\n",
       "               (output_quantizers): ModuleList(\n",
       "                 (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "               )\n",
       "             )\n",
       "             (mul): QuantizedMultiply(\n",
       "               (param_quantizers): ModuleDict()\n",
       "               (input_quantizers): ModuleList(\n",
       "                 (0-1): 2 x None\n",
       "               )\n",
       "               (output_quantizers): ModuleList(\n",
       "                 (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "               )\n",
       "             )\n",
       "           )\n",
       "           (module_conv_1): QuantizedConv2d(\n",
       "             96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "             (param_quantizers): ModuleDict(\n",
       "               (weight): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=True)\n",
       "             )\n",
       "             (input_quantizers): ModuleList(\n",
       "               (0): None\n",
       "             )\n",
       "             (output_quantizers): ModuleList(\n",
       "               (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "             )\n",
       "           )\n",
       "           (module_conv_2): QuantizedConv2d(\n",
       "             96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "             (param_quantizers): ModuleDict(\n",
       "               (weight): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=True)\n",
       "             )\n",
       "             (input_quantizers): ModuleList(\n",
       "               (0): None\n",
       "             )\n",
       "             (output_quantizers): ModuleList(\n",
       "               (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "             )\n",
       "           )\n",
       "         )\n",
       "         (1): Module(\n",
       "           (conv): QuantizedConv2d(\n",
       "             96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "             (param_quantizers): ModuleDict(\n",
       "               (weight): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=True)\n",
       "             )\n",
       "             (input_quantizers): ModuleList(\n",
       "               (0): None\n",
       "             )\n",
       "             (output_quantizers): ModuleList(\n",
       "               (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "             )\n",
       "           )\n",
       "           (bn): Module(\n",
       "             (module_batch_norm_65): FakeQuantizedBatchNorm(\n",
       "               (param_quantizers): ModuleDict()\n",
       "               (input_quantizers): ModuleList(\n",
       "                 (0-4): 5 x QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "               )\n",
       "               (output_quantizers): ModuleList(\n",
       "                 (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "               )\n",
       "             )\n",
       "           )\n",
       "           (activate): CustomSiLU(\n",
       "             (sigmoid): QuantizedSigmoid(\n",
       "               (param_quantizers): ModuleDict()\n",
       "               (input_quantizers): ModuleList(\n",
       "                 (0): None\n",
       "               )\n",
       "               (output_quantizers): ModuleList(\n",
       "                 (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "               )\n",
       "             )\n",
       "             (mul): QuantizedMultiply(\n",
       "               (param_quantizers): ModuleDict()\n",
       "               (input_quantizers): ModuleList(\n",
       "                 (0-1): 2 x None\n",
       "               )\n",
       "               (output_quantizers): ModuleList(\n",
       "                 (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "               )\n",
       "             )\n",
       "           )\n",
       "           (module_conv_1): QuantizedConv2d(\n",
       "             96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "             (param_quantizers): ModuleDict(\n",
       "               (weight): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=True)\n",
       "             )\n",
       "             (input_quantizers): ModuleList(\n",
       "               (0): None\n",
       "             )\n",
       "             (output_quantizers): ModuleList(\n",
       "               (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "             )\n",
       "           )\n",
       "           (module_conv_2): QuantizedConv2d(\n",
       "             96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "             (param_quantizers): ModuleDict(\n",
       "               (weight): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=True)\n",
       "             )\n",
       "             (input_quantizers): ModuleList(\n",
       "               (0): None\n",
       "             )\n",
       "             (output_quantizers): ModuleList(\n",
       "               (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "             )\n",
       "           )\n",
       "         )\n",
       "       )\n",
       "       (1): Module(\n",
       "         (0): Module(\n",
       "           (bn): Module(\n",
       "             (module_batch_norm_68): FakeQuantizedBatchNorm(\n",
       "               (param_quantizers): ModuleDict()\n",
       "               (input_quantizers): ModuleList(\n",
       "                 (0-4): 5 x QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "               )\n",
       "               (output_quantizers): ModuleList(\n",
       "                 (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "               )\n",
       "             )\n",
       "           )\n",
       "           (activate): CustomSiLU(\n",
       "             (sigmoid): QuantizedSigmoid(\n",
       "               (param_quantizers): ModuleDict()\n",
       "               (input_quantizers): ModuleList(\n",
       "                 (0): None\n",
       "               )\n",
       "               (output_quantizers): ModuleList(\n",
       "                 (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "               )\n",
       "             )\n",
       "             (mul): QuantizedMultiply(\n",
       "               (param_quantizers): ModuleDict()\n",
       "               (input_quantizers): ModuleList(\n",
       "                 (0-1): 2 x None\n",
       "               )\n",
       "               (output_quantizers): ModuleList(\n",
       "                 (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "               )\n",
       "             )\n",
       "           )\n",
       "         )\n",
       "         (1): Module(\n",
       "           (bn): Module(\n",
       "             (module_batch_norm_69): FakeQuantizedBatchNorm(\n",
       "               (param_quantizers): ModuleDict()\n",
       "               (input_quantizers): ModuleList(\n",
       "                 (0-4): 5 x QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "               )\n",
       "               (output_quantizers): ModuleList(\n",
       "                 (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "               )\n",
       "             )\n",
       "           )\n",
       "           (activate): CustomSiLU(\n",
       "             (sigmoid): QuantizedSigmoid(\n",
       "               (param_quantizers): ModuleDict()\n",
       "               (input_quantizers): ModuleList(\n",
       "                 (0): None\n",
       "               )\n",
       "               (output_quantizers): ModuleList(\n",
       "                 (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "               )\n",
       "             )\n",
       "             (mul): QuantizedMultiply(\n",
       "               (param_quantizers): ModuleDict()\n",
       "               (input_quantizers): ModuleList(\n",
       "                 (0-1): 2 x None\n",
       "               )\n",
       "               (output_quantizers): ModuleList(\n",
       "                 (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "               )\n",
       "             )\n",
       "           )\n",
       "         )\n",
       "       )\n",
       "       (2): Module(\n",
       "         (0): Module(\n",
       "           (bn): Module(\n",
       "             (module_batch_norm_72): FakeQuantizedBatchNorm(\n",
       "               (param_quantizers): ModuleDict()\n",
       "               (input_quantizers): ModuleList(\n",
       "                 (0-4): 5 x QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "               )\n",
       "               (output_quantizers): ModuleList(\n",
       "                 (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "               )\n",
       "             )\n",
       "           )\n",
       "           (activate): CustomSiLU(\n",
       "             (sigmoid): QuantizedSigmoid(\n",
       "               (param_quantizers): ModuleDict()\n",
       "               (input_quantizers): ModuleList(\n",
       "                 (0): None\n",
       "               )\n",
       "               (output_quantizers): ModuleList(\n",
       "                 (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "               )\n",
       "             )\n",
       "             (mul): QuantizedMultiply(\n",
       "               (param_quantizers): ModuleDict()\n",
       "               (input_quantizers): ModuleList(\n",
       "                 (0-1): 2 x None\n",
       "               )\n",
       "               (output_quantizers): ModuleList(\n",
       "                 (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "               )\n",
       "             )\n",
       "           )\n",
       "         )\n",
       "         (1): Module(\n",
       "           (bn): Module(\n",
       "             (module_batch_norm_73): FakeQuantizedBatchNorm(\n",
       "               (param_quantizers): ModuleDict()\n",
       "               (input_quantizers): ModuleList(\n",
       "                 (0-4): 5 x QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "               )\n",
       "               (output_quantizers): ModuleList(\n",
       "                 (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "               )\n",
       "             )\n",
       "           )\n",
       "           (activate): CustomSiLU(\n",
       "             (sigmoid): QuantizedSigmoid(\n",
       "               (param_quantizers): ModuleDict()\n",
       "               (input_quantizers): ModuleList(\n",
       "                 (0): None\n",
       "               )\n",
       "               (output_quantizers): ModuleList(\n",
       "                 (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "               )\n",
       "             )\n",
       "             (mul): QuantizedMultiply(\n",
       "               (param_quantizers): ModuleDict()\n",
       "               (input_quantizers): ModuleList(\n",
       "                 (0-1): 2 x None\n",
       "               )\n",
       "               (output_quantizers): ModuleList(\n",
       "                 (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "               )\n",
       "             )\n",
       "           )\n",
       "         )\n",
       "       )\n",
       "     )\n",
       "     (rtm_cls): Module(\n",
       "       (0): QuantizedConv2d(\n",
       "         96, 80, kernel_size=(1, 1), stride=(1, 1)\n",
       "         (param_quantizers): ModuleDict(\n",
       "           (weight): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=True)\n",
       "           (bias): None\n",
       "         )\n",
       "         (input_quantizers): ModuleList(\n",
       "           (0): None\n",
       "         )\n",
       "         (output_quantizers): ModuleList(\n",
       "           (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "         )\n",
       "       )\n",
       "       (1): QuantizedConv2d(\n",
       "         96, 80, kernel_size=(1, 1), stride=(1, 1)\n",
       "         (param_quantizers): ModuleDict(\n",
       "           (weight): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=True)\n",
       "           (bias): None\n",
       "         )\n",
       "         (input_quantizers): ModuleList(\n",
       "           (0): None\n",
       "         )\n",
       "         (output_quantizers): ModuleList(\n",
       "           (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "         )\n",
       "       )\n",
       "       (2): QuantizedConv2d(\n",
       "         96, 80, kernel_size=(1, 1), stride=(1, 1)\n",
       "         (param_quantizers): ModuleDict(\n",
       "           (weight): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=True)\n",
       "           (bias): None\n",
       "         )\n",
       "         (input_quantizers): ModuleList(\n",
       "           (0): None\n",
       "         )\n",
       "         (output_quantizers): ModuleList(\n",
       "           (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "         )\n",
       "       )\n",
       "     )\n",
       "     (reg_convs): Module(\n",
       "       (0): Module(\n",
       "         (0): Module(\n",
       "           (conv): QuantizedConv2d(\n",
       "             96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "             (param_quantizers): ModuleDict(\n",
       "               (weight): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=True)\n",
       "             )\n",
       "             (input_quantizers): ModuleList(\n",
       "               (0): None\n",
       "             )\n",
       "             (output_quantizers): ModuleList(\n",
       "               (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "             )\n",
       "           )\n",
       "           (bn): Module(\n",
       "             (module_batch_norm_66): FakeQuantizedBatchNorm(\n",
       "               (param_quantizers): ModuleDict()\n",
       "               (input_quantizers): ModuleList(\n",
       "                 (0-4): 5 x QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "               )\n",
       "               (output_quantizers): ModuleList(\n",
       "                 (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "               )\n",
       "             )\n",
       "           )\n",
       "           (activate): CustomSiLU(\n",
       "             (sigmoid): QuantizedSigmoid(\n",
       "               (param_quantizers): ModuleDict()\n",
       "               (input_quantizers): ModuleList(\n",
       "                 (0): None\n",
       "               )\n",
       "               (output_quantizers): ModuleList(\n",
       "                 (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "               )\n",
       "             )\n",
       "             (mul): QuantizedMultiply(\n",
       "               (param_quantizers): ModuleDict()\n",
       "               (input_quantizers): ModuleList(\n",
       "                 (0-1): 2 x None\n",
       "               )\n",
       "               (output_quantizers): ModuleList(\n",
       "                 (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "               )\n",
       "             )\n",
       "           )\n",
       "           (module_conv_1): QuantizedConv2d(\n",
       "             96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "             (param_quantizers): ModuleDict(\n",
       "               (weight): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=True)\n",
       "             )\n",
       "             (input_quantizers): ModuleList(\n",
       "               (0): None\n",
       "             )\n",
       "             (output_quantizers): ModuleList(\n",
       "               (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "             )\n",
       "           )\n",
       "           (module_conv_2): QuantizedConv2d(\n",
       "             96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "             (param_quantizers): ModuleDict(\n",
       "               (weight): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=True)\n",
       "             )\n",
       "             (input_quantizers): ModuleList(\n",
       "               (0): None\n",
       "             )\n",
       "             (output_quantizers): ModuleList(\n",
       "               (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "             )\n",
       "           )\n",
       "         )\n",
       "         (1): Module(\n",
       "           (conv): QuantizedConv2d(\n",
       "             96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "             (param_quantizers): ModuleDict(\n",
       "               (weight): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=True)\n",
       "             )\n",
       "             (input_quantizers): ModuleList(\n",
       "               (0): None\n",
       "             )\n",
       "             (output_quantizers): ModuleList(\n",
       "               (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "             )\n",
       "           )\n",
       "           (bn): Module(\n",
       "             (module_batch_norm_67): FakeQuantizedBatchNorm(\n",
       "               (param_quantizers): ModuleDict()\n",
       "               (input_quantizers): ModuleList(\n",
       "                 (0-4): 5 x QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "               )\n",
       "               (output_quantizers): ModuleList(\n",
       "                 (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "               )\n",
       "             )\n",
       "           )\n",
       "           (activate): CustomSiLU(\n",
       "             (sigmoid): QuantizedSigmoid(\n",
       "               (param_quantizers): ModuleDict()\n",
       "               (input_quantizers): ModuleList(\n",
       "                 (0): None\n",
       "               )\n",
       "               (output_quantizers): ModuleList(\n",
       "                 (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "               )\n",
       "             )\n",
       "             (mul): QuantizedMultiply(\n",
       "               (param_quantizers): ModuleDict()\n",
       "               (input_quantizers): ModuleList(\n",
       "                 (0-1): 2 x None\n",
       "               )\n",
       "               (output_quantizers): ModuleList(\n",
       "                 (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "               )\n",
       "             )\n",
       "           )\n",
       "           (module_conv_1): QuantizedConv2d(\n",
       "             96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "             (param_quantizers): ModuleDict(\n",
       "               (weight): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=True)\n",
       "             )\n",
       "             (input_quantizers): ModuleList(\n",
       "               (0): None\n",
       "             )\n",
       "             (output_quantizers): ModuleList(\n",
       "               (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "             )\n",
       "           )\n",
       "           (module_conv_2): QuantizedConv2d(\n",
       "             96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "             (param_quantizers): ModuleDict(\n",
       "               (weight): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=True)\n",
       "             )\n",
       "             (input_quantizers): ModuleList(\n",
       "               (0): None\n",
       "             )\n",
       "             (output_quantizers): ModuleList(\n",
       "               (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "             )\n",
       "           )\n",
       "         )\n",
       "       )\n",
       "       (1): Module(\n",
       "         (0): Module(\n",
       "           (bn): Module(\n",
       "             (module_batch_norm_70): FakeQuantizedBatchNorm(\n",
       "               (param_quantizers): ModuleDict()\n",
       "               (input_quantizers): ModuleList(\n",
       "                 (0-4): 5 x QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "               )\n",
       "               (output_quantizers): ModuleList(\n",
       "                 (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "               )\n",
       "             )\n",
       "           )\n",
       "           (activate): CustomSiLU(\n",
       "             (sigmoid): QuantizedSigmoid(\n",
       "               (param_quantizers): ModuleDict()\n",
       "               (input_quantizers): ModuleList(\n",
       "                 (0): None\n",
       "               )\n",
       "               (output_quantizers): ModuleList(\n",
       "                 (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "               )\n",
       "             )\n",
       "             (mul): QuantizedMultiply(\n",
       "               (param_quantizers): ModuleDict()\n",
       "               (input_quantizers): ModuleList(\n",
       "                 (0-1): 2 x None\n",
       "               )\n",
       "               (output_quantizers): ModuleList(\n",
       "                 (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "               )\n",
       "             )\n",
       "           )\n",
       "         )\n",
       "         (1): Module(\n",
       "           (bn): Module(\n",
       "             (module_batch_norm_71): FakeQuantizedBatchNorm(\n",
       "               (param_quantizers): ModuleDict()\n",
       "               (input_quantizers): ModuleList(\n",
       "                 (0-4): 5 x QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "               )\n",
       "               (output_quantizers): ModuleList(\n",
       "                 (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "               )\n",
       "             )\n",
       "           )\n",
       "           (activate): CustomSiLU(\n",
       "             (sigmoid): QuantizedSigmoid(\n",
       "               (param_quantizers): ModuleDict()\n",
       "               (input_quantizers): ModuleList(\n",
       "                 (0): None\n",
       "               )\n",
       "               (output_quantizers): ModuleList(\n",
       "                 (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "               )\n",
       "             )\n",
       "             (mul): QuantizedMultiply(\n",
       "               (param_quantizers): ModuleDict()\n",
       "               (input_quantizers): ModuleList(\n",
       "                 (0-1): 2 x None\n",
       "               )\n",
       "               (output_quantizers): ModuleList(\n",
       "                 (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "               )\n",
       "             )\n",
       "           )\n",
       "         )\n",
       "       )\n",
       "       (2): Module(\n",
       "         (0): Module(\n",
       "           (bn): Module(\n",
       "             (module_batch_norm_74): FakeQuantizedBatchNorm(\n",
       "               (param_quantizers): ModuleDict()\n",
       "               (input_quantizers): ModuleList(\n",
       "                 (0-4): 5 x QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "               )\n",
       "               (output_quantizers): ModuleList(\n",
       "                 (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "               )\n",
       "             )\n",
       "           )\n",
       "           (activate): CustomSiLU(\n",
       "             (sigmoid): QuantizedSigmoid(\n",
       "               (param_quantizers): ModuleDict()\n",
       "               (input_quantizers): ModuleList(\n",
       "                 (0): None\n",
       "               )\n",
       "               (output_quantizers): ModuleList(\n",
       "                 (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "               )\n",
       "             )\n",
       "             (mul): QuantizedMultiply(\n",
       "               (param_quantizers): ModuleDict()\n",
       "               (input_quantizers): ModuleList(\n",
       "                 (0-1): 2 x None\n",
       "               )\n",
       "               (output_quantizers): ModuleList(\n",
       "                 (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "               )\n",
       "             )\n",
       "           )\n",
       "         )\n",
       "         (1): Module(\n",
       "           (bn): Module(\n",
       "             (module_batch_norm_75): FakeQuantizedBatchNorm(\n",
       "               (param_quantizers): ModuleDict()\n",
       "               (input_quantizers): ModuleList(\n",
       "                 (0-4): 5 x QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "               )\n",
       "               (output_quantizers): ModuleList(\n",
       "                 (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "               )\n",
       "             )\n",
       "           )\n",
       "           (activate): CustomSiLU(\n",
       "             (sigmoid): QuantizedSigmoid(\n",
       "               (param_quantizers): ModuleDict()\n",
       "               (input_quantizers): ModuleList(\n",
       "                 (0): None\n",
       "               )\n",
       "               (output_quantizers): ModuleList(\n",
       "                 (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "               )\n",
       "             )\n",
       "             (mul): QuantizedMultiply(\n",
       "               (param_quantizers): ModuleDict()\n",
       "               (input_quantizers): ModuleList(\n",
       "                 (0-1): 2 x None\n",
       "               )\n",
       "               (output_quantizers): ModuleList(\n",
       "                 (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "               )\n",
       "             )\n",
       "           )\n",
       "         )\n",
       "       )\n",
       "     )\n",
       "     (rtm_reg): Module(\n",
       "       (0): QuantizedConv2d(\n",
       "         96, 4, kernel_size=(1, 1), stride=(1, 1)\n",
       "         (param_quantizers): ModuleDict(\n",
       "           (weight): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=True)\n",
       "           (bias): None\n",
       "         )\n",
       "         (input_quantizers): ModuleList(\n",
       "           (0): None\n",
       "         )\n",
       "         (output_quantizers): ModuleList(\n",
       "           (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "         )\n",
       "       )\n",
       "       (1): QuantizedConv2d(\n",
       "         96, 4, kernel_size=(1, 1), stride=(1, 1)\n",
       "         (param_quantizers): ModuleDict(\n",
       "           (weight): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=True)\n",
       "           (bias): None\n",
       "         )\n",
       "         (input_quantizers): ModuleList(\n",
       "           (0): None\n",
       "         )\n",
       "         (output_quantizers): ModuleList(\n",
       "           (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "         )\n",
       "       )\n",
       "       (2): QuantizedConv2d(\n",
       "         96, 4, kernel_size=(1, 1), stride=(1, 1)\n",
       "         (param_quantizers): ModuleDict(\n",
       "           (weight): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=True)\n",
       "           (bias): None\n",
       "         )\n",
       "         (input_quantizers): ModuleList(\n",
       "           (0): None\n",
       "         )\n",
       "         (output_quantizers): ModuleList(\n",
       "           (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "         )\n",
       "       )\n",
       "     )\n",
       "     (module_mul_4): QuantizedMultiply(\n",
       "       (param_quantizers): ModuleDict()\n",
       "       (input_quantizers): ModuleList(\n",
       "         (0): None\n",
       "         (1): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "       )\n",
       "       (output_quantizers): ModuleList(\n",
       "         (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "       )\n",
       "     )\n",
       "     (module_mul_5): QuantizedMultiply(\n",
       "       (param_quantizers): ModuleDict()\n",
       "       (input_quantizers): ModuleList(\n",
       "         (0): None\n",
       "         (1): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "       )\n",
       "       (output_quantizers): ModuleList(\n",
       "         (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "       )\n",
       "     )\n",
       "     (module_mul_6): QuantizedMultiply(\n",
       "       (param_quantizers): ModuleDict()\n",
       "       (input_quantizers): ModuleList(\n",
       "         (0): None\n",
       "         (1): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "       )\n",
       "       (output_quantizers): ModuleList(\n",
       "         (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "       )\n",
       "     )\n",
       "   )\n",
       " ),\n",
       " 'backbone': Module(\n",
       "   (stem): Module(\n",
       "     (0): Module(\n",
       "       (conv): QuantizedConv2d(\n",
       "         3, 12, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False\n",
       "         (param_quantizers): ModuleDict(\n",
       "           (weight): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=True)\n",
       "         )\n",
       "         (input_quantizers): ModuleList(\n",
       "           (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "         )\n",
       "         (output_quantizers): ModuleList(\n",
       "           (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "         )\n",
       "       )\n",
       "       (bn): Module(\n",
       "         (module_batch_norm): FakeQuantizedBatchNorm(\n",
       "           (param_quantizers): ModuleDict()\n",
       "           (input_quantizers): ModuleList(\n",
       "             (0-4): 5 x QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "           )\n",
       "           (output_quantizers): ModuleList(\n",
       "             (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "           )\n",
       "         )\n",
       "       )\n",
       "       (activate): CustomSiLU(\n",
       "         (sigmoid): QuantizedSigmoid(\n",
       "           (param_quantizers): ModuleDict()\n",
       "           (input_quantizers): ModuleList(\n",
       "             (0): None\n",
       "           )\n",
       "           (output_quantizers): ModuleList(\n",
       "             (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "           )\n",
       "         )\n",
       "         (mul): QuantizedMultiply(\n",
       "           (param_quantizers): ModuleDict()\n",
       "           (input_quantizers): ModuleList(\n",
       "             (0-1): 2 x None\n",
       "           )\n",
       "           (output_quantizers): ModuleList(\n",
       "             (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "           )\n",
       "         )\n",
       "       )\n",
       "     )\n",
       "     (1): Module(\n",
       "       (conv): QuantizedConv2d(\n",
       "         12, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "         (param_quantizers): ModuleDict(\n",
       "           (weight): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=True)\n",
       "         )\n",
       "         (input_quantizers): ModuleList(\n",
       "           (0): None\n",
       "         )\n",
       "         (output_quantizers): ModuleList(\n",
       "           (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "         )\n",
       "       )\n",
       "       (bn): Module(\n",
       "         (module_batch_norm_1): FakeQuantizedBatchNorm(\n",
       "           (param_quantizers): ModuleDict()\n",
       "           (input_quantizers): ModuleList(\n",
       "             (0-4): 5 x QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "           )\n",
       "           (output_quantizers): ModuleList(\n",
       "             (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "           )\n",
       "         )\n",
       "       )\n",
       "       (activate): CustomSiLU(\n",
       "         (sigmoid): QuantizedSigmoid(\n",
       "           (param_quantizers): ModuleDict()\n",
       "           (input_quantizers): ModuleList(\n",
       "             (0): None\n",
       "           )\n",
       "           (output_quantizers): ModuleList(\n",
       "             (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "           )\n",
       "         )\n",
       "         (mul): QuantizedMultiply(\n",
       "           (param_quantizers): ModuleDict()\n",
       "           (input_quantizers): ModuleList(\n",
       "             (0-1): 2 x None\n",
       "           )\n",
       "           (output_quantizers): ModuleList(\n",
       "             (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "           )\n",
       "         )\n",
       "       )\n",
       "     )\n",
       "     (2): Module(\n",
       "       (conv): QuantizedConv2d(\n",
       "         12, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "         (param_quantizers): ModuleDict(\n",
       "           (weight): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=True)\n",
       "         )\n",
       "         (input_quantizers): ModuleList(\n",
       "           (0): None\n",
       "         )\n",
       "         (output_quantizers): ModuleList(\n",
       "           (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "         )\n",
       "       )\n",
       "       (bn): Module(\n",
       "         (module_batch_norm_2): FakeQuantizedBatchNorm(\n",
       "           (param_quantizers): ModuleDict()\n",
       "           (input_quantizers): ModuleList(\n",
       "             (0-4): 5 x QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "           )\n",
       "           (output_quantizers): ModuleList(\n",
       "             (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "           )\n",
       "         )\n",
       "       )\n",
       "       (activate): CustomSiLU(\n",
       "         (sigmoid): QuantizedSigmoid(\n",
       "           (param_quantizers): ModuleDict()\n",
       "           (input_quantizers): ModuleList(\n",
       "             (0): None\n",
       "           )\n",
       "           (output_quantizers): ModuleList(\n",
       "             (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "           )\n",
       "         )\n",
       "         (mul): QuantizedMultiply(\n",
       "           (param_quantizers): ModuleDict()\n",
       "           (input_quantizers): ModuleList(\n",
       "             (0-1): 2 x None\n",
       "           )\n",
       "           (output_quantizers): ModuleList(\n",
       "             (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "           )\n",
       "         )\n",
       "       )\n",
       "     )\n",
       "   )\n",
       "   (stage1): Module(\n",
       "     (0): Module(\n",
       "       (conv): QuantizedConv2d(\n",
       "         24, 48, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False\n",
       "         (param_quantizers): ModuleDict(\n",
       "           (weight): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=True)\n",
       "         )\n",
       "         (input_quantizers): ModuleList(\n",
       "           (0): None\n",
       "         )\n",
       "         (output_quantizers): ModuleList(\n",
       "           (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "         )\n",
       "       )\n",
       "       (bn): Module(\n",
       "         (module_batch_norm_3): FakeQuantizedBatchNorm(\n",
       "           (param_quantizers): ModuleDict()\n",
       "           (input_quantizers): ModuleList(\n",
       "             (0-4): 5 x QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "           )\n",
       "           (output_quantizers): ModuleList(\n",
       "             (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "           )\n",
       "         )\n",
       "       )\n",
       "       (activate): CustomSiLU(\n",
       "         (sigmoid): QuantizedSigmoid(\n",
       "           (param_quantizers): ModuleDict()\n",
       "           (input_quantizers): ModuleList(\n",
       "             (0): None\n",
       "           )\n",
       "           (output_quantizers): ModuleList(\n",
       "             (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "           )\n",
       "         )\n",
       "         (mul): QuantizedMultiply(\n",
       "           (param_quantizers): ModuleDict()\n",
       "           (input_quantizers): ModuleList(\n",
       "             (0-1): 2 x None\n",
       "           )\n",
       "           (output_quantizers): ModuleList(\n",
       "             (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "           )\n",
       "         )\n",
       "       )\n",
       "     )\n",
       "     (1): Module(\n",
       "       (short_conv): Module(\n",
       "         (conv): QuantizedConv2d(\n",
       "           48, 24, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "           (param_quantizers): ModuleDict(\n",
       "             (weight): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=True)\n",
       "           )\n",
       "           (input_quantizers): ModuleList(\n",
       "             (0): None\n",
       "           )\n",
       "           (output_quantizers): ModuleList(\n",
       "             (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "           )\n",
       "         )\n",
       "         (bn): Module(\n",
       "           (module_batch_norm_4): FakeQuantizedBatchNorm(\n",
       "             (param_quantizers): ModuleDict()\n",
       "             (input_quantizers): ModuleList(\n",
       "               (0-4): 5 x QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "             )\n",
       "             (output_quantizers): ModuleList(\n",
       "               (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "             )\n",
       "           )\n",
       "         )\n",
       "         (activate): CustomSiLU(\n",
       "           (sigmoid): QuantizedSigmoid(\n",
       "             (param_quantizers): ModuleDict()\n",
       "             (input_quantizers): ModuleList(\n",
       "               (0): None\n",
       "             )\n",
       "             (output_quantizers): ModuleList(\n",
       "               (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "             )\n",
       "           )\n",
       "           (mul): QuantizedMultiply(\n",
       "             (param_quantizers): ModuleDict()\n",
       "             (input_quantizers): ModuleList(\n",
       "               (0-1): 2 x None\n",
       "             )\n",
       "             (output_quantizers): ModuleList(\n",
       "               (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "             )\n",
       "           )\n",
       "         )\n",
       "       )\n",
       "       (main_conv): Module(\n",
       "         (conv): QuantizedConv2d(\n",
       "           48, 24, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "           (param_quantizers): ModuleDict(\n",
       "             (weight): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=True)\n",
       "           )\n",
       "           (input_quantizers): ModuleList(\n",
       "             (0): None\n",
       "           )\n",
       "           (output_quantizers): ModuleList(\n",
       "             (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "           )\n",
       "         )\n",
       "         (bn): Module(\n",
       "           (module_batch_norm_5): FakeQuantizedBatchNorm(\n",
       "             (param_quantizers): ModuleDict()\n",
       "             (input_quantizers): ModuleList(\n",
       "               (0-4): 5 x QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "             )\n",
       "             (output_quantizers): ModuleList(\n",
       "               (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "             )\n",
       "           )\n",
       "         )\n",
       "         (activate): CustomSiLU(\n",
       "           (sigmoid): QuantizedSigmoid(\n",
       "             (param_quantizers): ModuleDict()\n",
       "             (input_quantizers): ModuleList(\n",
       "               (0): None\n",
       "             )\n",
       "             (output_quantizers): ModuleList(\n",
       "               (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "             )\n",
       "           )\n",
       "           (mul): QuantizedMultiply(\n",
       "             (param_quantizers): ModuleDict()\n",
       "             (input_quantizers): ModuleList(\n",
       "               (0-1): 2 x None\n",
       "             )\n",
       "             (output_quantizers): ModuleList(\n",
       "               (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "             )\n",
       "           )\n",
       "         )\n",
       "       )\n",
       "       (blocks): Module(\n",
       "         (0): Module(\n",
       "           (conv1): Module(\n",
       "             (conv): QuantizedConv2d(\n",
       "               24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "               (param_quantizers): ModuleDict(\n",
       "                 (weight): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=True)\n",
       "               )\n",
       "               (input_quantizers): ModuleList(\n",
       "                 (0): None\n",
       "               )\n",
       "               (output_quantizers): ModuleList(\n",
       "                 (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "               )\n",
       "             )\n",
       "             (bn): Module(\n",
       "               (module_batch_norm_6): FakeQuantizedBatchNorm(\n",
       "                 (param_quantizers): ModuleDict()\n",
       "                 (input_quantizers): ModuleList(\n",
       "                   (0-4): 5 x QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "                 )\n",
       "                 (output_quantizers): ModuleList(\n",
       "                   (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "                 )\n",
       "               )\n",
       "             )\n",
       "             (activate): CustomSiLU(\n",
       "               (sigmoid): QuantizedSigmoid(\n",
       "                 (param_quantizers): ModuleDict()\n",
       "                 (input_quantizers): ModuleList(\n",
       "                   (0): None\n",
       "                 )\n",
       "                 (output_quantizers): ModuleList(\n",
       "                   (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "                 )\n",
       "               )\n",
       "               (mul): QuantizedMultiply(\n",
       "                 (param_quantizers): ModuleDict()\n",
       "                 (input_quantizers): ModuleList(\n",
       "                   (0-1): 2 x None\n",
       "                 )\n",
       "                 (output_quantizers): ModuleList(\n",
       "                   (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "                 )\n",
       "               )\n",
       "             )\n",
       "           )\n",
       "           (conv2): Module(\n",
       "             (depthwise_conv): Module(\n",
       "               (conv): QuantizedConv2d(\n",
       "                 24, 24, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=24, bias=False\n",
       "                 (param_quantizers): ModuleDict(\n",
       "                   (weight): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=True)\n",
       "                 )\n",
       "                 (input_quantizers): ModuleList(\n",
       "                   (0): None\n",
       "                 )\n",
       "                 (output_quantizers): ModuleList(\n",
       "                   (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "                 )\n",
       "               )\n",
       "               (bn): Module(\n",
       "                 (module_batch_norm_7): BatchNorm()\n",
       "               )\n",
       "               (activate): CustomSiLU(\n",
       "                 (sigmoid): QuantizedSigmoid(\n",
       "                   (param_quantizers): ModuleDict()\n",
       "                   (input_quantizers): ModuleList(\n",
       "                     (0): None\n",
       "                   )\n",
       "                   (output_quantizers): ModuleList(\n",
       "                     (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "                   )\n",
       "                 )\n",
       "                 (mul): QuantizedMultiply(\n",
       "                   (param_quantizers): ModuleDict()\n",
       "                   (input_quantizers): ModuleList(\n",
       "                     (0-1): 2 x None\n",
       "                   )\n",
       "                   (output_quantizers): ModuleList(\n",
       "                     (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "                   )\n",
       "                 )\n",
       "               )\n",
       "             )\n",
       "             (pointwise_conv): Module(\n",
       "               (conv): QuantizedConv2d(\n",
       "                 24, 24, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "                 (param_quantizers): ModuleDict(\n",
       "                   (weight): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=True)\n",
       "                 )\n",
       "                 (input_quantizers): ModuleList(\n",
       "                   (0): None\n",
       "                 )\n",
       "                 (output_quantizers): ModuleList(\n",
       "                   (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "                 )\n",
       "               )\n",
       "               (bn): Module(\n",
       "                 (module_batch_norm_8): FakeQuantizedBatchNorm(\n",
       "                   (param_quantizers): ModuleDict()\n",
       "                   (input_quantizers): ModuleList(\n",
       "                     (0-4): 5 x QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "                   )\n",
       "                   (output_quantizers): ModuleList(\n",
       "                     (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "                   )\n",
       "                 )\n",
       "               )\n",
       "               (activate): CustomSiLU(\n",
       "                 (sigmoid): QuantizedSigmoid(\n",
       "                   (param_quantizers): ModuleDict()\n",
       "                   (input_quantizers): ModuleList(\n",
       "                     (0): None\n",
       "                   )\n",
       "                   (output_quantizers): ModuleList(\n",
       "                     (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "                   )\n",
       "                 )\n",
       "                 (mul): QuantizedMultiply(\n",
       "                   (param_quantizers): ModuleDict()\n",
       "                   (input_quantizers): ModuleList(\n",
       "                     (0-1): 2 x None\n",
       "                   )\n",
       "                   (output_quantizers): ModuleList(\n",
       "                     (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "                   )\n",
       "                 )\n",
       "               )\n",
       "             )\n",
       "           )\n",
       "           (module_add): QuantizedAdd(\n",
       "             (param_quantizers): ModuleDict()\n",
       "             (input_quantizers): ModuleList(\n",
       "               (0-1): 2 x None\n",
       "             )\n",
       "             (output_quantizers): ModuleList(\n",
       "               (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "             )\n",
       "           )\n",
       "         )\n",
       "       )\n",
       "       (attention): Module(\n",
       "         (global_avgpool): FakeQuantizedAdaptiveAvgPool2d(\n",
       "           output_size=1\n",
       "           (param_quantizers): ModuleDict()\n",
       "           (input_quantizers): ModuleList(\n",
       "             (0): None\n",
       "           )\n",
       "           (output_quantizers): ModuleList(\n",
       "             (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "           )\n",
       "         )\n",
       "         (fc): QuantizedConv2d(\n",
       "           48, 48, kernel_size=(1, 1), stride=(1, 1)\n",
       "           (param_quantizers): ModuleDict(\n",
       "             (weight): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=True)\n",
       "             (bias): None\n",
       "           )\n",
       "           (input_quantizers): ModuleList(\n",
       "             (0): None\n",
       "           )\n",
       "           (output_quantizers): ModuleList(\n",
       "             (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "           )\n",
       "         )\n",
       "         (act): FakeQuantizedHardsigmoid(\n",
       "           (param_quantizers): ModuleDict()\n",
       "           (input_quantizers): ModuleList(\n",
       "             (0): None\n",
       "           )\n",
       "           (output_quantizers): ModuleList(\n",
       "             (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "           )\n",
       "         )\n",
       "         (module_mul): QuantizedMultiply(\n",
       "           (param_quantizers): ModuleDict()\n",
       "           (input_quantizers): ModuleList(\n",
       "             (0-1): 2 x None\n",
       "           )\n",
       "           (output_quantizers): ModuleList(\n",
       "             (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "           )\n",
       "         )\n",
       "       )\n",
       "       (final_conv): Module(\n",
       "         (conv): QuantizedConv2d(\n",
       "           48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "           (param_quantizers): ModuleDict(\n",
       "             (weight): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=True)\n",
       "           )\n",
       "           (input_quantizers): ModuleList(\n",
       "             (0): None\n",
       "           )\n",
       "           (output_quantizers): ModuleList(\n",
       "             (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "           )\n",
       "         )\n",
       "         (bn): Module(\n",
       "           (module_batch_norm_9): FakeQuantizedBatchNorm(\n",
       "             (param_quantizers): ModuleDict()\n",
       "             (input_quantizers): ModuleList(\n",
       "               (0-4): 5 x QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "             )\n",
       "             (output_quantizers): ModuleList(\n",
       "               (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "             )\n",
       "           )\n",
       "         )\n",
       "         (activate): CustomSiLU(\n",
       "           (sigmoid): QuantizedSigmoid(\n",
       "             (param_quantizers): ModuleDict()\n",
       "             (input_quantizers): ModuleList(\n",
       "               (0): None\n",
       "             )\n",
       "             (output_quantizers): ModuleList(\n",
       "               (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "             )\n",
       "           )\n",
       "           (mul): QuantizedMultiply(\n",
       "             (param_quantizers): ModuleDict()\n",
       "             (input_quantizers): ModuleList(\n",
       "               (0-1): 2 x None\n",
       "             )\n",
       "             (output_quantizers): ModuleList(\n",
       "               (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "             )\n",
       "           )\n",
       "         )\n",
       "       )\n",
       "       (module_cat): FakeQuantizedConcat(\n",
       "         (param_quantizers): ModuleDict()\n",
       "         (input_quantizers): ModuleList(\n",
       "           (0): None\n",
       "         )\n",
       "         (output_quantizers): ModuleList(\n",
       "           (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "         )\n",
       "       )\n",
       "     )\n",
       "   )\n",
       "   (stage2): Module(\n",
       "     (0): Module(\n",
       "       (conv): QuantizedConv2d(\n",
       "         48, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False\n",
       "         (param_quantizers): ModuleDict(\n",
       "           (weight): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=True)\n",
       "         )\n",
       "         (input_quantizers): ModuleList(\n",
       "           (0): None\n",
       "         )\n",
       "         (output_quantizers): ModuleList(\n",
       "           (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "         )\n",
       "       )\n",
       "       (bn): Module(\n",
       "         (module_batch_norm_10): FakeQuantizedBatchNorm(\n",
       "           (param_quantizers): ModuleDict()\n",
       "           (input_quantizers): ModuleList(\n",
       "             (0-4): 5 x QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "           )\n",
       "           (output_quantizers): ModuleList(\n",
       "             (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "           )\n",
       "         )\n",
       "       )\n",
       "       (activate): CustomSiLU(\n",
       "         (sigmoid): QuantizedSigmoid(\n",
       "           (param_quantizers): ModuleDict()\n",
       "           (input_quantizers): ModuleList(\n",
       "             (0): None\n",
       "           )\n",
       "           (output_quantizers): ModuleList(\n",
       "             (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "           )\n",
       "         )\n",
       "         (mul): QuantizedMultiply(\n",
       "           (param_quantizers): ModuleDict()\n",
       "           (input_quantizers): ModuleList(\n",
       "             (0-1): 2 x None\n",
       "           )\n",
       "           (output_quantizers): ModuleList(\n",
       "             (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "           )\n",
       "         )\n",
       "       )\n",
       "     )\n",
       "     (1): Module(\n",
       "       (short_conv): Module(\n",
       "         (conv): QuantizedConv2d(\n",
       "           96, 48, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "           (param_quantizers): ModuleDict(\n",
       "             (weight): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=True)\n",
       "           )\n",
       "           (input_quantizers): ModuleList(\n",
       "             (0): None\n",
       "           )\n",
       "           (output_quantizers): ModuleList(\n",
       "             (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "           )\n",
       "         )\n",
       "         (bn): Module(\n",
       "           (module_batch_norm_11): FakeQuantizedBatchNorm(\n",
       "             (param_quantizers): ModuleDict()\n",
       "             (input_quantizers): ModuleList(\n",
       "               (0-4): 5 x QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "             )\n",
       "             (output_quantizers): ModuleList(\n",
       "               (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "             )\n",
       "           )\n",
       "         )\n",
       "         (activate): CustomSiLU(\n",
       "           (sigmoid): QuantizedSigmoid(\n",
       "             (param_quantizers): ModuleDict()\n",
       "             (input_quantizers): ModuleList(\n",
       "               (0): None\n",
       "             )\n",
       "             (output_quantizers): ModuleList(\n",
       "               (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "             )\n",
       "           )\n",
       "           (mul): QuantizedMultiply(\n",
       "             (param_quantizers): ModuleDict()\n",
       "             (input_quantizers): ModuleList(\n",
       "               (0-1): 2 x None\n",
       "             )\n",
       "             (output_quantizers): ModuleList(\n",
       "               (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "             )\n",
       "           )\n",
       "         )\n",
       "       )\n",
       "       (main_conv): Module(\n",
       "         (conv): QuantizedConv2d(\n",
       "           96, 48, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "           (param_quantizers): ModuleDict(\n",
       "             (weight): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=True)\n",
       "           )\n",
       "           (input_quantizers): ModuleList(\n",
       "             (0): None\n",
       "           )\n",
       "           (output_quantizers): ModuleList(\n",
       "             (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "           )\n",
       "         )\n",
       "         (bn): Module(\n",
       "           (module_batch_norm_12): FakeQuantizedBatchNorm(\n",
       "             (param_quantizers): ModuleDict()\n",
       "             (input_quantizers): ModuleList(\n",
       "               (0-4): 5 x QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "             )\n",
       "             (output_quantizers): ModuleList(\n",
       "               (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "             )\n",
       "           )\n",
       "         )\n",
       "         (activate): CustomSiLU(\n",
       "           (sigmoid): QuantizedSigmoid(\n",
       "             (param_quantizers): ModuleDict()\n",
       "             (input_quantizers): ModuleList(\n",
       "               (0): None\n",
       "             )\n",
       "             (output_quantizers): ModuleList(\n",
       "               (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "             )\n",
       "           )\n",
       "           (mul): QuantizedMultiply(\n",
       "             (param_quantizers): ModuleDict()\n",
       "             (input_quantizers): ModuleList(\n",
       "               (0-1): 2 x None\n",
       "             )\n",
       "             (output_quantizers): ModuleList(\n",
       "               (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "             )\n",
       "           )\n",
       "         )\n",
       "       )\n",
       "       (blocks): Module(\n",
       "         (0): Module(\n",
       "           (conv1): Module(\n",
       "             (conv): QuantizedConv2d(\n",
       "               48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "               (param_quantizers): ModuleDict(\n",
       "                 (weight): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=True)\n",
       "               )\n",
       "               (input_quantizers): ModuleList(\n",
       "                 (0): None\n",
       "               )\n",
       "               (output_quantizers): ModuleList(\n",
       "                 (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "               )\n",
       "             )\n",
       "             (bn): Module(\n",
       "               (module_batch_norm_13): FakeQuantizedBatchNorm(\n",
       "                 (param_quantizers): ModuleDict()\n",
       "                 (input_quantizers): ModuleList(\n",
       "                   (0-4): 5 x QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "                 )\n",
       "                 (output_quantizers): ModuleList(\n",
       "                   (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "                 )\n",
       "               )\n",
       "             )\n",
       "             (activate): CustomSiLU(\n",
       "               (sigmoid): QuantizedSigmoid(\n",
       "                 (param_quantizers): ModuleDict()\n",
       "                 (input_quantizers): ModuleList(\n",
       "                   (0): None\n",
       "                 )\n",
       "                 (output_quantizers): ModuleList(\n",
       "                   (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "                 )\n",
       "               )\n",
       "               (mul): QuantizedMultiply(\n",
       "                 (param_quantizers): ModuleDict()\n",
       "                 (input_quantizers): ModuleList(\n",
       "                   (0-1): 2 x None\n",
       "                 )\n",
       "                 (output_quantizers): ModuleList(\n",
       "                   (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "                 )\n",
       "               )\n",
       "             )\n",
       "           )\n",
       "           (conv2): Module(\n",
       "             (depthwise_conv): Module(\n",
       "               (conv): QuantizedConv2d(\n",
       "                 48, 48, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=48, bias=False\n",
       "                 (param_quantizers): ModuleDict(\n",
       "                   (weight): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=True)\n",
       "                 )\n",
       "                 (input_quantizers): ModuleList(\n",
       "                   (0): None\n",
       "                 )\n",
       "                 (output_quantizers): ModuleList(\n",
       "                   (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "                 )\n",
       "               )\n",
       "               (bn): Module(\n",
       "                 (module_batch_norm_14): BatchNorm()\n",
       "               )\n",
       "               (activate): CustomSiLU(\n",
       "                 (sigmoid): QuantizedSigmoid(\n",
       "                   (param_quantizers): ModuleDict()\n",
       "                   (input_quantizers): ModuleList(\n",
       "                     (0): None\n",
       "                   )\n",
       "                   (output_quantizers): ModuleList(\n",
       "                     (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "                   )\n",
       "                 )\n",
       "                 (mul): QuantizedMultiply(\n",
       "                   (param_quantizers): ModuleDict()\n",
       "                   (input_quantizers): ModuleList(\n",
       "                     (0-1): 2 x None\n",
       "                   )\n",
       "                   (output_quantizers): ModuleList(\n",
       "                     (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "                   )\n",
       "                 )\n",
       "               )\n",
       "             )\n",
       "             (pointwise_conv): Module(\n",
       "               (conv): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "               (bn): Module(\n",
       "                 (module_batch_norm_15): FakeQuantizedBatchNorm(\n",
       "                   (param_quantizers): ModuleDict()\n",
       "                   (input_quantizers): ModuleList(\n",
       "                     (0-4): 5 x QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "                   )\n",
       "                   (output_quantizers): ModuleList(\n",
       "                     (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "                   )\n",
       "                 )\n",
       "               )\n",
       "               (activate): CustomSiLU(\n",
       "                 (sigmoid): QuantizedSigmoid(\n",
       "                   (param_quantizers): ModuleDict()\n",
       "                   (input_quantizers): ModuleList(\n",
       "                     (0): None\n",
       "                   )\n",
       "                   (output_quantizers): ModuleList(\n",
       "                     (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "                   )\n",
       "                 )\n",
       "                 (mul): QuantizedMultiply(\n",
       "                   (param_quantizers): ModuleDict()\n",
       "                   (input_quantizers): ModuleList(\n",
       "                     (0-1): 2 x None\n",
       "                   )\n",
       "                   (output_quantizers): ModuleList(\n",
       "                     (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "                   )\n",
       "                 )\n",
       "               )\n",
       "             )\n",
       "           )\n",
       "           (module_add_1): QuantizedAdd(\n",
       "             (param_quantizers): ModuleDict()\n",
       "             (input_quantizers): ModuleList(\n",
       "               (0-1): 2 x None\n",
       "             )\n",
       "             (output_quantizers): ModuleList(\n",
       "               (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "             )\n",
       "           )\n",
       "         )\n",
       "       )\n",
       "       (attention): Module(\n",
       "         (global_avgpool): FakeQuantizedAdaptiveAvgPool2d(\n",
       "           output_size=1\n",
       "           (param_quantizers): ModuleDict()\n",
       "           (input_quantizers): ModuleList(\n",
       "             (0): None\n",
       "           )\n",
       "           (output_quantizers): ModuleList(\n",
       "             (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "           )\n",
       "         )\n",
       "         (fc): QuantizedConv2d(\n",
       "           96, 96, kernel_size=(1, 1), stride=(1, 1)\n",
       "           (param_quantizers): ModuleDict(\n",
       "             (weight): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=True)\n",
       "             (bias): None\n",
       "           )\n",
       "           (input_quantizers): ModuleList(\n",
       "             (0): None\n",
       "           )\n",
       "           (output_quantizers): ModuleList(\n",
       "             (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "           )\n",
       "         )\n",
       "         (act): FakeQuantizedHardsigmoid(\n",
       "           (param_quantizers): ModuleDict()\n",
       "           (input_quantizers): ModuleList(\n",
       "             (0): None\n",
       "           )\n",
       "           (output_quantizers): ModuleList(\n",
       "             (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "           )\n",
       "         )\n",
       "         (module_mul_1): QuantizedMultiply(\n",
       "           (param_quantizers): ModuleDict()\n",
       "           (input_quantizers): ModuleList(\n",
       "             (0-1): 2 x None\n",
       "           )\n",
       "           (output_quantizers): ModuleList(\n",
       "             (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "           )\n",
       "         )\n",
       "       )\n",
       "       (final_conv): Module(\n",
       "         (conv): QuantizedConv2d(\n",
       "           96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "           (param_quantizers): ModuleDict(\n",
       "             (weight): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=True)\n",
       "           )\n",
       "           (input_quantizers): ModuleList(\n",
       "             (0): None\n",
       "           )\n",
       "           (output_quantizers): ModuleList(\n",
       "             (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "           )\n",
       "         )\n",
       "         (bn): Module(\n",
       "           (module_batch_norm_16): FakeQuantizedBatchNorm(\n",
       "             (param_quantizers): ModuleDict()\n",
       "             (input_quantizers): ModuleList(\n",
       "               (0-4): 5 x QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "             )\n",
       "             (output_quantizers): ModuleList(\n",
       "               (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "             )\n",
       "           )\n",
       "         )\n",
       "         (activate): CustomSiLU(\n",
       "           (sigmoid): QuantizedSigmoid(\n",
       "             (param_quantizers): ModuleDict()\n",
       "             (input_quantizers): ModuleList(\n",
       "               (0): None\n",
       "             )\n",
       "             (output_quantizers): ModuleList(\n",
       "               (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "             )\n",
       "           )\n",
       "           (mul): QuantizedMultiply(\n",
       "             (param_quantizers): ModuleDict()\n",
       "             (input_quantizers): ModuleList(\n",
       "               (0-1): 2 x None\n",
       "             )\n",
       "             (output_quantizers): ModuleList(\n",
       "               (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "             )\n",
       "           )\n",
       "         )\n",
       "       )\n",
       "       (module_cat_1): FakeQuantizedConcat(\n",
       "         (param_quantizers): ModuleDict()\n",
       "         (input_quantizers): ModuleList(\n",
       "           (0): None\n",
       "         )\n",
       "         (output_quantizers): ModuleList(\n",
       "           (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "         )\n",
       "       )\n",
       "     )\n",
       "   )\n",
       "   (stage3): Module(\n",
       "     (0): Module(\n",
       "       (conv): QuantizedConv2d(\n",
       "         96, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False\n",
       "         (param_quantizers): ModuleDict(\n",
       "           (weight): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=True)\n",
       "         )\n",
       "         (input_quantizers): ModuleList(\n",
       "           (0): None\n",
       "         )\n",
       "         (output_quantizers): ModuleList(\n",
       "           (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "         )\n",
       "       )\n",
       "       (bn): Module(\n",
       "         (module_batch_norm_17): FakeQuantizedBatchNorm(\n",
       "           (param_quantizers): ModuleDict()\n",
       "           (input_quantizers): ModuleList(\n",
       "             (0-4): 5 x QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "           )\n",
       "           (output_quantizers): ModuleList(\n",
       "             (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "           )\n",
       "         )\n",
       "       )\n",
       "       (activate): CustomSiLU(\n",
       "         (sigmoid): QuantizedSigmoid(\n",
       "           (param_quantizers): ModuleDict()\n",
       "           (input_quantizers): ModuleList(\n",
       "             (0): None\n",
       "           )\n",
       "           (output_quantizers): ModuleList(\n",
       "             (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "           )\n",
       "         )\n",
       "         (mul): QuantizedMultiply(\n",
       "           (param_quantizers): ModuleDict()\n",
       "           (input_quantizers): ModuleList(\n",
       "             (0-1): 2 x None\n",
       "           )\n",
       "           (output_quantizers): ModuleList(\n",
       "             (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "           )\n",
       "         )\n",
       "       )\n",
       "     )\n",
       "     (1): Module(\n",
       "       (short_conv): Module(\n",
       "         (conv): QuantizedConv2d(\n",
       "           192, 96, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "           (param_quantizers): ModuleDict(\n",
       "             (weight): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=True)\n",
       "           )\n",
       "           (input_quantizers): ModuleList(\n",
       "             (0): None\n",
       "           )\n",
       "           (output_quantizers): ModuleList(\n",
       "             (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "           )\n",
       "         )\n",
       "         (bn): Module(\n",
       "           (module_batch_norm_18): FakeQuantizedBatchNorm(\n",
       "             (param_quantizers): ModuleDict()\n",
       "             (input_quantizers): ModuleList(\n",
       "               (0-4): 5 x QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "             )\n",
       "             (output_quantizers): ModuleList(\n",
       "               (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "             )\n",
       "           )\n",
       "         )\n",
       "         (activate): CustomSiLU(\n",
       "           (sigmoid): QuantizedSigmoid(\n",
       "             (param_quantizers): ModuleDict()\n",
       "             (input_quantizers): ModuleList(\n",
       "               (0): None\n",
       "             )\n",
       "             (output_quantizers): ModuleList(\n",
       "               (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "             )\n",
       "           )\n",
       "           (mul): QuantizedMultiply(\n",
       "             (param_quantizers): ModuleDict()\n",
       "             (input_quantizers): ModuleList(\n",
       "               (0-1): 2 x None\n",
       "             )\n",
       "             (output_quantizers): ModuleList(\n",
       "               (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "             )\n",
       "           )\n",
       "         )\n",
       "       )\n",
       "       (main_conv): Module(\n",
       "         (conv): QuantizedConv2d(\n",
       "           192, 96, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "           (param_quantizers): ModuleDict(\n",
       "             (weight): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=True)\n",
       "           )\n",
       "           (input_quantizers): ModuleList(\n",
       "             (0): None\n",
       "           )\n",
       "           (output_quantizers): ModuleList(\n",
       "             (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "           )\n",
       "         )\n",
       "         (bn): Module(\n",
       "           (module_batch_norm_19): FakeQuantizedBatchNorm(\n",
       "             (param_quantizers): ModuleDict()\n",
       "             (input_quantizers): ModuleList(\n",
       "               (0-4): 5 x QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "             )\n",
       "             (output_quantizers): ModuleList(\n",
       "               (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "             )\n",
       "           )\n",
       "         )\n",
       "         (activate): CustomSiLU(\n",
       "           (sigmoid): QuantizedSigmoid(\n",
       "             (param_quantizers): ModuleDict()\n",
       "             (input_quantizers): ModuleList(\n",
       "               (0): None\n",
       "             )\n",
       "             (output_quantizers): ModuleList(\n",
       "               (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "             )\n",
       "           )\n",
       "           (mul): QuantizedMultiply(\n",
       "             (param_quantizers): ModuleDict()\n",
       "             (input_quantizers): ModuleList(\n",
       "               (0-1): 2 x None\n",
       "             )\n",
       "             (output_quantizers): ModuleList(\n",
       "               (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "             )\n",
       "           )\n",
       "         )\n",
       "       )\n",
       "       (blocks): Module(\n",
       "         (0): Module(\n",
       "           (conv1): Module(\n",
       "             (conv): QuantizedConv2d(\n",
       "               96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "               (param_quantizers): ModuleDict(\n",
       "                 (weight): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=True)\n",
       "               )\n",
       "               (input_quantizers): ModuleList(\n",
       "                 (0): None\n",
       "               )\n",
       "               (output_quantizers): ModuleList(\n",
       "                 (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "               )\n",
       "             )\n",
       "             (bn): Module(\n",
       "               (module_batch_norm_20): FakeQuantizedBatchNorm(\n",
       "                 (param_quantizers): ModuleDict()\n",
       "                 (input_quantizers): ModuleList(\n",
       "                   (0-4): 5 x QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "                 )\n",
       "                 (output_quantizers): ModuleList(\n",
       "                   (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "                 )\n",
       "               )\n",
       "             )\n",
       "             (activate): CustomSiLU(\n",
       "               (sigmoid): QuantizedSigmoid(\n",
       "                 (param_quantizers): ModuleDict()\n",
       "                 (input_quantizers): ModuleList(\n",
       "                   (0): None\n",
       "                 )\n",
       "                 (output_quantizers): ModuleList(\n",
       "                   (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "                 )\n",
       "               )\n",
       "               (mul): QuantizedMultiply(\n",
       "                 (param_quantizers): ModuleDict()\n",
       "                 (input_quantizers): ModuleList(\n",
       "                   (0-1): 2 x None\n",
       "                 )\n",
       "                 (output_quantizers): ModuleList(\n",
       "                   (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "                 )\n",
       "               )\n",
       "             )\n",
       "           )\n",
       "           (conv2): Module(\n",
       "             (depthwise_conv): Module(\n",
       "               (conv): QuantizedConv2d(\n",
       "                 96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96, bias=False\n",
       "                 (param_quantizers): ModuleDict(\n",
       "                   (weight): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=True)\n",
       "                 )\n",
       "                 (input_quantizers): ModuleList(\n",
       "                   (0): None\n",
       "                 )\n",
       "                 (output_quantizers): ModuleList(\n",
       "                   (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "                 )\n",
       "               )\n",
       "               (bn): Module(\n",
       "                 (module_batch_norm_21): BatchNorm()\n",
       "               )\n",
       "               (activate): CustomSiLU(\n",
       "                 (sigmoid): QuantizedSigmoid(\n",
       "                   (param_quantizers): ModuleDict()\n",
       "                   (input_quantizers): ModuleList(\n",
       "                     (0): None\n",
       "                   )\n",
       "                   (output_quantizers): ModuleList(\n",
       "                     (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "                   )\n",
       "                 )\n",
       "                 (mul): QuantizedMultiply(\n",
       "                   (param_quantizers): ModuleDict()\n",
       "                   (input_quantizers): ModuleList(\n",
       "                     (0-1): 2 x None\n",
       "                   )\n",
       "                   (output_quantizers): ModuleList(\n",
       "                     (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "                   )\n",
       "                 )\n",
       "               )\n",
       "             )\n",
       "             (pointwise_conv): Module(\n",
       "               (conv): QuantizedConv2d(\n",
       "                 96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "                 (param_quantizers): ModuleDict(\n",
       "                   (weight): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=True)\n",
       "                 )\n",
       "                 (input_quantizers): ModuleList(\n",
       "                   (0): None\n",
       "                 )\n",
       "                 (output_quantizers): ModuleList(\n",
       "                   (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "                 )\n",
       "               )\n",
       "               (bn): Module(\n",
       "                 (module_batch_norm_22): FakeQuantizedBatchNorm(\n",
       "                   (param_quantizers): ModuleDict()\n",
       "                   (input_quantizers): ModuleList(\n",
       "                     (0-4): 5 x QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "                   )\n",
       "                   (output_quantizers): ModuleList(\n",
       "                     (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "                   )\n",
       "                 )\n",
       "               )\n",
       "               (activate): CustomSiLU(\n",
       "                 (sigmoid): QuantizedSigmoid(\n",
       "                   (param_quantizers): ModuleDict()\n",
       "                   (input_quantizers): ModuleList(\n",
       "                     (0): None\n",
       "                   )\n",
       "                   (output_quantizers): ModuleList(\n",
       "                     (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "                   )\n",
       "                 )\n",
       "                 (mul): QuantizedMultiply(\n",
       "                   (param_quantizers): ModuleDict()\n",
       "                   (input_quantizers): ModuleList(\n",
       "                     (0-1): 2 x None\n",
       "                   )\n",
       "                   (output_quantizers): ModuleList(\n",
       "                     (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "                   )\n",
       "                 )\n",
       "               )\n",
       "             )\n",
       "           )\n",
       "           (module_add_2): QuantizedAdd(\n",
       "             (param_quantizers): ModuleDict()\n",
       "             (input_quantizers): ModuleList(\n",
       "               (0-1): 2 x None\n",
       "             )\n",
       "             (output_quantizers): ModuleList(\n",
       "               (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "             )\n",
       "           )\n",
       "         )\n",
       "       )\n",
       "       (attention): Module(\n",
       "         (global_avgpool): FakeQuantizedAdaptiveAvgPool2d(\n",
       "           output_size=1\n",
       "           (param_quantizers): ModuleDict()\n",
       "           (input_quantizers): ModuleList(\n",
       "             (0): None\n",
       "           )\n",
       "           (output_quantizers): ModuleList(\n",
       "             (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "           )\n",
       "         )\n",
       "         (fc): QuantizedConv2d(\n",
       "           192, 192, kernel_size=(1, 1), stride=(1, 1)\n",
       "           (param_quantizers): ModuleDict(\n",
       "             (weight): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=True)\n",
       "             (bias): None\n",
       "           )\n",
       "           (input_quantizers): ModuleList(\n",
       "             (0): None\n",
       "           )\n",
       "           (output_quantizers): ModuleList(\n",
       "             (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "           )\n",
       "         )\n",
       "         (act): FakeQuantizedHardsigmoid(\n",
       "           (param_quantizers): ModuleDict()\n",
       "           (input_quantizers): ModuleList(\n",
       "             (0): None\n",
       "           )\n",
       "           (output_quantizers): ModuleList(\n",
       "             (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "           )\n",
       "         )\n",
       "         (module_mul_2): QuantizedMultiply(\n",
       "           (param_quantizers): ModuleDict()\n",
       "           (input_quantizers): ModuleList(\n",
       "             (0-1): 2 x None\n",
       "           )\n",
       "           (output_quantizers): ModuleList(\n",
       "             (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "           )\n",
       "         )\n",
       "       )\n",
       "       (final_conv): Module(\n",
       "         (conv): QuantizedConv2d(\n",
       "           192, 192, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "           (param_quantizers): ModuleDict(\n",
       "             (weight): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=True)\n",
       "           )\n",
       "           (input_quantizers): ModuleList(\n",
       "             (0): None\n",
       "           )\n",
       "           (output_quantizers): ModuleList(\n",
       "             (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "           )\n",
       "         )\n",
       "         (bn): Module(\n",
       "           (module_batch_norm_23): FakeQuantizedBatchNorm(\n",
       "             (param_quantizers): ModuleDict()\n",
       "             (input_quantizers): ModuleList(\n",
       "               (0-4): 5 x QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "             )\n",
       "             (output_quantizers): ModuleList(\n",
       "               (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "             )\n",
       "           )\n",
       "         )\n",
       "         (activate): CustomSiLU(\n",
       "           (sigmoid): QuantizedSigmoid(\n",
       "             (param_quantizers): ModuleDict()\n",
       "             (input_quantizers): ModuleList(\n",
       "               (0): None\n",
       "             )\n",
       "             (output_quantizers): ModuleList(\n",
       "               (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "             )\n",
       "           )\n",
       "           (mul): QuantizedMultiply(\n",
       "             (param_quantizers): ModuleDict()\n",
       "             (input_quantizers): ModuleList(\n",
       "               (0-1): 2 x None\n",
       "             )\n",
       "             (output_quantizers): ModuleList(\n",
       "               (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "             )\n",
       "           )\n",
       "         )\n",
       "       )\n",
       "       (module_cat_2): FakeQuantizedConcat(\n",
       "         (param_quantizers): ModuleDict()\n",
       "         (input_quantizers): ModuleList(\n",
       "           (0): None\n",
       "         )\n",
       "         (output_quantizers): ModuleList(\n",
       "           (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "         )\n",
       "       )\n",
       "     )\n",
       "   )\n",
       "   (stage4): Module(\n",
       "     (0): Module(\n",
       "       (conv): QuantizedConv2d(\n",
       "         192, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False\n",
       "         (param_quantizers): ModuleDict(\n",
       "           (weight): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=True)\n",
       "         )\n",
       "         (input_quantizers): ModuleList(\n",
       "           (0): None\n",
       "         )\n",
       "         (output_quantizers): ModuleList(\n",
       "           (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "         )\n",
       "       )\n",
       "       (bn): Module(\n",
       "         (module_batch_norm_24): FakeQuantizedBatchNorm(\n",
       "           (param_quantizers): ModuleDict()\n",
       "           (input_quantizers): ModuleList(\n",
       "             (0-4): 5 x QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "           )\n",
       "           (output_quantizers): ModuleList(\n",
       "             (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "           )\n",
       "         )\n",
       "       )\n",
       "       (activate): CustomSiLU(\n",
       "         (sigmoid): QuantizedSigmoid(\n",
       "           (param_quantizers): ModuleDict()\n",
       "           (input_quantizers): ModuleList(\n",
       "             (0): None\n",
       "           )\n",
       "           (output_quantizers): ModuleList(\n",
       "             (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "           )\n",
       "         )\n",
       "         (mul): QuantizedMultiply(\n",
       "           (param_quantizers): ModuleDict()\n",
       "           (input_quantizers): ModuleList(\n",
       "             (0-1): 2 x None\n",
       "           )\n",
       "           (output_quantizers): ModuleList(\n",
       "             (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "           )\n",
       "         )\n",
       "       )\n",
       "     )\n",
       "     (1): Module(\n",
       "       (conv1): Module(\n",
       "         (conv): QuantizedConv2d(\n",
       "           384, 192, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "           (param_quantizers): ModuleDict(\n",
       "             (weight): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=True)\n",
       "           )\n",
       "           (input_quantizers): ModuleList(\n",
       "             (0): None\n",
       "           )\n",
       "           (output_quantizers): ModuleList(\n",
       "             (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "           )\n",
       "         )\n",
       "         (bn): Module(\n",
       "           (module_batch_norm_25): FakeQuantizedBatchNorm(\n",
       "             (param_quantizers): ModuleDict()\n",
       "             (input_quantizers): ModuleList(\n",
       "               (0-4): 5 x QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "             )\n",
       "             (output_quantizers): ModuleList(\n",
       "               (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "             )\n",
       "           )\n",
       "         )\n",
       "         (activate): CustomSiLU(\n",
       "           (sigmoid): QuantizedSigmoid(\n",
       "             (param_quantizers): ModuleDict()\n",
       "             (input_quantizers): ModuleList(\n",
       "               (0): None\n",
       "             )\n",
       "             (output_quantizers): ModuleList(\n",
       "               (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "             )\n",
       "           )\n",
       "           (mul): QuantizedMultiply(\n",
       "             (param_quantizers): ModuleDict()\n",
       "             (input_quantizers): ModuleList(\n",
       "               (0-1): 2 x None\n",
       "             )\n",
       "             (output_quantizers): ModuleList(\n",
       "               (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "             )\n",
       "           )\n",
       "         )\n",
       "       )\n",
       "       (poolings): Module(\n",
       "         (0): FakeQuantizedMaxPool2d(\n",
       "           kernel_size=5, stride=1, padding=2, dilation=1, ceil_mode=False\n",
       "           (param_quantizers): ModuleDict()\n",
       "           (input_quantizers): ModuleList(\n",
       "             (0): None\n",
       "           )\n",
       "           (output_quantizers): ModuleList(\n",
       "             (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "           )\n",
       "         )\n",
       "         (1): FakeQuantizedMaxPool2d(\n",
       "           kernel_size=9, stride=1, padding=4, dilation=1, ceil_mode=False\n",
       "           (param_quantizers): ModuleDict()\n",
       "           (input_quantizers): ModuleList(\n",
       "             (0): None\n",
       "           )\n",
       "           (output_quantizers): ModuleList(\n",
       "             (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "           )\n",
       "         )\n",
       "         (2): FakeQuantizedMaxPool2d(\n",
       "           kernel_size=13, stride=1, padding=6, dilation=1, ceil_mode=False\n",
       "           (param_quantizers): ModuleDict()\n",
       "           (input_quantizers): ModuleList(\n",
       "             (0): None\n",
       "           )\n",
       "           (output_quantizers): ModuleList(\n",
       "             (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "           )\n",
       "         )\n",
       "       )\n",
       "       (conv2): Module(\n",
       "         (conv): QuantizedConv2d(\n",
       "           768, 384, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "           (param_quantizers): ModuleDict(\n",
       "             (weight): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=True)\n",
       "           )\n",
       "           (input_quantizers): ModuleList(\n",
       "             (0): None\n",
       "           )\n",
       "           (output_quantizers): ModuleList(\n",
       "             (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "           )\n",
       "         )\n",
       "         (bn): Module(\n",
       "           (module_batch_norm_26): FakeQuantizedBatchNorm(\n",
       "             (param_quantizers): ModuleDict()\n",
       "             (input_quantizers): ModuleList(\n",
       "               (0-4): 5 x QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "             )\n",
       "             (output_quantizers): ModuleList(\n",
       "               (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "             )\n",
       "           )\n",
       "         )\n",
       "         (activate): CustomSiLU(\n",
       "           (sigmoid): QuantizedSigmoid(\n",
       "             (param_quantizers): ModuleDict()\n",
       "             (input_quantizers): ModuleList(\n",
       "               (0): None\n",
       "             )\n",
       "             (output_quantizers): ModuleList(\n",
       "               (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "             )\n",
       "           )\n",
       "           (mul): QuantizedMultiply(\n",
       "             (param_quantizers): ModuleDict()\n",
       "             (input_quantizers): ModuleList(\n",
       "               (0-1): 2 x None\n",
       "             )\n",
       "             (output_quantizers): ModuleList(\n",
       "               (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "             )\n",
       "           )\n",
       "         )\n",
       "       )\n",
       "       (module_cat_3): FakeQuantizedConcat(\n",
       "         (param_quantizers): ModuleDict()\n",
       "         (input_quantizers): ModuleList(\n",
       "           (0): None\n",
       "         )\n",
       "         (output_quantizers): ModuleList(\n",
       "           (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "         )\n",
       "       )\n",
       "     )\n",
       "     (2): Module(\n",
       "       (short_conv): Module(\n",
       "         (conv): QuantizedConv2d(\n",
       "           384, 192, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "           (param_quantizers): ModuleDict(\n",
       "             (weight): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=True)\n",
       "           )\n",
       "           (input_quantizers): ModuleList(\n",
       "             (0): None\n",
       "           )\n",
       "           (output_quantizers): ModuleList(\n",
       "             (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "           )\n",
       "         )\n",
       "         (bn): Module(\n",
       "           (module_batch_norm_27): FakeQuantizedBatchNorm(\n",
       "             (param_quantizers): ModuleDict()\n",
       "             (input_quantizers): ModuleList(\n",
       "               (0-4): 5 x QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "             )\n",
       "             (output_quantizers): ModuleList(\n",
       "               (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "             )\n",
       "           )\n",
       "         )\n",
       "         (activate): CustomSiLU(\n",
       "           (sigmoid): QuantizedSigmoid(\n",
       "             (param_quantizers): ModuleDict()\n",
       "             (input_quantizers): ModuleList(\n",
       "               (0): None\n",
       "             )\n",
       "             (output_quantizers): ModuleList(\n",
       "               (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "             )\n",
       "           )\n",
       "           (mul): QuantizedMultiply(\n",
       "             (param_quantizers): ModuleDict()\n",
       "             (input_quantizers): ModuleList(\n",
       "               (0-1): 2 x None\n",
       "             )\n",
       "             (output_quantizers): ModuleList(\n",
       "               (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "             )\n",
       "           )\n",
       "         )\n",
       "       )\n",
       "       (main_conv): Module(\n",
       "         (conv): QuantizedConv2d(\n",
       "           384, 192, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "           (param_quantizers): ModuleDict(\n",
       "             (weight): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=True)\n",
       "           )\n",
       "           (input_quantizers): ModuleList(\n",
       "             (0): None\n",
       "           )\n",
       "           (output_quantizers): ModuleList(\n",
       "             (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "           )\n",
       "         )\n",
       "         (bn): Module(\n",
       "           (module_batch_norm_28): FakeQuantizedBatchNorm(\n",
       "             (param_quantizers): ModuleDict()\n",
       "             (input_quantizers): ModuleList(\n",
       "               (0-4): 5 x QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "             )\n",
       "             (output_quantizers): ModuleList(\n",
       "               (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "             )\n",
       "           )\n",
       "         )\n",
       "         (activate): CustomSiLU(\n",
       "           (sigmoid): QuantizedSigmoid(\n",
       "             (param_quantizers): ModuleDict()\n",
       "             (input_quantizers): ModuleList(\n",
       "               (0): None\n",
       "             )\n",
       "             (output_quantizers): ModuleList(\n",
       "               (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "             )\n",
       "           )\n",
       "           (mul): QuantizedMultiply(\n",
       "             (param_quantizers): ModuleDict()\n",
       "             (input_quantizers): ModuleList(\n",
       "               (0-1): 2 x None\n",
       "             )\n",
       "             (output_quantizers): ModuleList(\n",
       "               (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "             )\n",
       "           )\n",
       "         )\n",
       "       )\n",
       "       (blocks): Module(\n",
       "         (0): Module(\n",
       "           (conv1): Module(\n",
       "             (conv): QuantizedConv2d(\n",
       "               192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "               (param_quantizers): ModuleDict(\n",
       "                 (weight): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=True)\n",
       "               )\n",
       "               (input_quantizers): ModuleList(\n",
       "                 (0): None\n",
       "               )\n",
       "               (output_quantizers): ModuleList(\n",
       "                 (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "               )\n",
       "             )\n",
       "             (bn): Module(\n",
       "               (module_batch_norm_29): FakeQuantizedBatchNorm(\n",
       "                 (param_quantizers): ModuleDict()\n",
       "                 (input_quantizers): ModuleList(\n",
       "                   (0-4): 5 x QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "                 )\n",
       "                 (output_quantizers): ModuleList(\n",
       "                   (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "                 )\n",
       "               )\n",
       "             )\n",
       "             (activate): CustomSiLU(\n",
       "               (sigmoid): QuantizedSigmoid(\n",
       "                 (param_quantizers): ModuleDict()\n",
       "                 (input_quantizers): ModuleList(\n",
       "                   (0): None\n",
       "                 )\n",
       "                 (output_quantizers): ModuleList(\n",
       "                   (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "                 )\n",
       "               )\n",
       "               (mul): QuantizedMultiply(\n",
       "                 (param_quantizers): ModuleDict()\n",
       "                 (input_quantizers): ModuleList(\n",
       "                   (0-1): 2 x None\n",
       "                 )\n",
       "                 (output_quantizers): ModuleList(\n",
       "                   (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "                 )\n",
       "               )\n",
       "             )\n",
       "           )\n",
       "           (conv2): Module(\n",
       "             (depthwise_conv): Module(\n",
       "               (conv): QuantizedConv2d(\n",
       "                 192, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=192, bias=False\n",
       "                 (param_quantizers): ModuleDict(\n",
       "                   (weight): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=True)\n",
       "                 )\n",
       "                 (input_quantizers): ModuleList(\n",
       "                   (0): None\n",
       "                 )\n",
       "                 (output_quantizers): ModuleList(\n",
       "                   (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "                 )\n",
       "               )\n",
       "               (bn): Module(\n",
       "                 (module_batch_norm_30): BatchNorm()\n",
       "               )\n",
       "               (activate): CustomSiLU(\n",
       "                 (sigmoid): QuantizedSigmoid(\n",
       "                   (param_quantizers): ModuleDict()\n",
       "                   (input_quantizers): ModuleList(\n",
       "                     (0): None\n",
       "                   )\n",
       "                   (output_quantizers): ModuleList(\n",
       "                     (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "                   )\n",
       "                 )\n",
       "                 (mul): QuantizedMultiply(\n",
       "                   (param_quantizers): ModuleDict()\n",
       "                   (input_quantizers): ModuleList(\n",
       "                     (0-1): 2 x None\n",
       "                   )\n",
       "                   (output_quantizers): ModuleList(\n",
       "                     (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "                   )\n",
       "                 )\n",
       "               )\n",
       "             )\n",
       "             (pointwise_conv): Module(\n",
       "               (conv): QuantizedConv2d(\n",
       "                 192, 192, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "                 (param_quantizers): ModuleDict(\n",
       "                   (weight): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=True)\n",
       "                 )\n",
       "                 (input_quantizers): ModuleList(\n",
       "                   (0): None\n",
       "                 )\n",
       "                 (output_quantizers): ModuleList(\n",
       "                   (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "                 )\n",
       "               )\n",
       "               (bn): Module(\n",
       "                 (module_batch_norm_31): FakeQuantizedBatchNorm(\n",
       "                   (param_quantizers): ModuleDict()\n",
       "                   (input_quantizers): ModuleList(\n",
       "                     (0-4): 5 x QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "                   )\n",
       "                   (output_quantizers): ModuleList(\n",
       "                     (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "                   )\n",
       "                 )\n",
       "               )\n",
       "               (activate): CustomSiLU(\n",
       "                 (sigmoid): QuantizedSigmoid(\n",
       "                   (param_quantizers): ModuleDict()\n",
       "                   (input_quantizers): ModuleList(\n",
       "                     (0): None\n",
       "                   )\n",
       "                   (output_quantizers): ModuleList(\n",
       "                     (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "                   )\n",
       "                 )\n",
       "                 (mul): QuantizedMultiply(\n",
       "                   (param_quantizers): ModuleDict()\n",
       "                   (input_quantizers): ModuleList(\n",
       "                     (0-1): 2 x None\n",
       "                   )\n",
       "                   (output_quantizers): ModuleList(\n",
       "                     (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "                   )\n",
       "                 )\n",
       "               )\n",
       "             )\n",
       "           )\n",
       "         )\n",
       "       )\n",
       "       (attention): Module(\n",
       "         (global_avgpool): FakeQuantizedAdaptiveAvgPool2d(\n",
       "           output_size=1\n",
       "           (param_quantizers): ModuleDict()\n",
       "           (input_quantizers): ModuleList(\n",
       "             (0): None\n",
       "           )\n",
       "           (output_quantizers): ModuleList(\n",
       "             (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "           )\n",
       "         )\n",
       "         (fc): QuantizedConv2d(\n",
       "           384, 384, kernel_size=(1, 1), stride=(1, 1)\n",
       "           (param_quantizers): ModuleDict(\n",
       "             (weight): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=True)\n",
       "             (bias): None\n",
       "           )\n",
       "           (input_quantizers): ModuleList(\n",
       "             (0): None\n",
       "           )\n",
       "           (output_quantizers): ModuleList(\n",
       "             (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "           )\n",
       "         )\n",
       "         (act): FakeQuantizedHardsigmoid(\n",
       "           (param_quantizers): ModuleDict()\n",
       "           (input_quantizers): ModuleList(\n",
       "             (0): None\n",
       "           )\n",
       "           (output_quantizers): ModuleList(\n",
       "             (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "           )\n",
       "         )\n",
       "         (module_mul_3): QuantizedMultiply(\n",
       "           (param_quantizers): ModuleDict()\n",
       "           (input_quantizers): ModuleList(\n",
       "             (0-1): 2 x None\n",
       "           )\n",
       "           (output_quantizers): ModuleList(\n",
       "             (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "           )\n",
       "         )\n",
       "       )\n",
       "       (final_conv): Module(\n",
       "         (conv): QuantizedConv2d(\n",
       "           384, 384, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "           (param_quantizers): ModuleDict(\n",
       "             (weight): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=True)\n",
       "           )\n",
       "           (input_quantizers): ModuleList(\n",
       "             (0): None\n",
       "           )\n",
       "           (output_quantizers): ModuleList(\n",
       "             (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "           )\n",
       "         )\n",
       "         (bn): Module(\n",
       "           (module_batch_norm_32): FakeQuantizedBatchNorm(\n",
       "             (param_quantizers): ModuleDict()\n",
       "             (input_quantizers): ModuleList(\n",
       "               (0-4): 5 x QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "             )\n",
       "             (output_quantizers): ModuleList(\n",
       "               (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "             )\n",
       "           )\n",
       "         )\n",
       "         (activate): CustomSiLU(\n",
       "           (sigmoid): QuantizedSigmoid(\n",
       "             (param_quantizers): ModuleDict()\n",
       "             (input_quantizers): ModuleList(\n",
       "               (0): None\n",
       "             )\n",
       "             (output_quantizers): ModuleList(\n",
       "               (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "             )\n",
       "           )\n",
       "           (mul): QuantizedMultiply(\n",
       "             (param_quantizers): ModuleDict()\n",
       "             (input_quantizers): ModuleList(\n",
       "               (0-1): 2 x None\n",
       "             )\n",
       "             (output_quantizers): ModuleList(\n",
       "               (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "             )\n",
       "           )\n",
       "         )\n",
       "       )\n",
       "       (module_cat_4): FakeQuantizedConcat(\n",
       "         (param_quantizers): ModuleDict()\n",
       "         (input_quantizers): ModuleList(\n",
       "           (0): None\n",
       "         )\n",
       "         (output_quantizers): ModuleList(\n",
       "           (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "         )\n",
       "       )\n",
       "     )\n",
       "   )\n",
       " ),\n",
       " 'backbone.stem': Module(\n",
       "   (0): Module(\n",
       "     (conv): QuantizedConv2d(\n",
       "       3, 12, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False\n",
       "       (param_quantizers): ModuleDict(\n",
       "         (weight): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=True)\n",
       "       )\n",
       "       (input_quantizers): ModuleList(\n",
       "         (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "       )\n",
       "       (output_quantizers): ModuleList(\n",
       "         (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "       )\n",
       "     )\n",
       "     (bn): Module(\n",
       "       (module_batch_norm): FakeQuantizedBatchNorm(\n",
       "         (param_quantizers): ModuleDict()\n",
       "         (input_quantizers): ModuleList(\n",
       "           (0-4): 5 x QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "         )\n",
       "         (output_quantizers): ModuleList(\n",
       "           (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "         )\n",
       "       )\n",
       "     )\n",
       "     (activate): CustomSiLU(\n",
       "       (sigmoid): QuantizedSigmoid(\n",
       "         (param_quantizers): ModuleDict()\n",
       "         (input_quantizers): ModuleList(\n",
       "           (0): None\n",
       "         )\n",
       "         (output_quantizers): ModuleList(\n",
       "           (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "         )\n",
       "       )\n",
       "       (mul): QuantizedMultiply(\n",
       "         (param_quantizers): ModuleDict()\n",
       "         (input_quantizers): ModuleList(\n",
       "           (0-1): 2 x None\n",
       "         )\n",
       "         (output_quantizers): ModuleList(\n",
       "           (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "         )\n",
       "       )\n",
       "     )\n",
       "   )\n",
       "   (1): Module(\n",
       "     (conv): QuantizedConv2d(\n",
       "       12, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "       (param_quantizers): ModuleDict(\n",
       "         (weight): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=True)\n",
       "       )\n",
       "       (input_quantizers): ModuleList(\n",
       "         (0): None\n",
       "       )\n",
       "       (output_quantizers): ModuleList(\n",
       "         (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "       )\n",
       "     )\n",
       "     (bn): Module(\n",
       "       (module_batch_norm_1): FakeQuantizedBatchNorm(\n",
       "         (param_quantizers): ModuleDict()\n",
       "         (input_quantizers): ModuleList(\n",
       "           (0-4): 5 x QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "         )\n",
       "         (output_quantizers): ModuleList(\n",
       "           (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "         )\n",
       "       )\n",
       "     )\n",
       "     (activate): CustomSiLU(\n",
       "       (sigmoid): QuantizedSigmoid(\n",
       "         (param_quantizers): ModuleDict()\n",
       "         (input_quantizers): ModuleList(\n",
       "           (0): None\n",
       "         )\n",
       "         (output_quantizers): ModuleList(\n",
       "           (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "         )\n",
       "       )\n",
       "       (mul): QuantizedMultiply(\n",
       "         (param_quantizers): ModuleDict()\n",
       "         (input_quantizers): ModuleList(\n",
       "           (0-1): 2 x None\n",
       "         )\n",
       "         (output_quantizers): ModuleList(\n",
       "           (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "         )\n",
       "       )\n",
       "     )\n",
       "   )\n",
       "   (2): Module(\n",
       "     (conv): QuantizedConv2d(\n",
       "       12, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "       (param_quantizers): ModuleDict(\n",
       "         (weight): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=True)\n",
       "       )\n",
       "       (input_quantizers): ModuleList(\n",
       "         (0): None\n",
       "       )\n",
       "       (output_quantizers): ModuleList(\n",
       "         (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "       )\n",
       "     )\n",
       "     (bn): Module(\n",
       "       (module_batch_norm_2): FakeQuantizedBatchNorm(\n",
       "         (param_quantizers): ModuleDict()\n",
       "         (input_quantizers): ModuleList(\n",
       "           (0-4): 5 x QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "         )\n",
       "         (output_quantizers): ModuleList(\n",
       "           (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "         )\n",
       "       )\n",
       "     )\n",
       "     (activate): CustomSiLU(\n",
       "       (sigmoid): QuantizedSigmoid(\n",
       "         (param_quantizers): ModuleDict()\n",
       "         (input_quantizers): ModuleList(\n",
       "           (0): None\n",
       "         )\n",
       "         (output_quantizers): ModuleList(\n",
       "           (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "         )\n",
       "       )\n",
       "       (mul): QuantizedMultiply(\n",
       "         (param_quantizers): ModuleDict()\n",
       "         (input_quantizers): ModuleList(\n",
       "           (0-1): 2 x None\n",
       "         )\n",
       "         (output_quantizers): ModuleList(\n",
       "           (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "         )\n",
       "       )\n",
       "     )\n",
       "   )\n",
       " ),\n",
       " 'backbone.stem.0': Module(\n",
       "   (conv): QuantizedConv2d(\n",
       "     3, 12, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False\n",
       "     (param_quantizers): ModuleDict(\n",
       "       (weight): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=True)\n",
       "     )\n",
       "     (input_quantizers): ModuleList(\n",
       "       (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "     )\n",
       "     (output_quantizers): ModuleList(\n",
       "       (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "     )\n",
       "   )\n",
       "   (bn): Module(\n",
       "     (module_batch_norm): FakeQuantizedBatchNorm(\n",
       "       (param_quantizers): ModuleDict()\n",
       "       (input_quantizers): ModuleList(\n",
       "         (0-4): 5 x QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "       )\n",
       "       (output_quantizers): ModuleList(\n",
       "         (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "       )\n",
       "     )\n",
       "   )\n",
       "   (activate): CustomSiLU(\n",
       "     (sigmoid): QuantizedSigmoid(\n",
       "       (param_quantizers): ModuleDict()\n",
       "       (input_quantizers): ModuleList(\n",
       "         (0): None\n",
       "       )\n",
       "       (output_quantizers): ModuleList(\n",
       "         (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "       )\n",
       "     )\n",
       "     (mul): QuantizedMultiply(\n",
       "       (param_quantizers): ModuleDict()\n",
       "       (input_quantizers): ModuleList(\n",
       "         (0-1): 2 x None\n",
       "       )\n",
       "       (output_quantizers): ModuleList(\n",
       "         (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "       )\n",
       "     )\n",
       "   )\n",
       " ),\n",
       " 'backbone.stem.0.conv': QuantizedConv2d(\n",
       "   3, 12, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False\n",
       "   (param_quantizers): ModuleDict(\n",
       "     (weight): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=True)\n",
       "   )\n",
       "   (input_quantizers): ModuleList(\n",
       "     (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "   )\n",
       "   (output_quantizers): ModuleList(\n",
       "     (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "   )\n",
       " ),\n",
       " 'backbone.stem.0.conv.param_quantizers': ModuleDict(\n",
       "   (weight): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=True)\n",
       " ),\n",
       " 'backbone.stem.0.conv.param_quantizers.weight': QuantizeDequantize(shape=[1], bitwidth=8, symmetric=True),\n",
       " 'backbone.stem.0.conv.input_quantizers': ModuleList(\n",
       "   (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       " ),\n",
       " 'backbone.stem.0.conv.input_quantizers.0': QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False),\n",
       " 'backbone.stem.0.conv.output_quantizers': ModuleList(\n",
       "   (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       " ),\n",
       " 'backbone.stem.0.conv.output_quantizers.0': QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False),\n",
       " 'backbone.stem.0.bn': Module(\n",
       "   (module_batch_norm): FakeQuantizedBatchNorm(\n",
       "     (param_quantizers): ModuleDict()\n",
       "     (input_quantizers): ModuleList(\n",
       "       (0-4): 5 x QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "     )\n",
       "     (output_quantizers): ModuleList(\n",
       "       (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "     )\n",
       "   )\n",
       " ),\n",
       " 'backbone.stem.0.bn.module_batch_norm': FakeQuantizedBatchNorm(\n",
       "   (param_quantizers): ModuleDict()\n",
       "   (input_quantizers): ModuleList(\n",
       "     (0-4): 5 x QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "   )\n",
       "   (output_quantizers): ModuleList(\n",
       "     (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "   )\n",
       " ),\n",
       " 'backbone.stem.0.bn.module_batch_norm.param_quantizers': ModuleDict(),\n",
       " 'backbone.stem.0.bn.module_batch_norm.input_quantizers': ModuleList(\n",
       "   (0-4): 5 x QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       " ),\n",
       " 'backbone.stem.0.bn.module_batch_norm.input_quantizers.0': QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False),\n",
       " 'backbone.stem.0.bn.module_batch_norm.input_quantizers.1': QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False),\n",
       " 'backbone.stem.0.bn.module_batch_norm.input_quantizers.2': QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False),\n",
       " 'backbone.stem.0.bn.module_batch_norm.input_quantizers.3': QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False),\n",
       " 'backbone.stem.0.bn.module_batch_norm.input_quantizers.4': QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False),\n",
       " 'backbone.stem.0.bn.module_batch_norm.output_quantizers': ModuleList(\n",
       "   (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       " ),\n",
       " 'backbone.stem.0.bn.module_batch_norm.output_quantizers.0': QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False),\n",
       " 'backbone.stem.0.activate': CustomSiLU(\n",
       "   (sigmoid): QuantizedSigmoid(\n",
       "     (param_quantizers): ModuleDict()\n",
       "     (input_quantizers): ModuleList(\n",
       "       (0): None\n",
       "     )\n",
       "     (output_quantizers): ModuleList(\n",
       "       (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "     )\n",
       "   )\n",
       "   (mul): QuantizedMultiply(\n",
       "     (param_quantizers): ModuleDict()\n",
       "     (input_quantizers): ModuleList(\n",
       "       (0-1): 2 x None\n",
       "     )\n",
       "     (output_quantizers): ModuleList(\n",
       "       (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "     )\n",
       "   )\n",
       " ),\n",
       " 'backbone.stem.0.activate.sigmoid': QuantizedSigmoid(\n",
       "   (param_quantizers): ModuleDict()\n",
       "   (input_quantizers): ModuleList(\n",
       "     (0): None\n",
       "   )\n",
       "   (output_quantizers): ModuleList(\n",
       "     (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "   )\n",
       " ),\n",
       " 'backbone.stem.0.activate.sigmoid.param_quantizers': ModuleDict(),\n",
       " 'backbone.stem.0.activate.sigmoid.input_quantizers': ModuleList(\n",
       "   (0): None\n",
       " ),\n",
       " 'backbone.stem.0.activate.sigmoid.output_quantizers': ModuleList(\n",
       "   (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       " ),\n",
       " 'backbone.stem.0.activate.sigmoid.output_quantizers.0': QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False),\n",
       " 'backbone.stem.0.activate.mul': QuantizedMultiply(\n",
       "   (param_quantizers): ModuleDict()\n",
       "   (input_quantizers): ModuleList(\n",
       "     (0-1): 2 x None\n",
       "   )\n",
       "   (output_quantizers): ModuleList(\n",
       "     (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "   )\n",
       " ),\n",
       " 'backbone.stem.0.activate.mul.param_quantizers': ModuleDict(),\n",
       " 'backbone.stem.0.activate.mul.input_quantizers': ModuleList(\n",
       "   (0-1): 2 x None\n",
       " ),\n",
       " 'backbone.stem.0.activate.mul.output_quantizers': ModuleList(\n",
       "   (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       " ),\n",
       " 'backbone.stem.0.activate.mul.output_quantizers.0': QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False),\n",
       " 'backbone.stem.1': Module(\n",
       "   (conv): QuantizedConv2d(\n",
       "     12, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "     (param_quantizers): ModuleDict(\n",
       "       (weight): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=True)\n",
       "     )\n",
       "     (input_quantizers): ModuleList(\n",
       "       (0): None\n",
       "     )\n",
       "     (output_quantizers): ModuleList(\n",
       "       (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "     )\n",
       "   )\n",
       "   (bn): Module(\n",
       "     (module_batch_norm_1): FakeQuantizedBatchNorm(\n",
       "       (param_quantizers): ModuleDict()\n",
       "       (input_quantizers): ModuleList(\n",
       "         (0-4): 5 x QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "       )\n",
       "       (output_quantizers): ModuleList(\n",
       "         (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "       )\n",
       "     )\n",
       "   )\n",
       "   (activate): CustomSiLU(\n",
       "     (sigmoid): QuantizedSigmoid(\n",
       "       (param_quantizers): ModuleDict()\n",
       "       (input_quantizers): ModuleList(\n",
       "         (0): None\n",
       "       )\n",
       "       (output_quantizers): ModuleList(\n",
       "         (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "       )\n",
       "     )\n",
       "     (mul): QuantizedMultiply(\n",
       "       (param_quantizers): ModuleDict()\n",
       "       (input_quantizers): ModuleList(\n",
       "         (0-1): 2 x None\n",
       "       )\n",
       "       (output_quantizers): ModuleList(\n",
       "         (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "       )\n",
       "     )\n",
       "   )\n",
       " ),\n",
       " 'backbone.stem.1.conv': QuantizedConv2d(\n",
       "   12, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "   (param_quantizers): ModuleDict(\n",
       "     (weight): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=True)\n",
       "   )\n",
       "   (input_quantizers): ModuleList(\n",
       "     (0): None\n",
       "   )\n",
       "   (output_quantizers): ModuleList(\n",
       "     (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "   )\n",
       " ),\n",
       " 'backbone.stem.1.conv.param_quantizers': ModuleDict(\n",
       "   (weight): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=True)\n",
       " ),\n",
       " 'backbone.stem.1.conv.param_quantizers.weight': QuantizeDequantize(shape=[1], bitwidth=8, symmetric=True),\n",
       " 'backbone.stem.1.conv.input_quantizers': ModuleList(\n",
       "   (0): None\n",
       " ),\n",
       " 'backbone.stem.1.conv.output_quantizers': ModuleList(\n",
       "   (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       " ),\n",
       " 'backbone.stem.1.conv.output_quantizers.0': QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False),\n",
       " 'backbone.stem.1.bn': Module(\n",
       "   (module_batch_norm_1): FakeQuantizedBatchNorm(\n",
       "     (param_quantizers): ModuleDict()\n",
       "     (input_quantizers): ModuleList(\n",
       "       (0-4): 5 x QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "     )\n",
       "     (output_quantizers): ModuleList(\n",
       "       (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "     )\n",
       "   )\n",
       " ),\n",
       " 'backbone.stem.1.bn.module_batch_norm_1': FakeQuantizedBatchNorm(\n",
       "   (param_quantizers): ModuleDict()\n",
       "   (input_quantizers): ModuleList(\n",
       "     (0-4): 5 x QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "   )\n",
       "   (output_quantizers): ModuleList(\n",
       "     (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "   )\n",
       " ),\n",
       " 'backbone.stem.1.bn.module_batch_norm_1.param_quantizers': ModuleDict(),\n",
       " 'backbone.stem.1.bn.module_batch_norm_1.input_quantizers': ModuleList(\n",
       "   (0-4): 5 x QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       " ),\n",
       " 'backbone.stem.1.bn.module_batch_norm_1.input_quantizers.0': QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False),\n",
       " 'backbone.stem.1.bn.module_batch_norm_1.input_quantizers.1': QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False),\n",
       " 'backbone.stem.1.bn.module_batch_norm_1.input_quantizers.2': QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False),\n",
       " 'backbone.stem.1.bn.module_batch_norm_1.input_quantizers.3': QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False),\n",
       " 'backbone.stem.1.bn.module_batch_norm_1.input_quantizers.4': QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False),\n",
       " 'backbone.stem.1.bn.module_batch_norm_1.output_quantizers': ModuleList(\n",
       "   (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       " ),\n",
       " 'backbone.stem.1.bn.module_batch_norm_1.output_quantizers.0': QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False),\n",
       " 'backbone.stem.1.activate': CustomSiLU(\n",
       "   (sigmoid): QuantizedSigmoid(\n",
       "     (param_quantizers): ModuleDict()\n",
       "     (input_quantizers): ModuleList(\n",
       "       (0): None\n",
       "     )\n",
       "     (output_quantizers): ModuleList(\n",
       "       (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "     )\n",
       "   )\n",
       "   (mul): QuantizedMultiply(\n",
       "     (param_quantizers): ModuleDict()\n",
       "     (input_quantizers): ModuleList(\n",
       "       (0-1): 2 x None\n",
       "     )\n",
       "     (output_quantizers): ModuleList(\n",
       "       (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "     )\n",
       "   )\n",
       " ),\n",
       " 'backbone.stem.1.activate.sigmoid': QuantizedSigmoid(\n",
       "   (param_quantizers): ModuleDict()\n",
       "   (input_quantizers): ModuleList(\n",
       "     (0): None\n",
       "   )\n",
       "   (output_quantizers): ModuleList(\n",
       "     (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "   )\n",
       " ),\n",
       " 'backbone.stem.1.activate.sigmoid.param_quantizers': ModuleDict(),\n",
       " 'backbone.stem.1.activate.sigmoid.input_quantizers': ModuleList(\n",
       "   (0): None\n",
       " ),\n",
       " 'backbone.stem.1.activate.sigmoid.output_quantizers': ModuleList(\n",
       "   (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       " ),\n",
       " 'backbone.stem.1.activate.sigmoid.output_quantizers.0': QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False),\n",
       " 'backbone.stem.1.activate.mul': QuantizedMultiply(\n",
       "   (param_quantizers): ModuleDict()\n",
       "   (input_quantizers): ModuleList(\n",
       "     (0-1): 2 x None\n",
       "   )\n",
       "   (output_quantizers): ModuleList(\n",
       "     (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "   )\n",
       " ),\n",
       " 'backbone.stem.1.activate.mul.param_quantizers': ModuleDict(),\n",
       " 'backbone.stem.1.activate.mul.input_quantizers': ModuleList(\n",
       "   (0-1): 2 x None\n",
       " ),\n",
       " 'backbone.stem.1.activate.mul.output_quantizers': ModuleList(\n",
       "   (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       " ),\n",
       " 'backbone.stem.1.activate.mul.output_quantizers.0': QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False),\n",
       " 'backbone.stem.2': Module(\n",
       "   (conv): QuantizedConv2d(\n",
       "     12, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "     (param_quantizers): ModuleDict(\n",
       "       (weight): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=True)\n",
       "     )\n",
       "     (input_quantizers): ModuleList(\n",
       "       (0): None\n",
       "     )\n",
       "     (output_quantizers): ModuleList(\n",
       "       (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "     )\n",
       "   )\n",
       "   (bn): Module(\n",
       "     (module_batch_norm_2): FakeQuantizedBatchNorm(\n",
       "       (param_quantizers): ModuleDict()\n",
       "       (input_quantizers): ModuleList(\n",
       "         (0-4): 5 x QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "       )\n",
       "       (output_quantizers): ModuleList(\n",
       "         (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "       )\n",
       "     )\n",
       "   )\n",
       "   (activate): CustomSiLU(\n",
       "     (sigmoid): QuantizedSigmoid(\n",
       "       (param_quantizers): ModuleDict()\n",
       "       (input_quantizers): ModuleList(\n",
       "         (0): None\n",
       "       )\n",
       "       (output_quantizers): ModuleList(\n",
       "         (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "       )\n",
       "     )\n",
       "     (mul): QuantizedMultiply(\n",
       "       (param_quantizers): ModuleDict()\n",
       "       (input_quantizers): ModuleList(\n",
       "         (0-1): 2 x None\n",
       "       )\n",
       "       (output_quantizers): ModuleList(\n",
       "         (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "       )\n",
       "     )\n",
       "   )\n",
       " ),\n",
       " 'backbone.stem.2.conv': QuantizedConv2d(\n",
       "   12, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "   (param_quantizers): ModuleDict(\n",
       "     (weight): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=True)\n",
       "   )\n",
       "   (input_quantizers): ModuleList(\n",
       "     (0): None\n",
       "   )\n",
       "   (output_quantizers): ModuleList(\n",
       "     (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "   )\n",
       " ),\n",
       " 'backbone.stem.2.conv.param_quantizers': ModuleDict(\n",
       "   (weight): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=True)\n",
       " ),\n",
       " 'backbone.stem.2.conv.param_quantizers.weight': QuantizeDequantize(shape=[1], bitwidth=8, symmetric=True),\n",
       " 'backbone.stem.2.conv.input_quantizers': ModuleList(\n",
       "   (0): None\n",
       " ),\n",
       " 'backbone.stem.2.conv.output_quantizers': ModuleList(\n",
       "   (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       " ),\n",
       " 'backbone.stem.2.conv.output_quantizers.0': QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False),\n",
       " 'backbone.stem.2.bn': Module(\n",
       "   (module_batch_norm_2): FakeQuantizedBatchNorm(\n",
       "     (param_quantizers): ModuleDict()\n",
       "     (input_quantizers): ModuleList(\n",
       "       (0-4): 5 x QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "     )\n",
       "     (output_quantizers): ModuleList(\n",
       "       (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "     )\n",
       "   )\n",
       " ),\n",
       " 'backbone.stem.2.bn.module_batch_norm_2': FakeQuantizedBatchNorm(\n",
       "   (param_quantizers): ModuleDict()\n",
       "   (input_quantizers): ModuleList(\n",
       "     (0-4): 5 x QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "   )\n",
       "   (output_quantizers): ModuleList(\n",
       "     (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "   )\n",
       " ),\n",
       " 'backbone.stem.2.bn.module_batch_norm_2.param_quantizers': ModuleDict(),\n",
       " 'backbone.stem.2.bn.module_batch_norm_2.input_quantizers': ModuleList(\n",
       "   (0-4): 5 x QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       " ),\n",
       " 'backbone.stem.2.bn.module_batch_norm_2.input_quantizers.0': QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False),\n",
       " 'backbone.stem.2.bn.module_batch_norm_2.input_quantizers.1': QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False),\n",
       " 'backbone.stem.2.bn.module_batch_norm_2.input_quantizers.2': QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False),\n",
       " 'backbone.stem.2.bn.module_batch_norm_2.input_quantizers.3': QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False),\n",
       " 'backbone.stem.2.bn.module_batch_norm_2.input_quantizers.4': QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False),\n",
       " 'backbone.stem.2.bn.module_batch_norm_2.output_quantizers': ModuleList(\n",
       "   (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       " ),\n",
       " 'backbone.stem.2.bn.module_batch_norm_2.output_quantizers.0': QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False),\n",
       " 'backbone.stem.2.activate': CustomSiLU(\n",
       "   (sigmoid): QuantizedSigmoid(\n",
       "     (param_quantizers): ModuleDict()\n",
       "     (input_quantizers): ModuleList(\n",
       "       (0): None\n",
       "     )\n",
       "     (output_quantizers): ModuleList(\n",
       "       (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "     )\n",
       "   )\n",
       "   (mul): QuantizedMultiply(\n",
       "     (param_quantizers): ModuleDict()\n",
       "     (input_quantizers): ModuleList(\n",
       "       (0-1): 2 x None\n",
       "     )\n",
       "     (output_quantizers): ModuleList(\n",
       "       (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "     )\n",
       "   )\n",
       " ),\n",
       " 'backbone.stem.2.activate.sigmoid': QuantizedSigmoid(\n",
       "   (param_quantizers): ModuleDict()\n",
       "   (input_quantizers): ModuleList(\n",
       "     (0): None\n",
       "   )\n",
       "   (output_quantizers): ModuleList(\n",
       "     (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "   )\n",
       " ),\n",
       " 'backbone.stem.2.activate.sigmoid.param_quantizers': ModuleDict(),\n",
       " 'backbone.stem.2.activate.sigmoid.input_quantizers': ModuleList(\n",
       "   (0): None\n",
       " ),\n",
       " 'backbone.stem.2.activate.sigmoid.output_quantizers': ModuleList(\n",
       "   (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       " ),\n",
       " 'backbone.stem.2.activate.sigmoid.output_quantizers.0': QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False),\n",
       " 'backbone.stem.2.activate.mul': QuantizedMultiply(\n",
       "   (param_quantizers): ModuleDict()\n",
       "   (input_quantizers): ModuleList(\n",
       "     (0-1): 2 x None\n",
       "   )\n",
       "   (output_quantizers): ModuleList(\n",
       "     (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "   )\n",
       " ),\n",
       " 'backbone.stem.2.activate.mul.param_quantizers': ModuleDict(),\n",
       " 'backbone.stem.2.activate.mul.input_quantizers': ModuleList(\n",
       "   (0-1): 2 x None\n",
       " ),\n",
       " 'backbone.stem.2.activate.mul.output_quantizers': ModuleList(\n",
       "   (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       " ),\n",
       " 'backbone.stem.2.activate.mul.output_quantizers.0': QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False),\n",
       " 'backbone.stage1': Module(\n",
       "   (0): Module(\n",
       "     (conv): QuantizedConv2d(\n",
       "       24, 48, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False\n",
       "       (param_quantizers): ModuleDict(\n",
       "         (weight): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=True)\n",
       "       )\n",
       "       (input_quantizers): ModuleList(\n",
       "         (0): None\n",
       "       )\n",
       "       (output_quantizers): ModuleList(\n",
       "         (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "       )\n",
       "     )\n",
       "     (bn): Module(\n",
       "       (module_batch_norm_3): FakeQuantizedBatchNorm(\n",
       "         (param_quantizers): ModuleDict()\n",
       "         (input_quantizers): ModuleList(\n",
       "           (0-4): 5 x QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "         )\n",
       "         (output_quantizers): ModuleList(\n",
       "           (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "         )\n",
       "       )\n",
       "     )\n",
       "     (activate): CustomSiLU(\n",
       "       (sigmoid): QuantizedSigmoid(\n",
       "         (param_quantizers): ModuleDict()\n",
       "         (input_quantizers): ModuleList(\n",
       "           (0): None\n",
       "         )\n",
       "         (output_quantizers): ModuleList(\n",
       "           (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "         )\n",
       "       )\n",
       "       (mul): QuantizedMultiply(\n",
       "         (param_quantizers): ModuleDict()\n",
       "         (input_quantizers): ModuleList(\n",
       "           (0-1): 2 x None\n",
       "         )\n",
       "         (output_quantizers): ModuleList(\n",
       "           (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "         )\n",
       "       )\n",
       "     )\n",
       "   )\n",
       "   (1): Module(\n",
       "     (short_conv): Module(\n",
       "       (conv): QuantizedConv2d(\n",
       "         48, 24, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "         (param_quantizers): ModuleDict(\n",
       "           (weight): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=True)\n",
       "         )\n",
       "         (input_quantizers): ModuleList(\n",
       "           (0): None\n",
       "         )\n",
       "         (output_quantizers): ModuleList(\n",
       "           (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "         )\n",
       "       )\n",
       "       (bn): Module(\n",
       "         (module_batch_norm_4): FakeQuantizedBatchNorm(\n",
       "           (param_quantizers): ModuleDict()\n",
       "           (input_quantizers): ModuleList(\n",
       "             (0-4): 5 x QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "           )\n",
       "           (output_quantizers): ModuleList(\n",
       "             (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "           )\n",
       "         )\n",
       "       )\n",
       "       (activate): CustomSiLU(\n",
       "         (sigmoid): QuantizedSigmoid(\n",
       "           (param_quantizers): ModuleDict()\n",
       "           (input_quantizers): ModuleList(\n",
       "             (0): None\n",
       "           )\n",
       "           (output_quantizers): ModuleList(\n",
       "             (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "           )\n",
       "         )\n",
       "         (mul): QuantizedMultiply(\n",
       "           (param_quantizers): ModuleDict()\n",
       "           (input_quantizers): ModuleList(\n",
       "             (0-1): 2 x None\n",
       "           )\n",
       "           (output_quantizers): ModuleList(\n",
       "             (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "           )\n",
       "         )\n",
       "       )\n",
       "     )\n",
       "     (main_conv): Module(\n",
       "       (conv): QuantizedConv2d(\n",
       "         48, 24, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "         (param_quantizers): ModuleDict(\n",
       "           (weight): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=True)\n",
       "         )\n",
       "         (input_quantizers): ModuleList(\n",
       "           (0): None\n",
       "         )\n",
       "         (output_quantizers): ModuleList(\n",
       "           (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "         )\n",
       "       )\n",
       "       (bn): Module(\n",
       "         (module_batch_norm_5): FakeQuantizedBatchNorm(\n",
       "           (param_quantizers): ModuleDict()\n",
       "           (input_quantizers): ModuleList(\n",
       "             (0-4): 5 x QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "           )\n",
       "           (output_quantizers): ModuleList(\n",
       "             (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "           )\n",
       "         )\n",
       "       )\n",
       "       (activate): CustomSiLU(\n",
       "         (sigmoid): QuantizedSigmoid(\n",
       "           (param_quantizers): ModuleDict()\n",
       "           (input_quantizers): ModuleList(\n",
       "             (0): None\n",
       "           )\n",
       "           (output_quantizers): ModuleList(\n",
       "             (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "           )\n",
       "         )\n",
       "         (mul): QuantizedMultiply(\n",
       "           (param_quantizers): ModuleDict()\n",
       "           (input_quantizers): ModuleList(\n",
       "             (0-1): 2 x None\n",
       "           )\n",
       "           (output_quantizers): ModuleList(\n",
       "             (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "           )\n",
       "         )\n",
       "       )\n",
       "     )\n",
       "     (blocks): Module(\n",
       "       (0): Module(\n",
       "         (conv1): Module(\n",
       "           (conv): QuantizedConv2d(\n",
       "             24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "             (param_quantizers): ModuleDict(\n",
       "               (weight): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=True)\n",
       "             )\n",
       "             (input_quantizers): ModuleList(\n",
       "               (0): None\n",
       "             )\n",
       "             (output_quantizers): ModuleList(\n",
       "               (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "             )\n",
       "           )\n",
       "           (bn): Module(\n",
       "             (module_batch_norm_6): FakeQuantizedBatchNorm(\n",
       "               (param_quantizers): ModuleDict()\n",
       "               (input_quantizers): ModuleList(\n",
       "                 (0-4): 5 x QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "               )\n",
       "               (output_quantizers): ModuleList(\n",
       "                 (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "               )\n",
       "             )\n",
       "           )\n",
       "           (activate): CustomSiLU(\n",
       "             (sigmoid): QuantizedSigmoid(\n",
       "               (param_quantizers): ModuleDict()\n",
       "               (input_quantizers): ModuleList(\n",
       "                 (0): None\n",
       "               )\n",
       "               (output_quantizers): ModuleList(\n",
       "                 (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "               )\n",
       "             )\n",
       "             (mul): QuantizedMultiply(\n",
       "               (param_quantizers): ModuleDict()\n",
       "               (input_quantizers): ModuleList(\n",
       "                 (0-1): 2 x None\n",
       "               )\n",
       "               (output_quantizers): ModuleList(\n",
       "                 (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "               )\n",
       "             )\n",
       "           )\n",
       "         )\n",
       "         (conv2): Module(\n",
       "           (depthwise_conv): Module(\n",
       "             (conv): QuantizedConv2d(\n",
       "               24, 24, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=24, bias=False\n",
       "               (param_quantizers): ModuleDict(\n",
       "                 (weight): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=True)\n",
       "               )\n",
       "               (input_quantizers): ModuleList(\n",
       "                 (0): None\n",
       "               )\n",
       "               (output_quantizers): ModuleList(\n",
       "                 (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "               )\n",
       "             )\n",
       "             (bn): Module(\n",
       "               (module_batch_norm_7): BatchNorm()\n",
       "             )\n",
       "             (activate): CustomSiLU(\n",
       "               (sigmoid): QuantizedSigmoid(\n",
       "                 (param_quantizers): ModuleDict()\n",
       "                 (input_quantizers): ModuleList(\n",
       "                   (0): None\n",
       "                 )\n",
       "                 (output_quantizers): ModuleList(\n",
       "                   (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "                 )\n",
       "               )\n",
       "               (mul): QuantizedMultiply(\n",
       "                 (param_quantizers): ModuleDict()\n",
       "                 (input_quantizers): ModuleList(\n",
       "                   (0-1): 2 x None\n",
       "                 )\n",
       "                 (output_quantizers): ModuleList(\n",
       "                   (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "                 )\n",
       "               )\n",
       "             )\n",
       "           )\n",
       "           (pointwise_conv): Module(\n",
       "             (conv): QuantizedConv2d(\n",
       "               24, 24, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "               (param_quantizers): ModuleDict(\n",
       "                 (weight): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=True)\n",
       "               )\n",
       "               (input_quantizers): ModuleList(\n",
       "                 (0): None\n",
       "               )\n",
       "               (output_quantizers): ModuleList(\n",
       "                 (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "               )\n",
       "             )\n",
       "             (bn): Module(\n",
       "               (module_batch_norm_8): FakeQuantizedBatchNorm(\n",
       "                 (param_quantizers): ModuleDict()\n",
       "                 (input_quantizers): ModuleList(\n",
       "                   (0-4): 5 x QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "                 )\n",
       "                 (output_quantizers): ModuleList(\n",
       "                   (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "                 )\n",
       "               )\n",
       "             )\n",
       "             (activate): CustomSiLU(\n",
       "               (sigmoid): QuantizedSigmoid(\n",
       "                 (param_quantizers): ModuleDict()\n",
       "                 (input_quantizers): ModuleList(\n",
       "                   (0): None\n",
       "                 )\n",
       "                 (output_quantizers): ModuleList(\n",
       "                   (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "                 )\n",
       "               )\n",
       "               (mul): QuantizedMultiply(\n",
       "                 (param_quantizers): ModuleDict()\n",
       "                 (input_quantizers): ModuleList(\n",
       "                   (0-1): 2 x None\n",
       "                 )\n",
       "                 (output_quantizers): ModuleList(\n",
       "                   (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "                 )\n",
       "               )\n",
       "             )\n",
       "           )\n",
       "         )\n",
       "         (module_add): QuantizedAdd(\n",
       "           (param_quantizers): ModuleDict()\n",
       "           (input_quantizers): ModuleList(\n",
       "             (0-1): 2 x None\n",
       "           )\n",
       "           (output_quantizers): ModuleList(\n",
       "             (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "           )\n",
       "         )\n",
       "       )\n",
       "     )\n",
       "     (attention): Module(\n",
       "       (global_avgpool): FakeQuantizedAdaptiveAvgPool2d(\n",
       "         output_size=1\n",
       "         (param_quantizers): ModuleDict()\n",
       "         (input_quantizers): ModuleList(\n",
       "           (0): None\n",
       "         )\n",
       "         (output_quantizers): ModuleList(\n",
       "           (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "         )\n",
       "       )\n",
       "       (fc): QuantizedConv2d(\n",
       "         48, 48, kernel_size=(1, 1), stride=(1, 1)\n",
       "         (param_quantizers): ModuleDict(\n",
       "           (weight): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=True)\n",
       "           (bias): None\n",
       "         )\n",
       "         (input_quantizers): ModuleList(\n",
       "           (0): None\n",
       "         )\n",
       "         (output_quantizers): ModuleList(\n",
       "           (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "         )\n",
       "       )\n",
       "       (act): FakeQuantizedHardsigmoid(\n",
       "         (param_quantizers): ModuleDict()\n",
       "         (input_quantizers): ModuleList(\n",
       "           (0): None\n",
       "         )\n",
       "         (output_quantizers): ModuleList(\n",
       "           (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "         )\n",
       "       )\n",
       "       (module_mul): QuantizedMultiply(\n",
       "         (param_quantizers): ModuleDict()\n",
       "         (input_quantizers): ModuleList(\n",
       "           (0-1): 2 x None\n",
       "         )\n",
       "         (output_quantizers): ModuleList(\n",
       "           (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "         )\n",
       "       )\n",
       "     )\n",
       "     (final_conv): Module(\n",
       "       (conv): QuantizedConv2d(\n",
       "         48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "         (param_quantizers): ModuleDict(\n",
       "           (weight): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=True)\n",
       "         )\n",
       "         (input_quantizers): ModuleList(\n",
       "           (0): None\n",
       "         )\n",
       "         (output_quantizers): ModuleList(\n",
       "           (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "         )\n",
       "       )\n",
       "       (bn): Module(\n",
       "         (module_batch_norm_9): FakeQuantizedBatchNorm(\n",
       "           (param_quantizers): ModuleDict()\n",
       "           (input_quantizers): ModuleList(\n",
       "             (0-4): 5 x QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "           )\n",
       "           (output_quantizers): ModuleList(\n",
       "             (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "           )\n",
       "         )\n",
       "       )\n",
       "       (activate): CustomSiLU(\n",
       "         (sigmoid): QuantizedSigmoid(\n",
       "           (param_quantizers): ModuleDict()\n",
       "           (input_quantizers): ModuleList(\n",
       "             (0): None\n",
       "           )\n",
       "           (output_quantizers): ModuleList(\n",
       "             (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "           )\n",
       "         )\n",
       "         (mul): QuantizedMultiply(\n",
       "           (param_quantizers): ModuleDict()\n",
       "           (input_quantizers): ModuleList(\n",
       "             (0-1): 2 x None\n",
       "           )\n",
       "           (output_quantizers): ModuleList(\n",
       "             (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "           )\n",
       "         )\n",
       "       )\n",
       "     )\n",
       "     (module_cat): FakeQuantizedConcat(\n",
       "       (param_quantizers): ModuleDict()\n",
       "       (input_quantizers): ModuleList(\n",
       "         (0): None\n",
       "       )\n",
       "       (output_quantizers): ModuleList(\n",
       "         (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "       )\n",
       "     )\n",
       "   )\n",
       " ),\n",
       " 'backbone.stage1.0': Module(\n",
       "   (conv): QuantizedConv2d(\n",
       "     24, 48, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False\n",
       "     (param_quantizers): ModuleDict(\n",
       "       (weight): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=True)\n",
       "     )\n",
       "     (input_quantizers): ModuleList(\n",
       "       (0): None\n",
       "     )\n",
       "     (output_quantizers): ModuleList(\n",
       "       (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "     )\n",
       "   )\n",
       "   (bn): Module(\n",
       "     (module_batch_norm_3): FakeQuantizedBatchNorm(\n",
       "       (param_quantizers): ModuleDict()\n",
       "       (input_quantizers): ModuleList(\n",
       "         (0-4): 5 x QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "       )\n",
       "       (output_quantizers): ModuleList(\n",
       "         (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "       )\n",
       "     )\n",
       "   )\n",
       "   (activate): CustomSiLU(\n",
       "     (sigmoid): QuantizedSigmoid(\n",
       "       (param_quantizers): ModuleDict()\n",
       "       (input_quantizers): ModuleList(\n",
       "         (0): None\n",
       "       )\n",
       "       (output_quantizers): ModuleList(\n",
       "         (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "       )\n",
       "     )\n",
       "     (mul): QuantizedMultiply(\n",
       "       (param_quantizers): ModuleDict()\n",
       "       (input_quantizers): ModuleList(\n",
       "         (0-1): 2 x None\n",
       "       )\n",
       "       (output_quantizers): ModuleList(\n",
       "         (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "       )\n",
       "     )\n",
       "   )\n",
       " ),\n",
       " 'backbone.stage1.0.conv': QuantizedConv2d(\n",
       "   24, 48, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False\n",
       "   (param_quantizers): ModuleDict(\n",
       "     (weight): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=True)\n",
       "   )\n",
       "   (input_quantizers): ModuleList(\n",
       "     (0): None\n",
       "   )\n",
       "   (output_quantizers): ModuleList(\n",
       "     (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "   )\n",
       " ),\n",
       " 'backbone.stage1.0.conv.param_quantizers': ModuleDict(\n",
       "   (weight): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=True)\n",
       " ),\n",
       " 'backbone.stage1.0.conv.param_quantizers.weight': QuantizeDequantize(shape=[1], bitwidth=8, symmetric=True),\n",
       " 'backbone.stage1.0.conv.input_quantizers': ModuleList(\n",
       "   (0): None\n",
       " ),\n",
       " 'backbone.stage1.0.conv.output_quantizers': ModuleList(\n",
       "   (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       " ),\n",
       " 'backbone.stage1.0.conv.output_quantizers.0': QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False),\n",
       " 'backbone.stage1.0.bn': Module(\n",
       "   (module_batch_norm_3): FakeQuantizedBatchNorm(\n",
       "     (param_quantizers): ModuleDict()\n",
       "     (input_quantizers): ModuleList(\n",
       "       (0-4): 5 x QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "     )\n",
       "     (output_quantizers): ModuleList(\n",
       "       (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "     )\n",
       "   )\n",
       " ),\n",
       " 'backbone.stage1.0.bn.module_batch_norm_3': FakeQuantizedBatchNorm(\n",
       "   (param_quantizers): ModuleDict()\n",
       "   (input_quantizers): ModuleList(\n",
       "     (0-4): 5 x QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "   )\n",
       "   (output_quantizers): ModuleList(\n",
       "     (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "   )\n",
       " ),\n",
       " 'backbone.stage1.0.bn.module_batch_norm_3.param_quantizers': ModuleDict(),\n",
       " 'backbone.stage1.0.bn.module_batch_norm_3.input_quantizers': ModuleList(\n",
       "   (0-4): 5 x QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       " ),\n",
       " 'backbone.stage1.0.bn.module_batch_norm_3.input_quantizers.0': QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False),\n",
       " 'backbone.stage1.0.bn.module_batch_norm_3.input_quantizers.1': QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False),\n",
       " 'backbone.stage1.0.bn.module_batch_norm_3.input_quantizers.2': QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False),\n",
       " 'backbone.stage1.0.bn.module_batch_norm_3.input_quantizers.3': QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False),\n",
       " 'backbone.stage1.0.bn.module_batch_norm_3.input_quantizers.4': QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False),\n",
       " 'backbone.stage1.0.bn.module_batch_norm_3.output_quantizers': ModuleList(\n",
       "   (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       " ),\n",
       " 'backbone.stage1.0.bn.module_batch_norm_3.output_quantizers.0': QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False),\n",
       " 'backbone.stage1.0.activate': CustomSiLU(\n",
       "   (sigmoid): QuantizedSigmoid(\n",
       "     (param_quantizers): ModuleDict()\n",
       "     (input_quantizers): ModuleList(\n",
       "       (0): None\n",
       "     )\n",
       "     (output_quantizers): ModuleList(\n",
       "       (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "     )\n",
       "   )\n",
       "   (mul): QuantizedMultiply(\n",
       "     (param_quantizers): ModuleDict()\n",
       "     (input_quantizers): ModuleList(\n",
       "       (0-1): 2 x None\n",
       "     )\n",
       "     (output_quantizers): ModuleList(\n",
       "       (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "     )\n",
       "   )\n",
       " ),\n",
       " 'backbone.stage1.0.activate.sigmoid': QuantizedSigmoid(\n",
       "   (param_quantizers): ModuleDict()\n",
       "   (input_quantizers): ModuleList(\n",
       "     (0): None\n",
       "   )\n",
       "   (output_quantizers): ModuleList(\n",
       "     (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "   )\n",
       " ),\n",
       " 'backbone.stage1.0.activate.sigmoid.param_quantizers': ModuleDict(),\n",
       " 'backbone.stage1.0.activate.sigmoid.input_quantizers': ModuleList(\n",
       "   (0): None\n",
       " ),\n",
       " 'backbone.stage1.0.activate.sigmoid.output_quantizers': ModuleList(\n",
       "   (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       " ),\n",
       " 'backbone.stage1.0.activate.sigmoid.output_quantizers.0': QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False),\n",
       " 'backbone.stage1.0.activate.mul': QuantizedMultiply(\n",
       "   (param_quantizers): ModuleDict()\n",
       "   (input_quantizers): ModuleList(\n",
       "     (0-1): 2 x None\n",
       "   )\n",
       "   (output_quantizers): ModuleList(\n",
       "     (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "   )\n",
       " ),\n",
       " 'backbone.stage1.0.activate.mul.param_quantizers': ModuleDict(),\n",
       " 'backbone.stage1.0.activate.mul.input_quantizers': ModuleList(\n",
       "   (0-1): 2 x None\n",
       " ),\n",
       " 'backbone.stage1.0.activate.mul.output_quantizers': ModuleList(\n",
       "   (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       " ),\n",
       " 'backbone.stage1.0.activate.mul.output_quantizers.0': QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False),\n",
       " 'backbone.stage1.1': Module(\n",
       "   (short_conv): Module(\n",
       "     (conv): QuantizedConv2d(\n",
       "       48, 24, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "       (param_quantizers): ModuleDict(\n",
       "         (weight): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=True)\n",
       "       )\n",
       "       (input_quantizers): ModuleList(\n",
       "         (0): None\n",
       "       )\n",
       "       (output_quantizers): ModuleList(\n",
       "         (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "       )\n",
       "     )\n",
       "     (bn): Module(\n",
       "       (module_batch_norm_4): FakeQuantizedBatchNorm(\n",
       "         (param_quantizers): ModuleDict()\n",
       "         (input_quantizers): ModuleList(\n",
       "           (0-4): 5 x QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "         )\n",
       "         (output_quantizers): ModuleList(\n",
       "           (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "         )\n",
       "       )\n",
       "     )\n",
       "     (activate): CustomSiLU(\n",
       "       (sigmoid): QuantizedSigmoid(\n",
       "         (param_quantizers): ModuleDict()\n",
       "         (input_quantizers): ModuleList(\n",
       "           (0): None\n",
       "         )\n",
       "         (output_quantizers): ModuleList(\n",
       "           (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "         )\n",
       "       )\n",
       "       (mul): QuantizedMultiply(\n",
       "         (param_quantizers): ModuleDict()\n",
       "         (input_quantizers): ModuleList(\n",
       "           (0-1): 2 x None\n",
       "         )\n",
       "         (output_quantizers): ModuleList(\n",
       "           (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "         )\n",
       "       )\n",
       "     )\n",
       "   )\n",
       "   (main_conv): Module(\n",
       "     (conv): QuantizedConv2d(\n",
       "       48, 24, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "       (param_quantizers): ModuleDict(\n",
       "         (weight): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=True)\n",
       "       )\n",
       "       (input_quantizers): ModuleList(\n",
       "         (0): None\n",
       "       )\n",
       "       (output_quantizers): ModuleList(\n",
       "         (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "       )\n",
       "     )\n",
       "     (bn): Module(\n",
       "       (module_batch_norm_5): FakeQuantizedBatchNorm(\n",
       "         (param_quantizers): ModuleDict()\n",
       "         (input_quantizers): ModuleList(\n",
       "           (0-4): 5 x QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "         )\n",
       "         (output_quantizers): ModuleList(\n",
       "           (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "         )\n",
       "       )\n",
       "     )\n",
       "     (activate): CustomSiLU(\n",
       "       (sigmoid): QuantizedSigmoid(\n",
       "         (param_quantizers): ModuleDict()\n",
       "         (input_quantizers): ModuleList(\n",
       "           (0): None\n",
       "         )\n",
       "         (output_quantizers): ModuleList(\n",
       "           (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "         )\n",
       "       )\n",
       "       (mul): QuantizedMultiply(\n",
       "         (param_quantizers): ModuleDict()\n",
       "         (input_quantizers): ModuleList(\n",
       "           (0-1): 2 x None\n",
       "         )\n",
       "         (output_quantizers): ModuleList(\n",
       "           (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "         )\n",
       "       )\n",
       "     )\n",
       "   )\n",
       "   (blocks): Module(\n",
       "     (0): Module(\n",
       "       (conv1): Module(\n",
       "         (conv): QuantizedConv2d(\n",
       "           24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "           (param_quantizers): ModuleDict(\n",
       "             (weight): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=True)\n",
       "           )\n",
       "           (input_quantizers): ModuleList(\n",
       "             (0): None\n",
       "           )\n",
       "           (output_quantizers): ModuleList(\n",
       "             (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "           )\n",
       "         )\n",
       "         (bn): Module(\n",
       "           (module_batch_norm_6): FakeQuantizedBatchNorm(\n",
       "             (param_quantizers): ModuleDict()\n",
       "             (input_quantizers): ModuleList(\n",
       "               (0-4): 5 x QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "             )\n",
       "             (output_quantizers): ModuleList(\n",
       "               (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "             )\n",
       "           )\n",
       "         )\n",
       "         (activate): CustomSiLU(\n",
       "           (sigmoid): QuantizedSigmoid(\n",
       "             (param_quantizers): ModuleDict()\n",
       "             (input_quantizers): ModuleList(\n",
       "               (0): None\n",
       "             )\n",
       "             (output_quantizers): ModuleList(\n",
       "               (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "             )\n",
       "           )\n",
       "           (mul): QuantizedMultiply(\n",
       "             (param_quantizers): ModuleDict()\n",
       "             (input_quantizers): ModuleList(\n",
       "               (0-1): 2 x None\n",
       "             )\n",
       "             (output_quantizers): ModuleList(\n",
       "               (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "             )\n",
       "           )\n",
       "         )\n",
       "       )\n",
       "       (conv2): Module(\n",
       "         (depthwise_conv): Module(\n",
       "           (conv): QuantizedConv2d(\n",
       "             24, 24, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=24, bias=False\n",
       "             (param_quantizers): ModuleDict(\n",
       "               (weight): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=True)\n",
       "             )\n",
       "             (input_quantizers): ModuleList(\n",
       "               (0): None\n",
       "             )\n",
       "             (output_quantizers): ModuleList(\n",
       "               (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "             )\n",
       "           )\n",
       "           (bn): Module(\n",
       "             (module_batch_norm_7): BatchNorm()\n",
       "           )\n",
       "           (activate): CustomSiLU(\n",
       "             (sigmoid): QuantizedSigmoid(\n",
       "               (param_quantizers): ModuleDict()\n",
       "               (input_quantizers): ModuleList(\n",
       "                 (0): None\n",
       "               )\n",
       "               (output_quantizers): ModuleList(\n",
       "                 (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "               )\n",
       "             )\n",
       "             (mul): QuantizedMultiply(\n",
       "               (param_quantizers): ModuleDict()\n",
       "               (input_quantizers): ModuleList(\n",
       "                 (0-1): 2 x None\n",
       "               )\n",
       "               (output_quantizers): ModuleList(\n",
       "                 (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "               )\n",
       "             )\n",
       "           )\n",
       "         )\n",
       "         (pointwise_conv): Module(\n",
       "           (conv): QuantizedConv2d(\n",
       "             24, 24, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "             (param_quantizers): ModuleDict(\n",
       "               (weight): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=True)\n",
       "             )\n",
       "             (input_quantizers): ModuleList(\n",
       "               (0): None\n",
       "             )\n",
       "             (output_quantizers): ModuleList(\n",
       "               (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "             )\n",
       "           )\n",
       "           (bn): Module(\n",
       "             (module_batch_norm_8): FakeQuantizedBatchNorm(\n",
       "               (param_quantizers): ModuleDict()\n",
       "               (input_quantizers): ModuleList(\n",
       "                 (0-4): 5 x QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "               )\n",
       "               (output_quantizers): ModuleList(\n",
       "                 (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "               )\n",
       "             )\n",
       "           )\n",
       "           (activate): CustomSiLU(\n",
       "             (sigmoid): QuantizedSigmoid(\n",
       "               (param_quantizers): ModuleDict()\n",
       "               (input_quantizers): ModuleList(\n",
       "                 (0): None\n",
       "               )\n",
       "               (output_quantizers): ModuleList(\n",
       "                 (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "               )\n",
       "             )\n",
       "             (mul): QuantizedMultiply(\n",
       "               (param_quantizers): ModuleDict()\n",
       "               (input_quantizers): ModuleList(\n",
       "                 (0-1): 2 x None\n",
       "               )\n",
       "               (output_quantizers): ModuleList(\n",
       "                 (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "               )\n",
       "             )\n",
       "           )\n",
       "         )\n",
       "       )\n",
       "       (module_add): QuantizedAdd(\n",
       "         (param_quantizers): ModuleDict()\n",
       "         (input_quantizers): ModuleList(\n",
       "           (0-1): 2 x None\n",
       "         )\n",
       "         (output_quantizers): ModuleList(\n",
       "           (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "         )\n",
       "       )\n",
       "     )\n",
       "   )\n",
       "   (attention): Module(\n",
       "     (global_avgpool): FakeQuantizedAdaptiveAvgPool2d(\n",
       "       output_size=1\n",
       "       (param_quantizers): ModuleDict()\n",
       "       (input_quantizers): ModuleList(\n",
       "         (0): None\n",
       "       )\n",
       "       (output_quantizers): ModuleList(\n",
       "         (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "       )\n",
       "     )\n",
       "     (fc): QuantizedConv2d(\n",
       "       48, 48, kernel_size=(1, 1), stride=(1, 1)\n",
       "       (param_quantizers): ModuleDict(\n",
       "         (weight): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=True)\n",
       "         (bias): None\n",
       "       )\n",
       "       (input_quantizers): ModuleList(\n",
       "         (0): None\n",
       "       )\n",
       "       (output_quantizers): ModuleList(\n",
       "         (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "       )\n",
       "     )\n",
       "     (act): FakeQuantizedHardsigmoid(\n",
       "       (param_quantizers): ModuleDict()\n",
       "       (input_quantizers): ModuleList(\n",
       "         (0): None\n",
       "       )\n",
       "       (output_quantizers): ModuleList(\n",
       "         (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "       )\n",
       "     )\n",
       "     (module_mul): QuantizedMultiply(\n",
       "       (param_quantizers): ModuleDict()\n",
       "       (input_quantizers): ModuleList(\n",
       "         (0-1): 2 x None\n",
       "       )\n",
       "       (output_quantizers): ModuleList(\n",
       "         (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "       )\n",
       "     )\n",
       "   )\n",
       "   (final_conv): Module(\n",
       "     (conv): QuantizedConv2d(\n",
       "       48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "       (param_quantizers): ModuleDict(\n",
       "         (weight): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=True)\n",
       "       )\n",
       "       (input_quantizers): ModuleList(\n",
       "         (0): None\n",
       "       )\n",
       "       (output_quantizers): ModuleList(\n",
       "         (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "       )\n",
       "     )\n",
       "     (bn): Module(\n",
       "       (module_batch_norm_9): FakeQuantizedBatchNorm(\n",
       "         (param_quantizers): ModuleDict()\n",
       "         (input_quantizers): ModuleList(\n",
       "           (0-4): 5 x QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "         )\n",
       "         (output_quantizers): ModuleList(\n",
       "           (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "         )\n",
       "       )\n",
       "     )\n",
       "     (activate): CustomSiLU(\n",
       "       (sigmoid): QuantizedSigmoid(\n",
       "         (param_quantizers): ModuleDict()\n",
       "         (input_quantizers): ModuleList(\n",
       "           (0): None\n",
       "         )\n",
       "         (output_quantizers): ModuleList(\n",
       "           (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "         )\n",
       "       )\n",
       "       (mul): QuantizedMultiply(\n",
       "         (param_quantizers): ModuleDict()\n",
       "         (input_quantizers): ModuleList(\n",
       "           (0-1): 2 x None\n",
       "         )\n",
       "         (output_quantizers): ModuleList(\n",
       "           (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "         )\n",
       "       )\n",
       "     )\n",
       "   )\n",
       "   (module_cat): FakeQuantizedConcat(\n",
       "     (param_quantizers): ModuleDict()\n",
       "     (input_quantizers): ModuleList(\n",
       "       (0): None\n",
       "     )\n",
       "     (output_quantizers): ModuleList(\n",
       "       (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "     )\n",
       "   )\n",
       " ),\n",
       " 'backbone.stage1.1.short_conv': Module(\n",
       "   (conv): QuantizedConv2d(\n",
       "     48, 24, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "     (param_quantizers): ModuleDict(\n",
       "       (weight): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=True)\n",
       "     )\n",
       "     (input_quantizers): ModuleList(\n",
       "       (0): None\n",
       "     )\n",
       "     (output_quantizers): ModuleList(\n",
       "       (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "     )\n",
       "   )\n",
       "   (bn): Module(\n",
       "     (module_batch_norm_4): FakeQuantizedBatchNorm(\n",
       "       (param_quantizers): ModuleDict()\n",
       "       (input_quantizers): ModuleList(\n",
       "         (0-4): 5 x QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "       )\n",
       "       (output_quantizers): ModuleList(\n",
       "         (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "       )\n",
       "     )\n",
       "   )\n",
       "   (activate): CustomSiLU(\n",
       "     (sigmoid): QuantizedSigmoid(\n",
       "       (param_quantizers): ModuleDict()\n",
       "       (input_quantizers): ModuleList(\n",
       "         (0): None\n",
       "       )\n",
       "       (output_quantizers): ModuleList(\n",
       "         (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "       )\n",
       "     )\n",
       "     (mul): QuantizedMultiply(\n",
       "       (param_quantizers): ModuleDict()\n",
       "       (input_quantizers): ModuleList(\n",
       "         (0-1): 2 x None\n",
       "       )\n",
       "       (output_quantizers): ModuleList(\n",
       "         (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "       )\n",
       "     )\n",
       "   )\n",
       " ),\n",
       " 'backbone.stage1.1.short_conv.conv': QuantizedConv2d(\n",
       "   48, 24, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "   (param_quantizers): ModuleDict(\n",
       "     (weight): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=True)\n",
       "   )\n",
       "   (input_quantizers): ModuleList(\n",
       "     (0): None\n",
       "   )\n",
       "   (output_quantizers): ModuleList(\n",
       "     (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "   )\n",
       " ),\n",
       " 'backbone.stage1.1.short_conv.conv.param_quantizers': ModuleDict(\n",
       "   (weight): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=True)\n",
       " ),\n",
       " 'backbone.stage1.1.short_conv.conv.param_quantizers.weight': QuantizeDequantize(shape=[1], bitwidth=8, symmetric=True),\n",
       " 'backbone.stage1.1.short_conv.conv.input_quantizers': ModuleList(\n",
       "   (0): None\n",
       " ),\n",
       " 'backbone.stage1.1.short_conv.conv.output_quantizers': ModuleList(\n",
       "   (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       " ),\n",
       " 'backbone.stage1.1.short_conv.conv.output_quantizers.0': QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False),\n",
       " 'backbone.stage1.1.short_conv.bn': Module(\n",
       "   (module_batch_norm_4): FakeQuantizedBatchNorm(\n",
       "     (param_quantizers): ModuleDict()\n",
       "     (input_quantizers): ModuleList(\n",
       "       (0-4): 5 x QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "     )\n",
       "     (output_quantizers): ModuleList(\n",
       "       (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "     )\n",
       "   )\n",
       " ),\n",
       " 'backbone.stage1.1.short_conv.bn.module_batch_norm_4': FakeQuantizedBatchNorm(\n",
       "   (param_quantizers): ModuleDict()\n",
       "   (input_quantizers): ModuleList(\n",
       "     (0-4): 5 x QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "   )\n",
       "   (output_quantizers): ModuleList(\n",
       "     (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "   )\n",
       " ),\n",
       " 'backbone.stage1.1.short_conv.bn.module_batch_norm_4.param_quantizers': ModuleDict(),\n",
       " 'backbone.stage1.1.short_conv.bn.module_batch_norm_4.input_quantizers': ModuleList(\n",
       "   (0-4): 5 x QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       " ),\n",
       " 'backbone.stage1.1.short_conv.bn.module_batch_norm_4.input_quantizers.0': QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False),\n",
       " 'backbone.stage1.1.short_conv.bn.module_batch_norm_4.input_quantizers.1': QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False),\n",
       " 'backbone.stage1.1.short_conv.bn.module_batch_norm_4.input_quantizers.2': QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False),\n",
       " 'backbone.stage1.1.short_conv.bn.module_batch_norm_4.input_quantizers.3': QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False),\n",
       " 'backbone.stage1.1.short_conv.bn.module_batch_norm_4.input_quantizers.4': QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False),\n",
       " 'backbone.stage1.1.short_conv.bn.module_batch_norm_4.output_quantizers': ModuleList(\n",
       "   (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       " ),\n",
       " 'backbone.stage1.1.short_conv.bn.module_batch_norm_4.output_quantizers.0': QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False),\n",
       " 'backbone.stage1.1.short_conv.activate': CustomSiLU(\n",
       "   (sigmoid): QuantizedSigmoid(\n",
       "     (param_quantizers): ModuleDict()\n",
       "     (input_quantizers): ModuleList(\n",
       "       (0): None\n",
       "     )\n",
       "     (output_quantizers): ModuleList(\n",
       "       (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "     )\n",
       "   )\n",
       "   (mul): QuantizedMultiply(\n",
       "     (param_quantizers): ModuleDict()\n",
       "     (input_quantizers): ModuleList(\n",
       "       (0-1): 2 x None\n",
       "     )\n",
       "     (output_quantizers): ModuleList(\n",
       "       (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "     )\n",
       "   )\n",
       " ),\n",
       " 'backbone.stage1.1.short_conv.activate.sigmoid': QuantizedSigmoid(\n",
       "   (param_quantizers): ModuleDict()\n",
       "   (input_quantizers): ModuleList(\n",
       "     (0): None\n",
       "   )\n",
       "   (output_quantizers): ModuleList(\n",
       "     (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "   )\n",
       " ),\n",
       " 'backbone.stage1.1.short_conv.activate.sigmoid.param_quantizers': ModuleDict(),\n",
       " 'backbone.stage1.1.short_conv.activate.sigmoid.input_quantizers': ModuleList(\n",
       "   (0): None\n",
       " ),\n",
       " 'backbone.stage1.1.short_conv.activate.sigmoid.output_quantizers': ModuleList(\n",
       "   (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       " ),\n",
       " 'backbone.stage1.1.short_conv.activate.sigmoid.output_quantizers.0': QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False),\n",
       " 'backbone.stage1.1.short_conv.activate.mul': QuantizedMultiply(\n",
       "   (param_quantizers): ModuleDict()\n",
       "   (input_quantizers): ModuleList(\n",
       "     (0-1): 2 x None\n",
       "   )\n",
       "   (output_quantizers): ModuleList(\n",
       "     (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "   )\n",
       " ),\n",
       " 'backbone.stage1.1.short_conv.activate.mul.param_quantizers': ModuleDict(),\n",
       " 'backbone.stage1.1.short_conv.activate.mul.input_quantizers': ModuleList(\n",
       "   (0-1): 2 x None\n",
       " ),\n",
       " 'backbone.stage1.1.short_conv.activate.mul.output_quantizers': ModuleList(\n",
       "   (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       " ),\n",
       " 'backbone.stage1.1.short_conv.activate.mul.output_quantizers.0': QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False),\n",
       " 'backbone.stage1.1.main_conv': Module(\n",
       "   (conv): QuantizedConv2d(\n",
       "     48, 24, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "     (param_quantizers): ModuleDict(\n",
       "       (weight): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=True)\n",
       "     )\n",
       "     (input_quantizers): ModuleList(\n",
       "       (0): None\n",
       "     )\n",
       "     (output_quantizers): ModuleList(\n",
       "       (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "     )\n",
       "   )\n",
       "   (bn): Module(\n",
       "     (module_batch_norm_5): FakeQuantizedBatchNorm(\n",
       "       (param_quantizers): ModuleDict()\n",
       "       (input_quantizers): ModuleList(\n",
       "         (0-4): 5 x QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "       )\n",
       "       (output_quantizers): ModuleList(\n",
       "         (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "       )\n",
       "     )\n",
       "   )\n",
       "   (activate): CustomSiLU(\n",
       "     (sigmoid): QuantizedSigmoid(\n",
       "       (param_quantizers): ModuleDict()\n",
       "       (input_quantizers): ModuleList(\n",
       "         (0): None\n",
       "       )\n",
       "       (output_quantizers): ModuleList(\n",
       "         (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "       )\n",
       "     )\n",
       "     (mul): QuantizedMultiply(\n",
       "       (param_quantizers): ModuleDict()\n",
       "       (input_quantizers): ModuleList(\n",
       "         (0-1): 2 x None\n",
       "       )\n",
       "       (output_quantizers): ModuleList(\n",
       "         (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "       )\n",
       "     )\n",
       "   )\n",
       " ),\n",
       " 'backbone.stage1.1.main_conv.conv': QuantizedConv2d(\n",
       "   48, 24, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "   (param_quantizers): ModuleDict(\n",
       "     (weight): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=True)\n",
       "   )\n",
       "   (input_quantizers): ModuleList(\n",
       "     (0): None\n",
       "   )\n",
       "   (output_quantizers): ModuleList(\n",
       "     (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "   )\n",
       " ),\n",
       " 'backbone.stage1.1.main_conv.conv.param_quantizers': ModuleDict(\n",
       "   (weight): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=True)\n",
       " ),\n",
       " 'backbone.stage1.1.main_conv.conv.param_quantizers.weight': QuantizeDequantize(shape=[1], bitwidth=8, symmetric=True),\n",
       " 'backbone.stage1.1.main_conv.conv.input_quantizers': ModuleList(\n",
       "   (0): None\n",
       " ),\n",
       " 'backbone.stage1.1.main_conv.conv.output_quantizers': ModuleList(\n",
       "   (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       " ),\n",
       " 'backbone.stage1.1.main_conv.conv.output_quantizers.0': QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False),\n",
       " 'backbone.stage1.1.main_conv.bn': Module(\n",
       "   (module_batch_norm_5): FakeQuantizedBatchNorm(\n",
       "     (param_quantizers): ModuleDict()\n",
       "     (input_quantizers): ModuleList(\n",
       "       (0-4): 5 x QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "     )\n",
       "     (output_quantizers): ModuleList(\n",
       "       (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "     )\n",
       "   )\n",
       " ),\n",
       " 'backbone.stage1.1.main_conv.bn.module_batch_norm_5': FakeQuantizedBatchNorm(\n",
       "   (param_quantizers): ModuleDict()\n",
       "   (input_quantizers): ModuleList(\n",
       "     (0-4): 5 x QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "   )\n",
       "   (output_quantizers): ModuleList(\n",
       "     (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "   )\n",
       " ),\n",
       " 'backbone.stage1.1.main_conv.bn.module_batch_norm_5.param_quantizers': ModuleDict(),\n",
       " 'backbone.stage1.1.main_conv.bn.module_batch_norm_5.input_quantizers': ModuleList(\n",
       "   (0-4): 5 x QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       " ),\n",
       " 'backbone.stage1.1.main_conv.bn.module_batch_norm_5.input_quantizers.0': QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False),\n",
       " 'backbone.stage1.1.main_conv.bn.module_batch_norm_5.input_quantizers.1': QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False),\n",
       " 'backbone.stage1.1.main_conv.bn.module_batch_norm_5.input_quantizers.2': QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False),\n",
       " 'backbone.stage1.1.main_conv.bn.module_batch_norm_5.input_quantizers.3': QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False),\n",
       " 'backbone.stage1.1.main_conv.bn.module_batch_norm_5.input_quantizers.4': QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False),\n",
       " 'backbone.stage1.1.main_conv.bn.module_batch_norm_5.output_quantizers': ModuleList(\n",
       "   (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       " ),\n",
       " 'backbone.stage1.1.main_conv.bn.module_batch_norm_5.output_quantizers.0': QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False),\n",
       " 'backbone.stage1.1.main_conv.activate': CustomSiLU(\n",
       "   (sigmoid): QuantizedSigmoid(\n",
       "     (param_quantizers): ModuleDict()\n",
       "     (input_quantizers): ModuleList(\n",
       "       (0): None\n",
       "     )\n",
       "     (output_quantizers): ModuleList(\n",
       "       (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "     )\n",
       "   )\n",
       "   (mul): QuantizedMultiply(\n",
       "     (param_quantizers): ModuleDict()\n",
       "     (input_quantizers): ModuleList(\n",
       "       (0-1): 2 x None\n",
       "     )\n",
       "     (output_quantizers): ModuleList(\n",
       "       (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "     )\n",
       "   )\n",
       " ),\n",
       " 'backbone.stage1.1.main_conv.activate.sigmoid': QuantizedSigmoid(\n",
       "   (param_quantizers): ModuleDict()\n",
       "   (input_quantizers): ModuleList(\n",
       "     (0): None\n",
       "   )\n",
       "   (output_quantizers): ModuleList(\n",
       "     (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "   )\n",
       " ),\n",
       " 'backbone.stage1.1.main_conv.activate.sigmoid.param_quantizers': ModuleDict(),\n",
       " 'backbone.stage1.1.main_conv.activate.sigmoid.input_quantizers': ModuleList(\n",
       "   (0): None\n",
       " ),\n",
       " 'backbone.stage1.1.main_conv.activate.sigmoid.output_quantizers': ModuleList(\n",
       "   (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       " ),\n",
       " 'backbone.stage1.1.main_conv.activate.sigmoid.output_quantizers.0': QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False),\n",
       " 'backbone.stage1.1.main_conv.activate.mul': QuantizedMultiply(\n",
       "   (param_quantizers): ModuleDict()\n",
       "   (input_quantizers): ModuleList(\n",
       "     (0-1): 2 x None\n",
       "   )\n",
       "   (output_quantizers): ModuleList(\n",
       "     (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "   )\n",
       " ),\n",
       " 'backbone.stage1.1.main_conv.activate.mul.param_quantizers': ModuleDict(),\n",
       " 'backbone.stage1.1.main_conv.activate.mul.input_quantizers': ModuleList(\n",
       "   (0-1): 2 x None\n",
       " ),\n",
       " 'backbone.stage1.1.main_conv.activate.mul.output_quantizers': ModuleList(\n",
       "   (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       " ),\n",
       " 'backbone.stage1.1.main_conv.activate.mul.output_quantizers.0': QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False),\n",
       " 'backbone.stage1.1.blocks': Module(\n",
       "   (0): Module(\n",
       "     (conv1): Module(\n",
       "       (conv): QuantizedConv2d(\n",
       "         24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "         (param_quantizers): ModuleDict(\n",
       "           (weight): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=True)\n",
       "         )\n",
       "         (input_quantizers): ModuleList(\n",
       "           (0): None\n",
       "         )\n",
       "         (output_quantizers): ModuleList(\n",
       "           (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "         )\n",
       "       )\n",
       "       (bn): Module(\n",
       "         (module_batch_norm_6): FakeQuantizedBatchNorm(\n",
       "           (param_quantizers): ModuleDict()\n",
       "           (input_quantizers): ModuleList(\n",
       "             (0-4): 5 x QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "           )\n",
       "           (output_quantizers): ModuleList(\n",
       "             (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "           )\n",
       "         )\n",
       "       )\n",
       "       (activate): CustomSiLU(\n",
       "         (sigmoid): QuantizedSigmoid(\n",
       "           (param_quantizers): ModuleDict()\n",
       "           (input_quantizers): ModuleList(\n",
       "             (0): None\n",
       "           )\n",
       "           (output_quantizers): ModuleList(\n",
       "             (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "           )\n",
       "         )\n",
       "         (mul): QuantizedMultiply(\n",
       "           (param_quantizers): ModuleDict()\n",
       "           (input_quantizers): ModuleList(\n",
       "             (0-1): 2 x None\n",
       "           )\n",
       "           (output_quantizers): ModuleList(\n",
       "             (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "           )\n",
       "         )\n",
       "       )\n",
       "     )\n",
       "     (conv2): Module(\n",
       "       (depthwise_conv): Module(\n",
       "         (conv): QuantizedConv2d(\n",
       "           24, 24, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=24, bias=False\n",
       "           (param_quantizers): ModuleDict(\n",
       "             (weight): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=True)\n",
       "           )\n",
       "           (input_quantizers): ModuleList(\n",
       "             (0): None\n",
       "           )\n",
       "           (output_quantizers): ModuleList(\n",
       "             (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "           )\n",
       "         )\n",
       "         (bn): Module(\n",
       "           (module_batch_norm_7): BatchNorm()\n",
       "         )\n",
       "         (activate): CustomSiLU(\n",
       "           (sigmoid): QuantizedSigmoid(\n",
       "             (param_quantizers): ModuleDict()\n",
       "             (input_quantizers): ModuleList(\n",
       "               (0): None\n",
       "             )\n",
       "             (output_quantizers): ModuleList(\n",
       "               (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "             )\n",
       "           )\n",
       "           (mul): QuantizedMultiply(\n",
       "             (param_quantizers): ModuleDict()\n",
       "             (input_quantizers): ModuleList(\n",
       "               (0-1): 2 x None\n",
       "             )\n",
       "             (output_quantizers): ModuleList(\n",
       "               (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "             )\n",
       "           )\n",
       "         )\n",
       "       )\n",
       "       (pointwise_conv): Module(\n",
       "         (conv): QuantizedConv2d(\n",
       "           24, 24, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "           (param_quantizers): ModuleDict(\n",
       "             (weight): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=True)\n",
       "           )\n",
       "           (input_quantizers): ModuleList(\n",
       "             (0): None\n",
       "           )\n",
       "           (output_quantizers): ModuleList(\n",
       "             (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "           )\n",
       "         )\n",
       "         (bn): Module(\n",
       "           (module_batch_norm_8): FakeQuantizedBatchNorm(\n",
       "             (param_quantizers): ModuleDict()\n",
       "             (input_quantizers): ModuleList(\n",
       "               (0-4): 5 x QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "             )\n",
       "             (output_quantizers): ModuleList(\n",
       "               (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "             )\n",
       "           )\n",
       "         )\n",
       "         (activate): CustomSiLU(\n",
       "           (sigmoid): QuantizedSigmoid(\n",
       "             (param_quantizers): ModuleDict()\n",
       "             (input_quantizers): ModuleList(\n",
       "               (0): None\n",
       "             )\n",
       "             (output_quantizers): ModuleList(\n",
       "               (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "             )\n",
       "           )\n",
       "           (mul): QuantizedMultiply(\n",
       "             (param_quantizers): ModuleDict()\n",
       "             (input_quantizers): ModuleList(\n",
       "               (0-1): 2 x None\n",
       "             )\n",
       "             (output_quantizers): ModuleList(\n",
       "               (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "             )\n",
       "           )\n",
       "         )\n",
       "       )\n",
       "     )\n",
       "     (module_add): QuantizedAdd(\n",
       "       (param_quantizers): ModuleDict()\n",
       "       (input_quantizers): ModuleList(\n",
       "         (0-1): 2 x None\n",
       "       )\n",
       "       (output_quantizers): ModuleList(\n",
       "         (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "       )\n",
       "     )\n",
       "   )\n",
       " ),\n",
       " 'backbone.stage1.1.blocks.0': Module(\n",
       "   (conv1): Module(\n",
       "     (conv): QuantizedConv2d(\n",
       "       24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "       (param_quantizers): ModuleDict(\n",
       "         (weight): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=True)\n",
       "       )\n",
       "       (input_quantizers): ModuleList(\n",
       "         (0): None\n",
       "       )\n",
       "       (output_quantizers): ModuleList(\n",
       "         (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "       )\n",
       "     )\n",
       "     (bn): Module(\n",
       "       (module_batch_norm_6): FakeQuantizedBatchNorm(\n",
       "         (param_quantizers): ModuleDict()\n",
       "         (input_quantizers): ModuleList(\n",
       "           (0-4): 5 x QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "         )\n",
       "         (output_quantizers): ModuleList(\n",
       "           (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "         )\n",
       "       )\n",
       "     )\n",
       "     (activate): CustomSiLU(\n",
       "       (sigmoid): QuantizedSigmoid(\n",
       "         (param_quantizers): ModuleDict()\n",
       "         (input_quantizers): ModuleList(\n",
       "           (0): None\n",
       "         )\n",
       "         (output_quantizers): ModuleList(\n",
       "           (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "         )\n",
       "       )\n",
       "       (mul): QuantizedMultiply(\n",
       "         (param_quantizers): ModuleDict()\n",
       "         (input_quantizers): ModuleList(\n",
       "           (0-1): 2 x None\n",
       "         )\n",
       "         (output_quantizers): ModuleList(\n",
       "           (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "         )\n",
       "       )\n",
       "     )\n",
       "   )\n",
       "   (conv2): Module(\n",
       "     (depthwise_conv): Module(\n",
       "       (conv): QuantizedConv2d(\n",
       "         24, 24, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=24, bias=False\n",
       "         (param_quantizers): ModuleDict(\n",
       "           (weight): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=True)\n",
       "         )\n",
       "         (input_quantizers): ModuleList(\n",
       "           (0): None\n",
       "         )\n",
       "         (output_quantizers): ModuleList(\n",
       "           (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "         )\n",
       "       )\n",
       "       (bn): Module(\n",
       "         (module_batch_norm_7): BatchNorm()\n",
       "       )\n",
       "       (activate): CustomSiLU(\n",
       "         (sigmoid): QuantizedSigmoid(\n",
       "           (param_quantizers): ModuleDict()\n",
       "           (input_quantizers): ModuleList(\n",
       "             (0): None\n",
       "           )\n",
       "           (output_quantizers): ModuleList(\n",
       "             (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "           )\n",
       "         )\n",
       "         (mul): QuantizedMultiply(\n",
       "           (param_quantizers): ModuleDict()\n",
       "           (input_quantizers): ModuleList(\n",
       "             (0-1): 2 x None\n",
       "           )\n",
       "           (output_quantizers): ModuleList(\n",
       "             (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "           )\n",
       "         )\n",
       "       )\n",
       "     )\n",
       "     (pointwise_conv): Module(\n",
       "       (conv): QuantizedConv2d(\n",
       "         24, 24, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "         (param_quantizers): ModuleDict(\n",
       "           (weight): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=True)\n",
       "         )\n",
       "         (input_quantizers): ModuleList(\n",
       "           (0): None\n",
       "         )\n",
       "         (output_quantizers): ModuleList(\n",
       "           (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "         )\n",
       "       )\n",
       "       (bn): Module(\n",
       "         (module_batch_norm_8): FakeQuantizedBatchNorm(\n",
       "           (param_quantizers): ModuleDict()\n",
       "           (input_quantizers): ModuleList(\n",
       "             (0-4): 5 x QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "           )\n",
       "           (output_quantizers): ModuleList(\n",
       "             (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "           )\n",
       "         )\n",
       "       )\n",
       "       (activate): CustomSiLU(\n",
       "         (sigmoid): QuantizedSigmoid(\n",
       "           (param_quantizers): ModuleDict()\n",
       "           (input_quantizers): ModuleList(\n",
       "             (0): None\n",
       "           )\n",
       "           (output_quantizers): ModuleList(\n",
       "             (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "           )\n",
       "         )\n",
       "         (mul): QuantizedMultiply(\n",
       "           (param_quantizers): ModuleDict()\n",
       "           (input_quantizers): ModuleList(\n",
       "             (0-1): 2 x None\n",
       "           )\n",
       "           (output_quantizers): ModuleList(\n",
       "             (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "           )\n",
       "         )\n",
       "       )\n",
       "     )\n",
       "   )\n",
       "   (module_add): QuantizedAdd(\n",
       "     (param_quantizers): ModuleDict()\n",
       "     (input_quantizers): ModuleList(\n",
       "       (0-1): 2 x None\n",
       "     )\n",
       "     (output_quantizers): ModuleList(\n",
       "       (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "     )\n",
       "   )\n",
       " ),\n",
       " 'backbone.stage1.1.blocks.0.conv1': Module(\n",
       "   (conv): QuantizedConv2d(\n",
       "     24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "     (param_quantizers): ModuleDict(\n",
       "       (weight): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=True)\n",
       "     )\n",
       "     (input_quantizers): ModuleList(\n",
       "       (0): None\n",
       "     )\n",
       "     (output_quantizers): ModuleList(\n",
       "       (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "     )\n",
       "   )\n",
       "   (bn): Module(\n",
       "     (module_batch_norm_6): FakeQuantizedBatchNorm(\n",
       "       (param_quantizers): ModuleDict()\n",
       "       (input_quantizers): ModuleList(\n",
       "         (0-4): 5 x QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "       )\n",
       "       (output_quantizers): ModuleList(\n",
       "         (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "       )\n",
       "     )\n",
       "   )\n",
       "   (activate): CustomSiLU(\n",
       "     (sigmoid): QuantizedSigmoid(\n",
       "       (param_quantizers): ModuleDict()\n",
       "       (input_quantizers): ModuleList(\n",
       "         (0): None\n",
       "       )\n",
       "       (output_quantizers): ModuleList(\n",
       "         (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "       )\n",
       "     )\n",
       "     (mul): QuantizedMultiply(\n",
       "       (param_quantizers): ModuleDict()\n",
       "       (input_quantizers): ModuleList(\n",
       "         (0-1): 2 x None\n",
       "       )\n",
       "       (output_quantizers): ModuleList(\n",
       "         (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "       )\n",
       "     )\n",
       "   )\n",
       " ),\n",
       " 'backbone.stage1.1.blocks.0.conv1.conv': QuantizedConv2d(\n",
       "   24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "   (param_quantizers): ModuleDict(\n",
       "     (weight): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=True)\n",
       "   )\n",
       "   (input_quantizers): ModuleList(\n",
       "     (0): None\n",
       "   )\n",
       "   (output_quantizers): ModuleList(\n",
       "     (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "   )\n",
       " ),\n",
       " 'backbone.stage1.1.blocks.0.conv1.conv.param_quantizers': ModuleDict(\n",
       "   (weight): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=True)\n",
       " ),\n",
       " 'backbone.stage1.1.blocks.0.conv1.conv.param_quantizers.weight': QuantizeDequantize(shape=[1], bitwidth=8, symmetric=True),\n",
       " 'backbone.stage1.1.blocks.0.conv1.conv.input_quantizers': ModuleList(\n",
       "   (0): None\n",
       " ),\n",
       " 'backbone.stage1.1.blocks.0.conv1.conv.output_quantizers': ModuleList(\n",
       "   (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       " ),\n",
       " 'backbone.stage1.1.blocks.0.conv1.conv.output_quantizers.0': QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False),\n",
       " 'backbone.stage1.1.blocks.0.conv1.bn': Module(\n",
       "   (module_batch_norm_6): FakeQuantizedBatchNorm(\n",
       "     (param_quantizers): ModuleDict()\n",
       "     (input_quantizers): ModuleList(\n",
       "       (0-4): 5 x QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "     )\n",
       "     (output_quantizers): ModuleList(\n",
       "       (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "     )\n",
       "   )\n",
       " ),\n",
       " 'backbone.stage1.1.blocks.0.conv1.bn.module_batch_norm_6': FakeQuantizedBatchNorm(\n",
       "   (param_quantizers): ModuleDict()\n",
       "   (input_quantizers): ModuleList(\n",
       "     (0-4): 5 x QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "   )\n",
       "   (output_quantizers): ModuleList(\n",
       "     (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "   )\n",
       " ),\n",
       " 'backbone.stage1.1.blocks.0.conv1.bn.module_batch_norm_6.param_quantizers': ModuleDict(),\n",
       " 'backbone.stage1.1.blocks.0.conv1.bn.module_batch_norm_6.input_quantizers': ModuleList(\n",
       "   (0-4): 5 x QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       " ),\n",
       " 'backbone.stage1.1.blocks.0.conv1.bn.module_batch_norm_6.input_quantizers.0': QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False),\n",
       " 'backbone.stage1.1.blocks.0.conv1.bn.module_batch_norm_6.input_quantizers.1': QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False),\n",
       " 'backbone.stage1.1.blocks.0.conv1.bn.module_batch_norm_6.input_quantizers.2': QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False),\n",
       " 'backbone.stage1.1.blocks.0.conv1.bn.module_batch_norm_6.input_quantizers.3': QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False),\n",
       " 'backbone.stage1.1.blocks.0.conv1.bn.module_batch_norm_6.input_quantizers.4': QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False),\n",
       " 'backbone.stage1.1.blocks.0.conv1.bn.module_batch_norm_6.output_quantizers': ModuleList(\n",
       "   (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       " ),\n",
       " 'backbone.stage1.1.blocks.0.conv1.bn.module_batch_norm_6.output_quantizers.0': QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False),\n",
       " 'backbone.stage1.1.blocks.0.conv1.activate': CustomSiLU(\n",
       "   (sigmoid): QuantizedSigmoid(\n",
       "     (param_quantizers): ModuleDict()\n",
       "     (input_quantizers): ModuleList(\n",
       "       (0): None\n",
       "     )\n",
       "     (output_quantizers): ModuleList(\n",
       "       (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "     )\n",
       "   )\n",
       "   (mul): QuantizedMultiply(\n",
       "     (param_quantizers): ModuleDict()\n",
       "     (input_quantizers): ModuleList(\n",
       "       (0-1): 2 x None\n",
       "     )\n",
       "     (output_quantizers): ModuleList(\n",
       "       (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "     )\n",
       "   )\n",
       " ),\n",
       " 'backbone.stage1.1.blocks.0.conv1.activate.sigmoid': QuantizedSigmoid(\n",
       "   (param_quantizers): ModuleDict()\n",
       "   (input_quantizers): ModuleList(\n",
       "     (0): None\n",
       "   )\n",
       "   (output_quantizers): ModuleList(\n",
       "     (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "   )\n",
       " ),\n",
       " 'backbone.stage1.1.blocks.0.conv1.activate.sigmoid.param_quantizers': ModuleDict(),\n",
       " 'backbone.stage1.1.blocks.0.conv1.activate.sigmoid.input_quantizers': ModuleList(\n",
       "   (0): None\n",
       " ),\n",
       " 'backbone.stage1.1.blocks.0.conv1.activate.sigmoid.output_quantizers': ModuleList(\n",
       "   (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       " ),\n",
       " 'backbone.stage1.1.blocks.0.conv1.activate.sigmoid.output_quantizers.0': QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False),\n",
       " 'backbone.stage1.1.blocks.0.conv1.activate.mul': QuantizedMultiply(\n",
       "   (param_quantizers): ModuleDict()\n",
       "   (input_quantizers): ModuleList(\n",
       "     (0-1): 2 x None\n",
       "   )\n",
       "   (output_quantizers): ModuleList(\n",
       "     (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "   )\n",
       " ),\n",
       " 'backbone.stage1.1.blocks.0.conv1.activate.mul.param_quantizers': ModuleDict(),\n",
       " 'backbone.stage1.1.blocks.0.conv1.activate.mul.input_quantizers': ModuleList(\n",
       "   (0-1): 2 x None\n",
       " ),\n",
       " 'backbone.stage1.1.blocks.0.conv1.activate.mul.output_quantizers': ModuleList(\n",
       "   (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       " ),\n",
       " 'backbone.stage1.1.blocks.0.conv1.activate.mul.output_quantizers.0': QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False),\n",
       " 'backbone.stage1.1.blocks.0.conv2': Module(\n",
       "   (depthwise_conv): Module(\n",
       "     (conv): QuantizedConv2d(\n",
       "       24, 24, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=24, bias=False\n",
       "       (param_quantizers): ModuleDict(\n",
       "         (weight): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=True)\n",
       "       )\n",
       "       (input_quantizers): ModuleList(\n",
       "         (0): None\n",
       "       )\n",
       "       (output_quantizers): ModuleList(\n",
       "         (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "       )\n",
       "     )\n",
       "     (bn): Module(\n",
       "       (module_batch_norm_7): BatchNorm()\n",
       "     )\n",
       "     (activate): CustomSiLU(\n",
       "       (sigmoid): QuantizedSigmoid(\n",
       "         (param_quantizers): ModuleDict()\n",
       "         (input_quantizers): ModuleList(\n",
       "           (0): None\n",
       "         )\n",
       "         (output_quantizers): ModuleList(\n",
       "           (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "         )\n",
       "       )\n",
       "       (mul): QuantizedMultiply(\n",
       "         (param_quantizers): ModuleDict()\n",
       "         (input_quantizers): ModuleList(\n",
       "           (0-1): 2 x None\n",
       "         )\n",
       "         (output_quantizers): ModuleList(\n",
       "           (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "         )\n",
       "       )\n",
       "     )\n",
       "   )\n",
       "   (pointwise_conv): Module(\n",
       "     (conv): QuantizedConv2d(\n",
       "       24, 24, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "       (param_quantizers): ModuleDict(\n",
       "         (weight): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=True)\n",
       "       )\n",
       "       (input_quantizers): ModuleList(\n",
       "         (0): None\n",
       "       )\n",
       "       (output_quantizers): ModuleList(\n",
       "         (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "       )\n",
       "     )\n",
       "     (bn): Module(\n",
       "       (module_batch_norm_8): FakeQuantizedBatchNorm(\n",
       "         (param_quantizers): ModuleDict()\n",
       "         (input_quantizers): ModuleList(\n",
       "           (0-4): 5 x QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "         )\n",
       "         (output_quantizers): ModuleList(\n",
       "           (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "         )\n",
       "       )\n",
       "     )\n",
       "     (activate): CustomSiLU(\n",
       "       (sigmoid): QuantizedSigmoid(\n",
       "         (param_quantizers): ModuleDict()\n",
       "         (input_quantizers): ModuleList(\n",
       "           (0): None\n",
       "         )\n",
       "         (output_quantizers): ModuleList(\n",
       "           (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "         )\n",
       "       )\n",
       "       (mul): QuantizedMultiply(\n",
       "         (param_quantizers): ModuleDict()\n",
       "         (input_quantizers): ModuleList(\n",
       "           (0-1): 2 x None\n",
       "         )\n",
       "         (output_quantizers): ModuleList(\n",
       "           (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "         )\n",
       "       )\n",
       "     )\n",
       "   )\n",
       " ),\n",
       " 'backbone.stage1.1.blocks.0.conv2.depthwise_conv': Module(\n",
       "   (conv): QuantizedConv2d(\n",
       "     24, 24, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=24, bias=False\n",
       "     (param_quantizers): ModuleDict(\n",
       "       (weight): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=True)\n",
       "     )\n",
       "     (input_quantizers): ModuleList(\n",
       "       (0): None\n",
       "     )\n",
       "     (output_quantizers): ModuleList(\n",
       "       (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "     )\n",
       "   )\n",
       "   (bn): Module(\n",
       "     (module_batch_norm_7): BatchNorm()\n",
       "   )\n",
       "   (activate): CustomSiLU(\n",
       "     (sigmoid): QuantizedSigmoid(\n",
       "       (param_quantizers): ModuleDict()\n",
       "       (input_quantizers): ModuleList(\n",
       "         (0): None\n",
       "       )\n",
       "       (output_quantizers): ModuleList(\n",
       "         (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "       )\n",
       "     )\n",
       "     (mul): QuantizedMultiply(\n",
       "       (param_quantizers): ModuleDict()\n",
       "       (input_quantizers): ModuleList(\n",
       "         (0-1): 2 x None\n",
       "       )\n",
       "       (output_quantizers): ModuleList(\n",
       "         (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "       )\n",
       "     )\n",
       "   )\n",
       " ),\n",
       " 'backbone.stage1.1.blocks.0.conv2.depthwise_conv.conv': QuantizedConv2d(\n",
       "   24, 24, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=24, bias=False\n",
       "   (param_quantizers): ModuleDict(\n",
       "     (weight): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=True)\n",
       "   )\n",
       "   (input_quantizers): ModuleList(\n",
       "     (0): None\n",
       "   )\n",
       "   (output_quantizers): ModuleList(\n",
       "     (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "   )\n",
       " ),\n",
       " 'backbone.stage1.1.blocks.0.conv2.depthwise_conv.conv.param_quantizers': ModuleDict(\n",
       "   (weight): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=True)\n",
       " ),\n",
       " 'backbone.stage1.1.blocks.0.conv2.depthwise_conv.conv.param_quantizers.weight': QuantizeDequantize(shape=[1], bitwidth=8, symmetric=True),\n",
       " 'backbone.stage1.1.blocks.0.conv2.depthwise_conv.conv.input_quantizers': ModuleList(\n",
       "   (0): None\n",
       " ),\n",
       " 'backbone.stage1.1.blocks.0.conv2.depthwise_conv.conv.output_quantizers': ModuleList(\n",
       "   (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       " ),\n",
       " 'backbone.stage1.1.blocks.0.conv2.depthwise_conv.conv.output_quantizers.0': QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False),\n",
       " 'backbone.stage1.1.blocks.0.conv2.depthwise_conv.bn': Module(\n",
       "   (module_batch_norm_7): BatchNorm()\n",
       " ),\n",
       " 'backbone.stage1.1.blocks.0.conv2.depthwise_conv.bn.module_batch_norm_7': BatchNorm(),\n",
       " 'backbone.stage1.1.blocks.0.conv2.depthwise_conv.activate': CustomSiLU(\n",
       "   (sigmoid): QuantizedSigmoid(\n",
       "     (param_quantizers): ModuleDict()\n",
       "     (input_quantizers): ModuleList(\n",
       "       (0): None\n",
       "     )\n",
       "     (output_quantizers): ModuleList(\n",
       "       (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "     )\n",
       "   )\n",
       "   (mul): QuantizedMultiply(\n",
       "     (param_quantizers): ModuleDict()\n",
       "     (input_quantizers): ModuleList(\n",
       "       (0-1): 2 x None\n",
       "     )\n",
       "     (output_quantizers): ModuleList(\n",
       "       (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "     )\n",
       "   )\n",
       " ),\n",
       " 'backbone.stage1.1.blocks.0.conv2.depthwise_conv.activate.sigmoid': QuantizedSigmoid(\n",
       "   (param_quantizers): ModuleDict()\n",
       "   (input_quantizers): ModuleList(\n",
       "     (0): None\n",
       "   )\n",
       "   (output_quantizers): ModuleList(\n",
       "     (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "   )\n",
       " ),\n",
       " 'backbone.stage1.1.blocks.0.conv2.depthwise_conv.activate.sigmoid.param_quantizers': ModuleDict(),\n",
       " 'backbone.stage1.1.blocks.0.conv2.depthwise_conv.activate.sigmoid.input_quantizers': ModuleList(\n",
       "   (0): None\n",
       " ),\n",
       " 'backbone.stage1.1.blocks.0.conv2.depthwise_conv.activate.sigmoid.output_quantizers': ModuleList(\n",
       "   (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       " ),\n",
       " 'backbone.stage1.1.blocks.0.conv2.depthwise_conv.activate.sigmoid.output_quantizers.0': QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False),\n",
       " 'backbone.stage1.1.blocks.0.conv2.depthwise_conv.activate.mul': QuantizedMultiply(\n",
       "   (param_quantizers): ModuleDict()\n",
       "   (input_quantizers): ModuleList(\n",
       "     (0-1): 2 x None\n",
       "   )\n",
       "   (output_quantizers): ModuleList(\n",
       "     (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "   )\n",
       " ),\n",
       " 'backbone.stage1.1.blocks.0.conv2.depthwise_conv.activate.mul.param_quantizers': ModuleDict(),\n",
       " 'backbone.stage1.1.blocks.0.conv2.depthwise_conv.activate.mul.input_quantizers': ModuleList(\n",
       "   (0-1): 2 x None\n",
       " ),\n",
       " 'backbone.stage1.1.blocks.0.conv2.depthwise_conv.activate.mul.output_quantizers': ModuleList(\n",
       "   (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       " ),\n",
       " 'backbone.stage1.1.blocks.0.conv2.depthwise_conv.activate.mul.output_quantizers.0': QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False),\n",
       " 'backbone.stage1.1.blocks.0.conv2.pointwise_conv': Module(\n",
       "   (conv): QuantizedConv2d(\n",
       "     24, 24, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "     (param_quantizers): ModuleDict(\n",
       "       (weight): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=True)\n",
       "     )\n",
       "     (input_quantizers): ModuleList(\n",
       "       (0): None\n",
       "     )\n",
       "     (output_quantizers): ModuleList(\n",
       "       (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "     )\n",
       "   )\n",
       "   (bn): Module(\n",
       "     (module_batch_norm_8): FakeQuantizedBatchNorm(\n",
       "       (param_quantizers): ModuleDict()\n",
       "       (input_quantizers): ModuleList(\n",
       "         (0-4): 5 x QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "       )\n",
       "       (output_quantizers): ModuleList(\n",
       "         (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "       )\n",
       "     )\n",
       "   )\n",
       "   (activate): CustomSiLU(\n",
       "     (sigmoid): QuantizedSigmoid(\n",
       "       (param_quantizers): ModuleDict()\n",
       "       (input_quantizers): ModuleList(\n",
       "         (0): None\n",
       "       )\n",
       "       (output_quantizers): ModuleList(\n",
       "         (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "       )\n",
       "     )\n",
       "     (mul): QuantizedMultiply(\n",
       "       (param_quantizers): ModuleDict()\n",
       "       (input_quantizers): ModuleList(\n",
       "         (0-1): 2 x None\n",
       "       )\n",
       "       (output_quantizers): ModuleList(\n",
       "         (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "       )\n",
       "     )\n",
       "   )\n",
       " ),\n",
       " 'backbone.stage1.1.blocks.0.conv2.pointwise_conv.conv': QuantizedConv2d(\n",
       "   24, 24, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "   (param_quantizers): ModuleDict(\n",
       "     (weight): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=True)\n",
       "   )\n",
       "   (input_quantizers): ModuleList(\n",
       "     (0): None\n",
       "   )\n",
       "   (output_quantizers): ModuleList(\n",
       "     (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "   )\n",
       " ),\n",
       " 'backbone.stage1.1.blocks.0.conv2.pointwise_conv.conv.param_quantizers': ModuleDict(\n",
       "   (weight): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=True)\n",
       " ),\n",
       " 'backbone.stage1.1.blocks.0.conv2.pointwise_conv.conv.param_quantizers.weight': QuantizeDequantize(shape=[1], bitwidth=8, symmetric=True),\n",
       " 'backbone.stage1.1.blocks.0.conv2.pointwise_conv.conv.input_quantizers': ModuleList(\n",
       "   (0): None\n",
       " ),\n",
       " 'backbone.stage1.1.blocks.0.conv2.pointwise_conv.conv.output_quantizers': ModuleList(\n",
       "   (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       " ),\n",
       " 'backbone.stage1.1.blocks.0.conv2.pointwise_conv.conv.output_quantizers.0': QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False),\n",
       " 'backbone.stage1.1.blocks.0.conv2.pointwise_conv.bn': Module(\n",
       "   (module_batch_norm_8): FakeQuantizedBatchNorm(\n",
       "     (param_quantizers): ModuleDict()\n",
       "     (input_quantizers): ModuleList(\n",
       "       (0-4): 5 x QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "     )\n",
       "     (output_quantizers): ModuleList(\n",
       "       (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "     )\n",
       "   )\n",
       " ),\n",
       " 'backbone.stage1.1.blocks.0.conv2.pointwise_conv.bn.module_batch_norm_8': FakeQuantizedBatchNorm(\n",
       "   (param_quantizers): ModuleDict()\n",
       "   (input_quantizers): ModuleList(\n",
       "     (0-4): 5 x QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "   )\n",
       "   (output_quantizers): ModuleList(\n",
       "     (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "   )\n",
       " ),\n",
       " 'backbone.stage1.1.blocks.0.conv2.pointwise_conv.bn.module_batch_norm_8.param_quantizers': ModuleDict(),\n",
       " 'backbone.stage1.1.blocks.0.conv2.pointwise_conv.bn.module_batch_norm_8.input_quantizers': ModuleList(\n",
       "   (0-4): 5 x QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       " ),\n",
       " 'backbone.stage1.1.blocks.0.conv2.pointwise_conv.bn.module_batch_norm_8.input_quantizers.0': QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False),\n",
       " 'backbone.stage1.1.blocks.0.conv2.pointwise_conv.bn.module_batch_norm_8.input_quantizers.1': QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False),\n",
       " 'backbone.stage1.1.blocks.0.conv2.pointwise_conv.bn.module_batch_norm_8.input_quantizers.2': QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False),\n",
       " 'backbone.stage1.1.blocks.0.conv2.pointwise_conv.bn.module_batch_norm_8.input_quantizers.3': QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False),\n",
       " 'backbone.stage1.1.blocks.0.conv2.pointwise_conv.bn.module_batch_norm_8.input_quantizers.4': QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False),\n",
       " 'backbone.stage1.1.blocks.0.conv2.pointwise_conv.bn.module_batch_norm_8.output_quantizers': ModuleList(\n",
       "   (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       " ),\n",
       " 'backbone.stage1.1.blocks.0.conv2.pointwise_conv.bn.module_batch_norm_8.output_quantizers.0': QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False),\n",
       " 'backbone.stage1.1.blocks.0.conv2.pointwise_conv.activate': CustomSiLU(\n",
       "   (sigmoid): QuantizedSigmoid(\n",
       "     (param_quantizers): ModuleDict()\n",
       "     (input_quantizers): ModuleList(\n",
       "       (0): None\n",
       "     )\n",
       "     (output_quantizers): ModuleList(\n",
       "       (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "     )\n",
       "   )\n",
       "   (mul): QuantizedMultiply(\n",
       "     (param_quantizers): ModuleDict()\n",
       "     (input_quantizers): ModuleList(\n",
       "       (0-1): 2 x None\n",
       "     )\n",
       "     (output_quantizers): ModuleList(\n",
       "       (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "     )\n",
       "   )\n",
       " ),\n",
       " 'backbone.stage1.1.blocks.0.conv2.pointwise_conv.activate.sigmoid': QuantizedSigmoid(\n",
       "   (param_quantizers): ModuleDict()\n",
       "   (input_quantizers): ModuleList(\n",
       "     (0): None\n",
       "   )\n",
       "   (output_quantizers): ModuleList(\n",
       "     (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "   )\n",
       " ),\n",
       " 'backbone.stage1.1.blocks.0.conv2.pointwise_conv.activate.sigmoid.param_quantizers': ModuleDict(),\n",
       " 'backbone.stage1.1.blocks.0.conv2.pointwise_conv.activate.sigmoid.input_quantizers': ModuleList(\n",
       "   (0): None\n",
       " ),\n",
       " 'backbone.stage1.1.blocks.0.conv2.pointwise_conv.activate.sigmoid.output_quantizers': ModuleList(\n",
       "   (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       " ),\n",
       " 'backbone.stage1.1.blocks.0.conv2.pointwise_conv.activate.sigmoid.output_quantizers.0': QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False),\n",
       " 'backbone.stage1.1.blocks.0.conv2.pointwise_conv.activate.mul': QuantizedMultiply(\n",
       "   (param_quantizers): ModuleDict()\n",
       "   (input_quantizers): ModuleList(\n",
       "     (0-1): 2 x None\n",
       "   )\n",
       "   (output_quantizers): ModuleList(\n",
       "     (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "   )\n",
       " ),\n",
       " 'backbone.stage1.1.blocks.0.conv2.pointwise_conv.activate.mul.param_quantizers': ModuleDict(),\n",
       " 'backbone.stage1.1.blocks.0.conv2.pointwise_conv.activate.mul.input_quantizers': ModuleList(\n",
       "   (0-1): 2 x None\n",
       " ),\n",
       " 'backbone.stage1.1.blocks.0.conv2.pointwise_conv.activate.mul.output_quantizers': ModuleList(\n",
       "   (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       " ),\n",
       " 'backbone.stage1.1.blocks.0.conv2.pointwise_conv.activate.mul.output_quantizers.0': QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False),\n",
       " 'backbone.stage1.1.blocks.0.module_add': QuantizedAdd(\n",
       "   (param_quantizers): ModuleDict()\n",
       "   (input_quantizers): ModuleList(\n",
       "     (0-1): 2 x None\n",
       "   )\n",
       "   (output_quantizers): ModuleList(\n",
       "     (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "   )\n",
       " ),\n",
       " 'backbone.stage1.1.blocks.0.module_add.param_quantizers': ModuleDict(),\n",
       " 'backbone.stage1.1.blocks.0.module_add.input_quantizers': ModuleList(\n",
       "   (0-1): 2 x None\n",
       " ),\n",
       " 'backbone.stage1.1.blocks.0.module_add.output_quantizers': ModuleList(\n",
       "   (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       " ),\n",
       " 'backbone.stage1.1.blocks.0.module_add.output_quantizers.0': QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False),\n",
       " 'backbone.stage1.1.attention': Module(\n",
       "   (global_avgpool): FakeQuantizedAdaptiveAvgPool2d(\n",
       "     output_size=1\n",
       "     (param_quantizers): ModuleDict()\n",
       "     (input_quantizers): ModuleList(\n",
       "       (0): None\n",
       "     )\n",
       "     (output_quantizers): ModuleList(\n",
       "       (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "     )\n",
       "   )\n",
       "   (fc): QuantizedConv2d(\n",
       "     48, 48, kernel_size=(1, 1), stride=(1, 1)\n",
       "     (param_quantizers): ModuleDict(\n",
       "       (weight): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=True)\n",
       "       (bias): None\n",
       "     )\n",
       "     (input_quantizers): ModuleList(\n",
       "       (0): None\n",
       "     )\n",
       "     (output_quantizers): ModuleList(\n",
       "       (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "     )\n",
       "   )\n",
       "   (act): FakeQuantizedHardsigmoid(\n",
       "     (param_quantizers): ModuleDict()\n",
       "     (input_quantizers): ModuleList(\n",
       "       (0): None\n",
       "     )\n",
       "     (output_quantizers): ModuleList(\n",
       "       (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "     )\n",
       "   )\n",
       "   (module_mul): QuantizedMultiply(\n",
       "     (param_quantizers): ModuleDict()\n",
       "     (input_quantizers): ModuleList(\n",
       "       (0-1): 2 x None\n",
       "     )\n",
       "     (output_quantizers): ModuleList(\n",
       "       (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "     )\n",
       "   )\n",
       " ),\n",
       " 'backbone.stage1.1.attention.global_avgpool': FakeQuantizedAdaptiveAvgPool2d(\n",
       "   output_size=1\n",
       "   (param_quantizers): ModuleDict()\n",
       "   (input_quantizers): ModuleList(\n",
       "     (0): None\n",
       "   )\n",
       "   (output_quantizers): ModuleList(\n",
       "     (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "   )\n",
       " ),\n",
       " 'backbone.stage1.1.attention.global_avgpool.param_quantizers': ModuleDict(),\n",
       " 'backbone.stage1.1.attention.global_avgpool.input_quantizers': ModuleList(\n",
       "   (0): None\n",
       " ),\n",
       " 'backbone.stage1.1.attention.global_avgpool.output_quantizers': ModuleList(\n",
       "   (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       " ),\n",
       " 'backbone.stage1.1.attention.global_avgpool.output_quantizers.0': QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False),\n",
       " 'backbone.stage1.1.attention.fc': QuantizedConv2d(\n",
       "   48, 48, kernel_size=(1, 1), stride=(1, 1)\n",
       "   (param_quantizers): ModuleDict(\n",
       "     (weight): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=True)\n",
       "     (bias): None\n",
       "   )\n",
       "   (input_quantizers): ModuleList(\n",
       "     (0): None\n",
       "   )\n",
       "   (output_quantizers): ModuleList(\n",
       "     (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "   )\n",
       " ),\n",
       " 'backbone.stage1.1.attention.fc.param_quantizers': ModuleDict(\n",
       "   (weight): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=True)\n",
       "   (bias): None\n",
       " ),\n",
       " 'backbone.stage1.1.attention.fc.param_quantizers.weight': QuantizeDequantize(shape=[1], bitwidth=8, symmetric=True),\n",
       " 'backbone.stage1.1.attention.fc.input_quantizers': ModuleList(\n",
       "   (0): None\n",
       " ),\n",
       " 'backbone.stage1.1.attention.fc.output_quantizers': ModuleList(\n",
       "   (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       " ),\n",
       " 'backbone.stage1.1.attention.fc.output_quantizers.0': QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False),\n",
       " 'backbone.stage1.1.attention.act': FakeQuantizedHardsigmoid(\n",
       "   (param_quantizers): ModuleDict()\n",
       "   (input_quantizers): ModuleList(\n",
       "     (0): None\n",
       "   )\n",
       "   (output_quantizers): ModuleList(\n",
       "     (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "   )\n",
       " ),\n",
       " 'backbone.stage1.1.attention.act.param_quantizers': ModuleDict(),\n",
       " 'backbone.stage1.1.attention.act.input_quantizers': ModuleList(\n",
       "   (0): None\n",
       " ),\n",
       " 'backbone.stage1.1.attention.act.output_quantizers': ModuleList(\n",
       "   (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       " ),\n",
       " 'backbone.stage1.1.attention.act.output_quantizers.0': QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False),\n",
       " 'backbone.stage1.1.attention.module_mul': QuantizedMultiply(\n",
       "   (param_quantizers): ModuleDict()\n",
       "   (input_quantizers): ModuleList(\n",
       "     (0-1): 2 x None\n",
       "   )\n",
       "   (output_quantizers): ModuleList(\n",
       "     (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "   )\n",
       " ),\n",
       " 'backbone.stage1.1.attention.module_mul.param_quantizers': ModuleDict(),\n",
       " 'backbone.stage1.1.attention.module_mul.input_quantizers': ModuleList(\n",
       "   (0-1): 2 x None\n",
       " ),\n",
       " 'backbone.stage1.1.attention.module_mul.output_quantizers': ModuleList(\n",
       "   (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       " ),\n",
       " 'backbone.stage1.1.attention.module_mul.output_quantizers.0': QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False),\n",
       " 'backbone.stage1.1.final_conv': Module(\n",
       "   (conv): QuantizedConv2d(\n",
       "     48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "     (param_quantizers): ModuleDict(\n",
       "       (weight): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=True)\n",
       "     )\n",
       "     (input_quantizers): ModuleList(\n",
       "       (0): None\n",
       "     )\n",
       "     (output_quantizers): ModuleList(\n",
       "       (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "     )\n",
       "   )\n",
       "   (bn): Module(\n",
       "     (module_batch_norm_9): FakeQuantizedBatchNorm(\n",
       "       (param_quantizers): ModuleDict()\n",
       "       (input_quantizers): ModuleList(\n",
       "         (0-4): 5 x QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "       )\n",
       "       (output_quantizers): ModuleList(\n",
       "         (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "       )\n",
       "     )\n",
       "   )\n",
       "   (activate): CustomSiLU(\n",
       "     (sigmoid): QuantizedSigmoid(\n",
       "       (param_quantizers): ModuleDict()\n",
       "       (input_quantizers): ModuleList(\n",
       "         (0): None\n",
       "       )\n",
       "       (output_quantizers): ModuleList(\n",
       "         (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "       )\n",
       "     )\n",
       "     (mul): QuantizedMultiply(\n",
       "       (param_quantizers): ModuleDict()\n",
       "       (input_quantizers): ModuleList(\n",
       "         (0-1): 2 x None\n",
       "       )\n",
       "       (output_quantizers): ModuleList(\n",
       "         (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "       )\n",
       "     )\n",
       "   )\n",
       " ),\n",
       " 'backbone.stage1.1.final_conv.conv': QuantizedConv2d(\n",
       "   48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "   (param_quantizers): ModuleDict(\n",
       "     (weight): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=True)\n",
       "   )\n",
       "   (input_quantizers): ModuleList(\n",
       "     (0): None\n",
       "   )\n",
       "   (output_quantizers): ModuleList(\n",
       "     (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "   )\n",
       " ),\n",
       " 'backbone.stage1.1.final_conv.conv.param_quantizers': ModuleDict(\n",
       "   (weight): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=True)\n",
       " ),\n",
       " 'backbone.stage1.1.final_conv.conv.param_quantizers.weight': QuantizeDequantize(shape=[1], bitwidth=8, symmetric=True),\n",
       " 'backbone.stage1.1.final_conv.conv.input_quantizers': ModuleList(\n",
       "   (0): None\n",
       " ),\n",
       " 'backbone.stage1.1.final_conv.conv.output_quantizers': ModuleList(\n",
       "   (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       " ),\n",
       " 'backbone.stage1.1.final_conv.conv.output_quantizers.0': QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False),\n",
       " 'backbone.stage1.1.final_conv.bn': Module(\n",
       "   (module_batch_norm_9): FakeQuantizedBatchNorm(\n",
       "     (param_quantizers): ModuleDict()\n",
       "     (input_quantizers): ModuleList(\n",
       "       (0-4): 5 x QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "     )\n",
       "     (output_quantizers): ModuleList(\n",
       "       (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "     )\n",
       "   )\n",
       " ),\n",
       " 'backbone.stage1.1.final_conv.bn.module_batch_norm_9': FakeQuantizedBatchNorm(\n",
       "   (param_quantizers): ModuleDict()\n",
       "   (input_quantizers): ModuleList(\n",
       "     (0-4): 5 x QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "   )\n",
       "   (output_quantizers): ModuleList(\n",
       "     (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "   )\n",
       " ),\n",
       " 'backbone.stage1.1.final_conv.bn.module_batch_norm_9.param_quantizers': ModuleDict(),\n",
       " 'backbone.stage1.1.final_conv.bn.module_batch_norm_9.input_quantizers': ModuleList(\n",
       "   (0-4): 5 x QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       " ),\n",
       " 'backbone.stage1.1.final_conv.bn.module_batch_norm_9.input_quantizers.0': QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False),\n",
       " 'backbone.stage1.1.final_conv.bn.module_batch_norm_9.input_quantizers.1': QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False),\n",
       " 'backbone.stage1.1.final_conv.bn.module_batch_norm_9.input_quantizers.2': QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False),\n",
       " 'backbone.stage1.1.final_conv.bn.module_batch_norm_9.input_quantizers.3': QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False),\n",
       " 'backbone.stage1.1.final_conv.bn.module_batch_norm_9.input_quantizers.4': QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False),\n",
       " 'backbone.stage1.1.final_conv.bn.module_batch_norm_9.output_quantizers': ModuleList(\n",
       "   (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       " ),\n",
       " 'backbone.stage1.1.final_conv.bn.module_batch_norm_9.output_quantizers.0': QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False),\n",
       " 'backbone.stage1.1.final_conv.activate': CustomSiLU(\n",
       "   (sigmoid): QuantizedSigmoid(\n",
       "     (param_quantizers): ModuleDict()\n",
       "     (input_quantizers): ModuleList(\n",
       "       (0): None\n",
       "     )\n",
       "     (output_quantizers): ModuleList(\n",
       "       (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "     )\n",
       "   )\n",
       "   (mul): QuantizedMultiply(\n",
       "     (param_quantizers): ModuleDict()\n",
       "     (input_quantizers): ModuleList(\n",
       "       (0-1): 2 x None\n",
       "     )\n",
       "     (output_quantizers): ModuleList(\n",
       "       (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "     )\n",
       "   )\n",
       " ),\n",
       " 'backbone.stage1.1.final_conv.activate.sigmoid': QuantizedSigmoid(\n",
       "   (param_quantizers): ModuleDict()\n",
       "   (input_quantizers): ModuleList(\n",
       "     (0): None\n",
       "   )\n",
       "   (output_quantizers): ModuleList(\n",
       "     (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "   )\n",
       " ),\n",
       " 'backbone.stage1.1.final_conv.activate.sigmoid.param_quantizers': ModuleDict(),\n",
       " 'backbone.stage1.1.final_conv.activate.sigmoid.input_quantizers': ModuleList(\n",
       "   (0): None\n",
       " ),\n",
       " 'backbone.stage1.1.final_conv.activate.sigmoid.output_quantizers': ModuleList(\n",
       "   (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       " ),\n",
       " 'backbone.stage1.1.final_conv.activate.sigmoid.output_quantizers.0': QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False),\n",
       " 'backbone.stage1.1.final_conv.activate.mul': QuantizedMultiply(\n",
       "   (param_quantizers): ModuleDict()\n",
       "   (input_quantizers): ModuleList(\n",
       "     (0-1): 2 x None\n",
       "   )\n",
       "   (output_quantizers): ModuleList(\n",
       "     (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "   )\n",
       " ),\n",
       " 'backbone.stage1.1.final_conv.activate.mul.param_quantizers': ModuleDict(),\n",
       " 'backbone.stage1.1.final_conv.activate.mul.input_quantizers': ModuleList(\n",
       "   (0-1): 2 x None\n",
       " ),\n",
       " 'backbone.stage1.1.final_conv.activate.mul.output_quantizers': ModuleList(\n",
       "   (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       " ),\n",
       " 'backbone.stage1.1.final_conv.activate.mul.output_quantizers.0': QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False),\n",
       " 'backbone.stage1.1.module_cat': FakeQuantizedConcat(\n",
       "   (param_quantizers): ModuleDict()\n",
       "   (input_quantizers): ModuleList(\n",
       "     (0): None\n",
       "   )\n",
       "   (output_quantizers): ModuleList(\n",
       "     (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "   )\n",
       " ),\n",
       " 'backbone.stage1.1.module_cat.param_quantizers': ModuleDict(),\n",
       " 'backbone.stage1.1.module_cat.input_quantizers': ModuleList(\n",
       "   (0): None\n",
       " ),\n",
       " 'backbone.stage1.1.module_cat.output_quantizers': ModuleList(\n",
       "   (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       " ),\n",
       " 'backbone.stage1.1.module_cat.output_quantizers.0': QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False),\n",
       " 'backbone.stage2': Module(\n",
       "   (0): Module(\n",
       "     (conv): QuantizedConv2d(\n",
       "       48, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False\n",
       "       (param_quantizers): ModuleDict(\n",
       "         (weight): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=True)\n",
       "       )\n",
       "       (input_quantizers): ModuleList(\n",
       "         (0): None\n",
       "       )\n",
       "       (output_quantizers): ModuleList(\n",
       "         (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "       )\n",
       "     )\n",
       "     (bn): Module(\n",
       "       (module_batch_norm_10): FakeQuantizedBatchNorm(\n",
       "         (param_quantizers): ModuleDict()\n",
       "         (input_quantizers): ModuleList(\n",
       "           (0-4): 5 x QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "         )\n",
       "         (output_quantizers): ModuleList(\n",
       "           (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "         )\n",
       "       )\n",
       "     )\n",
       "     (activate): CustomSiLU(\n",
       "       (sigmoid): QuantizedSigmoid(\n",
       "         (param_quantizers): ModuleDict()\n",
       "         (input_quantizers): ModuleList(\n",
       "           (0): None\n",
       "         )\n",
       "         (output_quantizers): ModuleList(\n",
       "           (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "         )\n",
       "       )\n",
       "       (mul): QuantizedMultiply(\n",
       "         (param_quantizers): ModuleDict()\n",
       "         (input_quantizers): ModuleList(\n",
       "           (0-1): 2 x None\n",
       "         )\n",
       "         (output_quantizers): ModuleList(\n",
       "           (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "         )\n",
       "       )\n",
       "     )\n",
       "   )\n",
       "   (1): Module(\n",
       "     (short_conv): Module(\n",
       "       (conv): QuantizedConv2d(\n",
       "         96, 48, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "         (param_quantizers): ModuleDict(\n",
       "           (weight): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=True)\n",
       "         )\n",
       "         (input_quantizers): ModuleList(\n",
       "           (0): None\n",
       "         )\n",
       "         (output_quantizers): ModuleList(\n",
       "           (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "         )\n",
       "       )\n",
       "       (bn): Module(\n",
       "         (module_batch_norm_11): FakeQuantizedBatchNorm(\n",
       "           (param_quantizers): ModuleDict()\n",
       "           (input_quantizers): ModuleList(\n",
       "             (0-4): 5 x QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "           )\n",
       "           (output_quantizers): ModuleList(\n",
       "             (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "           )\n",
       "         )\n",
       "       )\n",
       "       (activate): CustomSiLU(\n",
       "         (sigmoid): QuantizedSigmoid(\n",
       "           (param_quantizers): ModuleDict()\n",
       "           (input_quantizers): ModuleList(\n",
       "             (0): None\n",
       "           )\n",
       "           (output_quantizers): ModuleList(\n",
       "             (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "           )\n",
       "         )\n",
       "         (mul): QuantizedMultiply(\n",
       "           (param_quantizers): ModuleDict()\n",
       "           (input_quantizers): ModuleList(\n",
       "             (0-1): 2 x None\n",
       "           )\n",
       "           (output_quantizers): ModuleList(\n",
       "             (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "           )\n",
       "         )\n",
       "       )\n",
       "     )\n",
       "     (main_conv): Module(\n",
       "       (conv): QuantizedConv2d(\n",
       "         96, 48, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "         (param_quantizers): ModuleDict(\n",
       "           (weight): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=True)\n",
       "         )\n",
       "         (input_quantizers): ModuleList(\n",
       "           (0): None\n",
       "         )\n",
       "         (output_quantizers): ModuleList(\n",
       "           (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "         )\n",
       "       )\n",
       "       (bn): Module(\n",
       "         (module_batch_norm_12): FakeQuantizedBatchNorm(\n",
       "           (param_quantizers): ModuleDict()\n",
       "           (input_quantizers): ModuleList(\n",
       "             (0-4): 5 x QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "           )\n",
       "           (output_quantizers): ModuleList(\n",
       "             (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "           )\n",
       "         )\n",
       "       )\n",
       "       (activate): CustomSiLU(\n",
       "         (sigmoid): QuantizedSigmoid(\n",
       "           (param_quantizers): ModuleDict()\n",
       "           (input_quantizers): ModuleList(\n",
       "             (0): None\n",
       "           )\n",
       "           (output_quantizers): ModuleList(\n",
       "             (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "           )\n",
       "         )\n",
       "         (mul): QuantizedMultiply(\n",
       "           (param_quantizers): ModuleDict()\n",
       "           (input_quantizers): ModuleList(\n",
       "             (0-1): 2 x None\n",
       "           )\n",
       "           (output_quantizers): ModuleList(\n",
       "             (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "           )\n",
       "         )\n",
       "       )\n",
       "     )\n",
       "     (blocks): Module(\n",
       "       (0): Module(\n",
       "         (conv1): Module(\n",
       "           (conv): QuantizedConv2d(\n",
       "             48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "             (param_quantizers): ModuleDict(\n",
       "               (weight): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=True)\n",
       "             )\n",
       "             (input_quantizers): ModuleList(\n",
       "               (0): None\n",
       "             )\n",
       "             (output_quantizers): ModuleList(\n",
       "               (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "             )\n",
       "           )\n",
       "           (bn): Module(\n",
       "             (module_batch_norm_13): FakeQuantizedBatchNorm(\n",
       "               (param_quantizers): ModuleDict()\n",
       "               (input_quantizers): ModuleList(\n",
       "                 (0-4): 5 x QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "               )\n",
       "               (output_quantizers): ModuleList(\n",
       "                 (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "               )\n",
       "             )\n",
       "           )\n",
       "           (activate): CustomSiLU(\n",
       "             (sigmoid): QuantizedSigmoid(\n",
       "               (param_quantizers): ModuleDict()\n",
       "               (input_quantizers): ModuleList(\n",
       "                 (0): None\n",
       "               )\n",
       "               (output_quantizers): ModuleList(\n",
       "                 (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "               )\n",
       "             )\n",
       "             (mul): QuantizedMultiply(\n",
       "               (param_quantizers): ModuleDict()\n",
       "               (input_quantizers): ModuleList(\n",
       "                 (0-1): 2 x None\n",
       "               )\n",
       "               (output_quantizers): ModuleList(\n",
       "                 (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "               )\n",
       "             )\n",
       "           )\n",
       "         )\n",
       "         (conv2): Module(\n",
       "           (depthwise_conv): Module(\n",
       "             (conv): QuantizedConv2d(\n",
       "               48, 48, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=48, bias=False\n",
       "               (param_quantizers): ModuleDict(\n",
       "                 (weight): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=True)\n",
       "               )\n",
       "               (input_quantizers): ModuleList(\n",
       "                 (0): None\n",
       "               )\n",
       "               (output_quantizers): ModuleList(\n",
       "                 (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "               )\n",
       "             )\n",
       "             (bn): Module(\n",
       "               (module_batch_norm_14): BatchNorm()\n",
       "             )\n",
       "             (activate): CustomSiLU(\n",
       "               (sigmoid): QuantizedSigmoid(\n",
       "                 (param_quantizers): ModuleDict()\n",
       "                 (input_quantizers): ModuleList(\n",
       "                   (0): None\n",
       "                 )\n",
       "                 (output_quantizers): ModuleList(\n",
       "                   (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "                 )\n",
       "               )\n",
       "               (mul): QuantizedMultiply(\n",
       "                 (param_quantizers): ModuleDict()\n",
       "                 (input_quantizers): ModuleList(\n",
       "                   (0-1): 2 x None\n",
       "                 )\n",
       "                 (output_quantizers): ModuleList(\n",
       "                   (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "                 )\n",
       "               )\n",
       "             )\n",
       "           )\n",
       "           (pointwise_conv): Module(\n",
       "             (conv): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "             (bn): Module(\n",
       "               (module_batch_norm_15): FakeQuantizedBatchNorm(\n",
       "                 (param_quantizers): ModuleDict()\n",
       "                 (input_quantizers): ModuleList(\n",
       "                   (0-4): 5 x QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "                 )\n",
       "                 (output_quantizers): ModuleList(\n",
       "                   (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "                 )\n",
       "               )\n",
       "             )\n",
       "             (activate): CustomSiLU(\n",
       "               (sigmoid): QuantizedSigmoid(\n",
       "                 (param_quantizers): ModuleDict()\n",
       "                 (input_quantizers): ModuleList(\n",
       "                   (0): None\n",
       "                 )\n",
       "                 (output_quantizers): ModuleList(\n",
       "                   (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "                 )\n",
       "               )\n",
       "               (mul): QuantizedMultiply(\n",
       "                 (param_quantizers): ModuleDict()\n",
       "                 (input_quantizers): ModuleList(\n",
       "                   (0-1): 2 x None\n",
       "                 )\n",
       "                 (output_quantizers): ModuleList(\n",
       "                   (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "                 )\n",
       "               )\n",
       "             )\n",
       "           )\n",
       "         )\n",
       "         (module_add_1): QuantizedAdd(\n",
       "           (param_quantizers): ModuleDict()\n",
       "           (input_quantizers): ModuleList(\n",
       "             (0-1): 2 x None\n",
       "           )\n",
       "           (output_quantizers): ModuleList(\n",
       "             (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "           )\n",
       "         )\n",
       "       )\n",
       "     )\n",
       "     (attention): Module(\n",
       "       (global_avgpool): FakeQuantizedAdaptiveAvgPool2d(\n",
       "         output_size=1\n",
       "         (param_quantizers): ModuleDict()\n",
       "         (input_quantizers): ModuleList(\n",
       "           (0): None\n",
       "         )\n",
       "         (output_quantizers): ModuleList(\n",
       "           (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "         )\n",
       "       )\n",
       "       (fc): QuantizedConv2d(\n",
       "         96, 96, kernel_size=(1, 1), stride=(1, 1)\n",
       "         (param_quantizers): ModuleDict(\n",
       "           (weight): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=True)\n",
       "           (bias): None\n",
       "         )\n",
       "         (input_quantizers): ModuleList(\n",
       "           (0): None\n",
       "         )\n",
       "         (output_quantizers): ModuleList(\n",
       "           (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "         )\n",
       "       )\n",
       "       (act): FakeQuantizedHardsigmoid(\n",
       "         (param_quantizers): ModuleDict()\n",
       "         (input_quantizers): ModuleList(\n",
       "           (0): None\n",
       "         )\n",
       "         (output_quantizers): ModuleList(\n",
       "           (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "         )\n",
       "       )\n",
       "       (module_mul_1): QuantizedMultiply(\n",
       "         (param_quantizers): ModuleDict()\n",
       "         (input_quantizers): ModuleList(\n",
       "           (0-1): 2 x None\n",
       "         )\n",
       "         (output_quantizers): ModuleList(\n",
       "           (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "         )\n",
       "       )\n",
       "     )\n",
       "     (final_conv): Module(\n",
       "       (conv): QuantizedConv2d(\n",
       "         96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "         (param_quantizers): ModuleDict(\n",
       "           (weight): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=True)\n",
       "         )\n",
       "         (input_quantizers): ModuleList(\n",
       "           (0): None\n",
       "         )\n",
       "         (output_quantizers): ModuleList(\n",
       "           (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "         )\n",
       "       )\n",
       "       (bn): Module(\n",
       "         (module_batch_norm_16): FakeQuantizedBatchNorm(\n",
       "           (param_quantizers): ModuleDict()\n",
       "           (input_quantizers): ModuleList(\n",
       "             (0-4): 5 x QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "           )\n",
       "           (output_quantizers): ModuleList(\n",
       "             (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "           )\n",
       "         )\n",
       "       )\n",
       "       (activate): CustomSiLU(\n",
       "         (sigmoid): QuantizedSigmoid(\n",
       "           (param_quantizers): ModuleDict()\n",
       "           (input_quantizers): ModuleList(\n",
       "             (0): None\n",
       "           )\n",
       "           (output_quantizers): ModuleList(\n",
       "             (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "           )\n",
       "         )\n",
       "         (mul): QuantizedMultiply(\n",
       "           (param_quantizers): ModuleDict()\n",
       "           (input_quantizers): ModuleList(\n",
       "             (0-1): 2 x None\n",
       "           )\n",
       "           (output_quantizers): ModuleList(\n",
       "             (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "           )\n",
       "         )\n",
       "       )\n",
       "     )\n",
       "     (module_cat_1): FakeQuantizedConcat(\n",
       "       (param_quantizers): ModuleDict()\n",
       "       (input_quantizers): ModuleList(\n",
       "         (0): None\n",
       "       )\n",
       "       (output_quantizers): ModuleList(\n",
       "         (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "       )\n",
       "     )\n",
       "   )\n",
       " ),\n",
       " 'backbone.stage2.0': Module(\n",
       "   (conv): QuantizedConv2d(\n",
       "     48, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False\n",
       "     (param_quantizers): ModuleDict(\n",
       "       (weight): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=True)\n",
       "     )\n",
       "     (input_quantizers): ModuleList(\n",
       "       (0): None\n",
       "     )\n",
       "     (output_quantizers): ModuleList(\n",
       "       (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "     )\n",
       "   )\n",
       "   (bn): Module(\n",
       "     (module_batch_norm_10): FakeQuantizedBatchNorm(\n",
       "       (param_quantizers): ModuleDict()\n",
       "       (input_quantizers): ModuleList(\n",
       "         (0-4): 5 x QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "       )\n",
       "       (output_quantizers): ModuleList(\n",
       "         (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "       )\n",
       "     )\n",
       "   )\n",
       "   (activate): CustomSiLU(\n",
       "     (sigmoid): QuantizedSigmoid(\n",
       "       (param_quantizers): ModuleDict()\n",
       "       (input_quantizers): ModuleList(\n",
       "         (0): None\n",
       "       )\n",
       "       (output_quantizers): ModuleList(\n",
       "         (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "       )\n",
       "     )\n",
       "     (mul): QuantizedMultiply(\n",
       "       (param_quantizers): ModuleDict()\n",
       "       (input_quantizers): ModuleList(\n",
       "         (0-1): 2 x None\n",
       "       )\n",
       "       (output_quantizers): ModuleList(\n",
       "         (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "       )\n",
       "     )\n",
       "   )\n",
       " ),\n",
       " 'backbone.stage2.0.conv': QuantizedConv2d(\n",
       "   48, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False\n",
       "   (param_quantizers): ModuleDict(\n",
       "     (weight): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=True)\n",
       "   )\n",
       "   (input_quantizers): ModuleList(\n",
       "     (0): None\n",
       "   )\n",
       "   (output_quantizers): ModuleList(\n",
       "     (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "   )\n",
       " ),\n",
       " 'backbone.stage2.0.conv.param_quantizers': ModuleDict(\n",
       "   (weight): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=True)\n",
       " ),\n",
       " 'backbone.stage2.0.conv.param_quantizers.weight': QuantizeDequantize(shape=[1], bitwidth=8, symmetric=True),\n",
       " 'backbone.stage2.0.conv.input_quantizers': ModuleList(\n",
       "   (0): None\n",
       " ),\n",
       " 'backbone.stage2.0.conv.output_quantizers': ModuleList(\n",
       "   (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       " ),\n",
       " 'backbone.stage2.0.conv.output_quantizers.0': QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False),\n",
       " 'backbone.stage2.0.bn': Module(\n",
       "   (module_batch_norm_10): FakeQuantizedBatchNorm(\n",
       "     (param_quantizers): ModuleDict()\n",
       "     (input_quantizers): ModuleList(\n",
       "       (0-4): 5 x QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "     )\n",
       "     (output_quantizers): ModuleList(\n",
       "       (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "     )\n",
       "   )\n",
       " ),\n",
       " 'backbone.stage2.0.bn.module_batch_norm_10': FakeQuantizedBatchNorm(\n",
       "   (param_quantizers): ModuleDict()\n",
       "   (input_quantizers): ModuleList(\n",
       "     (0-4): 5 x QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "   )\n",
       "   (output_quantizers): ModuleList(\n",
       "     (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "   )\n",
       " ),\n",
       " 'backbone.stage2.0.bn.module_batch_norm_10.param_quantizers': ModuleDict(),\n",
       " 'backbone.stage2.0.bn.module_batch_norm_10.input_quantizers': ModuleList(\n",
       "   (0-4): 5 x QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       " ),\n",
       " 'backbone.stage2.0.bn.module_batch_norm_10.input_quantizers.0': QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False),\n",
       " 'backbone.stage2.0.bn.module_batch_norm_10.input_quantizers.1': QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False),\n",
       " 'backbone.stage2.0.bn.module_batch_norm_10.input_quantizers.2': QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False),\n",
       " 'backbone.stage2.0.bn.module_batch_norm_10.input_quantizers.3': QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False),\n",
       " 'backbone.stage2.0.bn.module_batch_norm_10.input_quantizers.4': QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False),\n",
       " 'backbone.stage2.0.bn.module_batch_norm_10.output_quantizers': ModuleList(\n",
       "   (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       " ),\n",
       " 'backbone.stage2.0.bn.module_batch_norm_10.output_quantizers.0': QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False),\n",
       " 'backbone.stage2.0.activate': CustomSiLU(\n",
       "   (sigmoid): QuantizedSigmoid(\n",
       "     (param_quantizers): ModuleDict()\n",
       "     (input_quantizers): ModuleList(\n",
       "       (0): None\n",
       "     )\n",
       "     (output_quantizers): ModuleList(\n",
       "       (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "     )\n",
       "   )\n",
       "   (mul): QuantizedMultiply(\n",
       "     (param_quantizers): ModuleDict()\n",
       "     (input_quantizers): ModuleList(\n",
       "       (0-1): 2 x None\n",
       "     )\n",
       "     (output_quantizers): ModuleList(\n",
       "       (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "     )\n",
       "   )\n",
       " ),\n",
       " 'backbone.stage2.0.activate.sigmoid': QuantizedSigmoid(\n",
       "   (param_quantizers): ModuleDict()\n",
       "   (input_quantizers): ModuleList(\n",
       "     (0): None\n",
       "   )\n",
       "   (output_quantizers): ModuleList(\n",
       "     (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "   )\n",
       " ),\n",
       " 'backbone.stage2.0.activate.sigmoid.param_quantizers': ModuleDict(),\n",
       " 'backbone.stage2.0.activate.sigmoid.input_quantizers': ModuleList(\n",
       "   (0): None\n",
       " ),\n",
       " 'backbone.stage2.0.activate.sigmoid.output_quantizers': ModuleList(\n",
       "   (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       " ),\n",
       " 'backbone.stage2.0.activate.sigmoid.output_quantizers.0': QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False),\n",
       " 'backbone.stage2.0.activate.mul': QuantizedMultiply(\n",
       "   (param_quantizers): ModuleDict()\n",
       "   (input_quantizers): ModuleList(\n",
       "     (0-1): 2 x None\n",
       "   )\n",
       "   (output_quantizers): ModuleList(\n",
       "     (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "   )\n",
       " ),\n",
       " 'backbone.stage2.0.activate.mul.param_quantizers': ModuleDict(),\n",
       " 'backbone.stage2.0.activate.mul.input_quantizers': ModuleList(\n",
       "   (0-1): 2 x None\n",
       " ),\n",
       " 'backbone.stage2.0.activate.mul.output_quantizers': ModuleList(\n",
       "   (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       " ),\n",
       " 'backbone.stage2.0.activate.mul.output_quantizers.0': QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False),\n",
       " 'backbone.stage2.1': Module(\n",
       "   (short_conv): Module(\n",
       "     (conv): QuantizedConv2d(\n",
       "       96, 48, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "       (param_quantizers): ModuleDict(\n",
       "         (weight): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=True)\n",
       "       )\n",
       "       (input_quantizers): ModuleList(\n",
       "         (0): None\n",
       "       )\n",
       "       (output_quantizers): ModuleList(\n",
       "         (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "       )\n",
       "     )\n",
       "     (bn): Module(\n",
       "       (module_batch_norm_11): FakeQuantizedBatchNorm(\n",
       "         (param_quantizers): ModuleDict()\n",
       "         (input_quantizers): ModuleList(\n",
       "           (0-4): 5 x QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "         )\n",
       "         (output_quantizers): ModuleList(\n",
       "           (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "         )\n",
       "       )\n",
       "     )\n",
       "     (activate): CustomSiLU(\n",
       "       (sigmoid): QuantizedSigmoid(\n",
       "         (param_quantizers): ModuleDict()\n",
       "         (input_quantizers): ModuleList(\n",
       "           (0): None\n",
       "         )\n",
       "         (output_quantizers): ModuleList(\n",
       "           (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "         )\n",
       "       )\n",
       "       (mul): QuantizedMultiply(\n",
       "         (param_quantizers): ModuleDict()\n",
       "         (input_quantizers): ModuleList(\n",
       "           (0-1): 2 x None\n",
       "         )\n",
       "         (output_quantizers): ModuleList(\n",
       "           (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "         )\n",
       "       )\n",
       "     )\n",
       "   )\n",
       "   (main_conv): Module(\n",
       "     (conv): QuantizedConv2d(\n",
       "       96, 48, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "       (param_quantizers): ModuleDict(\n",
       "         (weight): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=True)\n",
       "       )\n",
       "       (input_quantizers): ModuleList(\n",
       "         (0): None\n",
       "       )\n",
       "       (output_quantizers): ModuleList(\n",
       "         (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "       )\n",
       "     )\n",
       "     (bn): Module(\n",
       "       (module_batch_norm_12): FakeQuantizedBatchNorm(\n",
       "         (param_quantizers): ModuleDict()\n",
       "         (input_quantizers): ModuleList(\n",
       "           (0-4): 5 x QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "         )\n",
       "         (output_quantizers): ModuleList(\n",
       "           (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "         )\n",
       "       )\n",
       "     )\n",
       "     (activate): CustomSiLU(\n",
       "       (sigmoid): QuantizedSigmoid(\n",
       "         (param_quantizers): ModuleDict()\n",
       "         (input_quantizers): ModuleList(\n",
       "           (0): None\n",
       "         )\n",
       "         (output_quantizers): ModuleList(\n",
       "           (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "         )\n",
       "       )\n",
       "       (mul): QuantizedMultiply(\n",
       "         (param_quantizers): ModuleDict()\n",
       "         (input_quantizers): ModuleList(\n",
       "           (0-1): 2 x None\n",
       "         )\n",
       "         (output_quantizers): ModuleList(\n",
       "           (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "         )\n",
       "       )\n",
       "     )\n",
       "   )\n",
       "   (blocks): Module(\n",
       "     (0): Module(\n",
       "       (conv1): Module(\n",
       "         (conv): QuantizedConv2d(\n",
       "           48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "           (param_quantizers): ModuleDict(\n",
       "             (weight): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=True)\n",
       "           )\n",
       "           (input_quantizers): ModuleList(\n",
       "             (0): None\n",
       "           )\n",
       "           (output_quantizers): ModuleList(\n",
       "             (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "           )\n",
       "         )\n",
       "         (bn): Module(\n",
       "           (module_batch_norm_13): FakeQuantizedBatchNorm(\n",
       "             (param_quantizers): ModuleDict()\n",
       "             (input_quantizers): ModuleList(\n",
       "               (0-4): 5 x QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "             )\n",
       "             (output_quantizers): ModuleList(\n",
       "               (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "             )\n",
       "           )\n",
       "         )\n",
       "         (activate): CustomSiLU(\n",
       "           (sigmoid): QuantizedSigmoid(\n",
       "             (param_quantizers): ModuleDict()\n",
       "             (input_quantizers): ModuleList(\n",
       "               (0): None\n",
       "             )\n",
       "             (output_quantizers): ModuleList(\n",
       "               (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "             )\n",
       "           )\n",
       "           (mul): QuantizedMultiply(\n",
       "             (param_quantizers): ModuleDict()\n",
       "             (input_quantizers): ModuleList(\n",
       "               (0-1): 2 x None\n",
       "             )\n",
       "             (output_quantizers): ModuleList(\n",
       "               (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "             )\n",
       "           )\n",
       "         )\n",
       "       )\n",
       "       (conv2): Module(\n",
       "         (depthwise_conv): Module(\n",
       "           (conv): QuantizedConv2d(\n",
       "             48, 48, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=48, bias=False\n",
       "             (param_quantizers): ModuleDict(\n",
       "               (weight): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=True)\n",
       "             )\n",
       "             (input_quantizers): ModuleList(\n",
       "               (0): None\n",
       "             )\n",
       "             (output_quantizers): ModuleList(\n",
       "               (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "             )\n",
       "           )\n",
       "           (bn): Module(\n",
       "             (module_batch_norm_14): BatchNorm()\n",
       "           )\n",
       "           (activate): CustomSiLU(\n",
       "             (sigmoid): QuantizedSigmoid(\n",
       "               (param_quantizers): ModuleDict()\n",
       "               (input_quantizers): ModuleList(\n",
       "                 (0): None\n",
       "               )\n",
       "               (output_quantizers): ModuleList(\n",
       "                 (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "               )\n",
       "             )\n",
       "             (mul): QuantizedMultiply(\n",
       "               (param_quantizers): ModuleDict()\n",
       "               (input_quantizers): ModuleList(\n",
       "                 (0-1): 2 x None\n",
       "               )\n",
       "               (output_quantizers): ModuleList(\n",
       "                 (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "               )\n",
       "             )\n",
       "           )\n",
       "         )\n",
       "         (pointwise_conv): Module(\n",
       "           (conv): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "           (bn): Module(\n",
       "             (module_batch_norm_15): FakeQuantizedBatchNorm(\n",
       "               (param_quantizers): ModuleDict()\n",
       "               (input_quantizers): ModuleList(\n",
       "                 (0-4): 5 x QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "               )\n",
       "               (output_quantizers): ModuleList(\n",
       "                 (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "               )\n",
       "             )\n",
       "           )\n",
       "           (activate): CustomSiLU(\n",
       "             (sigmoid): QuantizedSigmoid(\n",
       "               (param_quantizers): ModuleDict()\n",
       "               (input_quantizers): ModuleList(\n",
       "                 (0): None\n",
       "               )\n",
       "               (output_quantizers): ModuleList(\n",
       "                 (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "               )\n",
       "             )\n",
       "             (mul): QuantizedMultiply(\n",
       "               (param_quantizers): ModuleDict()\n",
       "               (input_quantizers): ModuleList(\n",
       "                 (0-1): 2 x None\n",
       "               )\n",
       "               (output_quantizers): ModuleList(\n",
       "                 (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "               )\n",
       "             )\n",
       "           )\n",
       "         )\n",
       "       )\n",
       "       (module_add_1): QuantizedAdd(\n",
       "         (param_quantizers): ModuleDict()\n",
       "         (input_quantizers): ModuleList(\n",
       "           (0-1): 2 x None\n",
       "         )\n",
       "         (output_quantizers): ModuleList(\n",
       "           (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "         )\n",
       "       )\n",
       "     )\n",
       "   )\n",
       "   (attention): Module(\n",
       "     (global_avgpool): FakeQuantizedAdaptiveAvgPool2d(\n",
       "       output_size=1\n",
       "       (param_quantizers): ModuleDict()\n",
       "       (input_quantizers): ModuleList(\n",
       "         (0): None\n",
       "       )\n",
       "       (output_quantizers): ModuleList(\n",
       "         (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "       )\n",
       "     )\n",
       "     (fc): QuantizedConv2d(\n",
       "       96, 96, kernel_size=(1, 1), stride=(1, 1)\n",
       "       (param_quantizers): ModuleDict(\n",
       "         (weight): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=True)\n",
       "         (bias): None\n",
       "       )\n",
       "       (input_quantizers): ModuleList(\n",
       "         (0): None\n",
       "       )\n",
       "       (output_quantizers): ModuleList(\n",
       "         (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "       )\n",
       "     )\n",
       "     (act): FakeQuantizedHardsigmoid(\n",
       "       (param_quantizers): ModuleDict()\n",
       "       (input_quantizers): ModuleList(\n",
       "         (0): None\n",
       "       )\n",
       "       (output_quantizers): ModuleList(\n",
       "         (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "       )\n",
       "     )\n",
       "     (module_mul_1): QuantizedMultiply(\n",
       "       (param_quantizers): ModuleDict()\n",
       "       (input_quantizers): ModuleList(\n",
       "         (0-1): 2 x None\n",
       "       )\n",
       "       (output_quantizers): ModuleList(\n",
       "         (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "       )\n",
       "     )\n",
       "   )\n",
       "   (final_conv): Module(\n",
       "     (conv): QuantizedConv2d(\n",
       "       96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "       (param_quantizers): ModuleDict(\n",
       "         (weight): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=True)\n",
       "       )\n",
       "       (input_quantizers): ModuleList(\n",
       "         (0): None\n",
       "       )\n",
       "       (output_quantizers): ModuleList(\n",
       "         (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "       )\n",
       "     )\n",
       "     (bn): Module(\n",
       "       (module_batch_norm_16): FakeQuantizedBatchNorm(\n",
       "         (param_quantizers): ModuleDict()\n",
       "         (input_quantizers): ModuleList(\n",
       "           (0-4): 5 x QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "         )\n",
       "         (output_quantizers): ModuleList(\n",
       "           (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "         )\n",
       "       )\n",
       "     )\n",
       "     (activate): CustomSiLU(\n",
       "       (sigmoid): QuantizedSigmoid(\n",
       "         (param_quantizers): ModuleDict()\n",
       "         (input_quantizers): ModuleList(\n",
       "           (0): None\n",
       "         )\n",
       "         (output_quantizers): ModuleList(\n",
       "           (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "         )\n",
       "       )\n",
       "       (mul): QuantizedMultiply(\n",
       "         (param_quantizers): ModuleDict()\n",
       "         (input_quantizers): ModuleList(\n",
       "           (0-1): 2 x None\n",
       "         )\n",
       "         (output_quantizers): ModuleList(\n",
       "           (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "         )\n",
       "       )\n",
       "     )\n",
       "   )\n",
       "   (module_cat_1): FakeQuantizedConcat(\n",
       "     (param_quantizers): ModuleDict()\n",
       "     (input_quantizers): ModuleList(\n",
       "       (0): None\n",
       "     )\n",
       "     (output_quantizers): ModuleList(\n",
       "       (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "     )\n",
       "   )\n",
       " ),\n",
       " 'backbone.stage2.1.short_conv': Module(\n",
       "   (conv): QuantizedConv2d(\n",
       "     96, 48, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "     (param_quantizers): ModuleDict(\n",
       "       (weight): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=True)\n",
       "     )\n",
       "     (input_quantizers): ModuleList(\n",
       "       (0): None\n",
       "     )\n",
       "     (output_quantizers): ModuleList(\n",
       "       (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "     )\n",
       "   )\n",
       "   (bn): Module(\n",
       "     (module_batch_norm_11): FakeQuantizedBatchNorm(\n",
       "       (param_quantizers): ModuleDict()\n",
       "       (input_quantizers): ModuleList(\n",
       "         (0-4): 5 x QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "       )\n",
       "       (output_quantizers): ModuleList(\n",
       "         (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "       )\n",
       "     )\n",
       "   )\n",
       "   (activate): CustomSiLU(\n",
       "     (sigmoid): QuantizedSigmoid(\n",
       "       (param_quantizers): ModuleDict()\n",
       "       (input_quantizers): ModuleList(\n",
       "         (0): None\n",
       "       )\n",
       "       (output_quantizers): ModuleList(\n",
       "         (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "       )\n",
       "     )\n",
       "     (mul): QuantizedMultiply(\n",
       "       (param_quantizers): ModuleDict()\n",
       "       (input_quantizers): ModuleList(\n",
       "         (0-1): 2 x None\n",
       "       )\n",
       "       (output_quantizers): ModuleList(\n",
       "         (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "       )\n",
       "     )\n",
       "   )\n",
       " ),\n",
       " 'backbone.stage2.1.short_conv.conv': QuantizedConv2d(\n",
       "   96, 48, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "   (param_quantizers): ModuleDict(\n",
       "     (weight): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=True)\n",
       "   )\n",
       "   (input_quantizers): ModuleList(\n",
       "     (0): None\n",
       "   )\n",
       "   (output_quantizers): ModuleList(\n",
       "     (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "   )\n",
       " ),\n",
       " 'backbone.stage2.1.short_conv.conv.param_quantizers': ModuleDict(\n",
       "   (weight): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=True)\n",
       " ),\n",
       " 'backbone.stage2.1.short_conv.conv.param_quantizers.weight': QuantizeDequantize(shape=[1], bitwidth=8, symmetric=True),\n",
       " 'backbone.stage2.1.short_conv.conv.input_quantizers': ModuleList(\n",
       "   (0): None\n",
       " ),\n",
       " 'backbone.stage2.1.short_conv.conv.output_quantizers': ModuleList(\n",
       "   (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       " ),\n",
       " 'backbone.stage2.1.short_conv.conv.output_quantizers.0': QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False),\n",
       " 'backbone.stage2.1.short_conv.bn': Module(\n",
       "   (module_batch_norm_11): FakeQuantizedBatchNorm(\n",
       "     (param_quantizers): ModuleDict()\n",
       "     (input_quantizers): ModuleList(\n",
       "       (0-4): 5 x QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "     )\n",
       "     (output_quantizers): ModuleList(\n",
       "       (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "     )\n",
       "   )\n",
       " ),\n",
       " 'backbone.stage2.1.short_conv.bn.module_batch_norm_11': FakeQuantizedBatchNorm(\n",
       "   (param_quantizers): ModuleDict()\n",
       "   (input_quantizers): ModuleList(\n",
       "     (0-4): 5 x QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "   )\n",
       "   (output_quantizers): ModuleList(\n",
       "     (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "   )\n",
       " ),\n",
       " 'backbone.stage2.1.short_conv.bn.module_batch_norm_11.param_quantizers': ModuleDict(),\n",
       " 'backbone.stage2.1.short_conv.bn.module_batch_norm_11.input_quantizers': ModuleList(\n",
       "   (0-4): 5 x QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       " ),\n",
       " 'backbone.stage2.1.short_conv.bn.module_batch_norm_11.input_quantizers.0': QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False),\n",
       " 'backbone.stage2.1.short_conv.bn.module_batch_norm_11.input_quantizers.1': QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False),\n",
       " 'backbone.stage2.1.short_conv.bn.module_batch_norm_11.input_quantizers.2': QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False),\n",
       " 'backbone.stage2.1.short_conv.bn.module_batch_norm_11.input_quantizers.3': QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False),\n",
       " 'backbone.stage2.1.short_conv.bn.module_batch_norm_11.input_quantizers.4': QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False),\n",
       " 'backbone.stage2.1.short_conv.bn.module_batch_norm_11.output_quantizers': ModuleList(\n",
       "   (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       " ),\n",
       " 'backbone.stage2.1.short_conv.bn.module_batch_norm_11.output_quantizers.0': QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False),\n",
       " 'backbone.stage2.1.short_conv.activate': CustomSiLU(\n",
       "   (sigmoid): QuantizedSigmoid(\n",
       "     (param_quantizers): ModuleDict()\n",
       "     (input_quantizers): ModuleList(\n",
       "       (0): None\n",
       "     )\n",
       "     (output_quantizers): ModuleList(\n",
       "       (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "     )\n",
       "   )\n",
       "   (mul): QuantizedMultiply(\n",
       "     (param_quantizers): ModuleDict()\n",
       "     (input_quantizers): ModuleList(\n",
       "       (0-1): 2 x None\n",
       "     )\n",
       "     (output_quantizers): ModuleList(\n",
       "       (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "     )\n",
       "   )\n",
       " ),\n",
       " 'backbone.stage2.1.short_conv.activate.sigmoid': QuantizedSigmoid(\n",
       "   (param_quantizers): ModuleDict()\n",
       "   (input_quantizers): ModuleList(\n",
       "     (0): None\n",
       "   )\n",
       "   (output_quantizers): ModuleList(\n",
       "     (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "   )\n",
       " ),\n",
       " 'backbone.stage2.1.short_conv.activate.sigmoid.param_quantizers': ModuleDict(),\n",
       " 'backbone.stage2.1.short_conv.activate.sigmoid.input_quantizers': ModuleList(\n",
       "   (0): None\n",
       " ),\n",
       " 'backbone.stage2.1.short_conv.activate.sigmoid.output_quantizers': ModuleList(\n",
       "   (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       " ),\n",
       " 'backbone.stage2.1.short_conv.activate.sigmoid.output_quantizers.0': QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False),\n",
       " 'backbone.stage2.1.short_conv.activate.mul': QuantizedMultiply(\n",
       "   (param_quantizers): ModuleDict()\n",
       "   (input_quantizers): ModuleList(\n",
       "     (0-1): 2 x None\n",
       "   )\n",
       "   (output_quantizers): ModuleList(\n",
       "     (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "   )\n",
       " ),\n",
       " 'backbone.stage2.1.short_conv.activate.mul.param_quantizers': ModuleDict(),\n",
       " 'backbone.stage2.1.short_conv.activate.mul.input_quantizers': ModuleList(\n",
       "   (0-1): 2 x None\n",
       " ),\n",
       " 'backbone.stage2.1.short_conv.activate.mul.output_quantizers': ModuleList(\n",
       "   (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       " ),\n",
       " 'backbone.stage2.1.short_conv.activate.mul.output_quantizers.0': QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False),\n",
       " 'backbone.stage2.1.main_conv': Module(\n",
       "   (conv): QuantizedConv2d(\n",
       "     96, 48, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "     (param_quantizers): ModuleDict(\n",
       "       (weight): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=True)\n",
       "     )\n",
       "     (input_quantizers): ModuleList(\n",
       "       (0): None\n",
       "     )\n",
       "     (output_quantizers): ModuleList(\n",
       "       (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "     )\n",
       "   )\n",
       "   (bn): Module(\n",
       "     (module_batch_norm_12): FakeQuantizedBatchNorm(\n",
       "       (param_quantizers): ModuleDict()\n",
       "       (input_quantizers): ModuleList(\n",
       "         (0-4): 5 x QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "       )\n",
       "       (output_quantizers): ModuleList(\n",
       "         (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "       )\n",
       "     )\n",
       "   )\n",
       "   (activate): CustomSiLU(\n",
       "     (sigmoid): QuantizedSigmoid(\n",
       "       (param_quantizers): ModuleDict()\n",
       "       (input_quantizers): ModuleList(\n",
       "         (0): None\n",
       "       )\n",
       "       (output_quantizers): ModuleList(\n",
       "         (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "       )\n",
       "     )\n",
       "     (mul): QuantizedMultiply(\n",
       "       (param_quantizers): ModuleDict()\n",
       "       (input_quantizers): ModuleList(\n",
       "         (0-1): 2 x None\n",
       "       )\n",
       "       (output_quantizers): ModuleList(\n",
       "         (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "       )\n",
       "     )\n",
       "   )\n",
       " ),\n",
       " 'backbone.stage2.1.main_conv.conv': QuantizedConv2d(\n",
       "   96, 48, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "   (param_quantizers): ModuleDict(\n",
       "     (weight): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=True)\n",
       "   )\n",
       "   (input_quantizers): ModuleList(\n",
       "     (0): None\n",
       "   )\n",
       "   (output_quantizers): ModuleList(\n",
       "     (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "   )\n",
       " ),\n",
       " 'backbone.stage2.1.main_conv.conv.param_quantizers': ModuleDict(\n",
       "   (weight): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=True)\n",
       " ),\n",
       " 'backbone.stage2.1.main_conv.conv.param_quantizers.weight': QuantizeDequantize(shape=[1], bitwidth=8, symmetric=True),\n",
       " 'backbone.stage2.1.main_conv.conv.input_quantizers': ModuleList(\n",
       "   (0): None\n",
       " ),\n",
       " 'backbone.stage2.1.main_conv.conv.output_quantizers': ModuleList(\n",
       "   (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       " ),\n",
       " 'backbone.stage2.1.main_conv.conv.output_quantizers.0': QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False),\n",
       " 'backbone.stage2.1.main_conv.bn': Module(\n",
       "   (module_batch_norm_12): FakeQuantizedBatchNorm(\n",
       "     (param_quantizers): ModuleDict()\n",
       "     (input_quantizers): ModuleList(\n",
       "       (0-4): 5 x QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "     )\n",
       "     (output_quantizers): ModuleList(\n",
       "       (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "     )\n",
       "   )\n",
       " ),\n",
       " 'backbone.stage2.1.main_conv.bn.module_batch_norm_12': FakeQuantizedBatchNorm(\n",
       "   (param_quantizers): ModuleDict()\n",
       "   (input_quantizers): ModuleList(\n",
       "     (0-4): 5 x QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "   )\n",
       "   (output_quantizers): ModuleList(\n",
       "     (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "   )\n",
       " ),\n",
       " 'backbone.stage2.1.main_conv.bn.module_batch_norm_12.param_quantizers': ModuleDict(),\n",
       " 'backbone.stage2.1.main_conv.bn.module_batch_norm_12.input_quantizers': ModuleList(\n",
       "   (0-4): 5 x QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       " ),\n",
       " 'backbone.stage2.1.main_conv.bn.module_batch_norm_12.input_quantizers.0': QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False),\n",
       " 'backbone.stage2.1.main_conv.bn.module_batch_norm_12.input_quantizers.1': QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False),\n",
       " 'backbone.stage2.1.main_conv.bn.module_batch_norm_12.input_quantizers.2': QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False),\n",
       " 'backbone.stage2.1.main_conv.bn.module_batch_norm_12.input_quantizers.3': QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False),\n",
       " 'backbone.stage2.1.main_conv.bn.module_batch_norm_12.input_quantizers.4': QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False),\n",
       " 'backbone.stage2.1.main_conv.bn.module_batch_norm_12.output_quantizers': ModuleList(\n",
       "   (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       " ),\n",
       " 'backbone.stage2.1.main_conv.bn.module_batch_norm_12.output_quantizers.0': QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False),\n",
       " 'backbone.stage2.1.main_conv.activate': CustomSiLU(\n",
       "   (sigmoid): QuantizedSigmoid(\n",
       "     (param_quantizers): ModuleDict()\n",
       "     (input_quantizers): ModuleList(\n",
       "       (0): None\n",
       "     )\n",
       "     (output_quantizers): ModuleList(\n",
       "       (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "     )\n",
       "   )\n",
       "   (mul): QuantizedMultiply(\n",
       "     (param_quantizers): ModuleDict()\n",
       "     (input_quantizers): ModuleList(\n",
       "       (0-1): 2 x None\n",
       "     )\n",
       "     (output_quantizers): ModuleList(\n",
       "       (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "     )\n",
       "   )\n",
       " ),\n",
       " 'backbone.stage2.1.main_conv.activate.sigmoid': QuantizedSigmoid(\n",
       "   (param_quantizers): ModuleDict()\n",
       "   (input_quantizers): ModuleList(\n",
       "     (0): None\n",
       "   )\n",
       "   (output_quantizers): ModuleList(\n",
       "     (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "   )\n",
       " ),\n",
       " 'backbone.stage2.1.main_conv.activate.sigmoid.param_quantizers': ModuleDict(),\n",
       " 'backbone.stage2.1.main_conv.activate.sigmoid.input_quantizers': ModuleList(\n",
       "   (0): None\n",
       " ),\n",
       " 'backbone.stage2.1.main_conv.activate.sigmoid.output_quantizers': ModuleList(\n",
       "   (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       " ),\n",
       " 'backbone.stage2.1.main_conv.activate.sigmoid.output_quantizers.0': QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False),\n",
       " 'backbone.stage2.1.main_conv.activate.mul': QuantizedMultiply(\n",
       "   (param_quantizers): ModuleDict()\n",
       "   (input_quantizers): ModuleList(\n",
       "     (0-1): 2 x None\n",
       "   )\n",
       "   (output_quantizers): ModuleList(\n",
       "     (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "   )\n",
       " ),\n",
       " 'backbone.stage2.1.main_conv.activate.mul.param_quantizers': ModuleDict(),\n",
       " 'backbone.stage2.1.main_conv.activate.mul.input_quantizers': ModuleList(\n",
       "   (0-1): 2 x None\n",
       " ),\n",
       " 'backbone.stage2.1.main_conv.activate.mul.output_quantizers': ModuleList(\n",
       "   (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       " ),\n",
       " 'backbone.stage2.1.main_conv.activate.mul.output_quantizers.0': QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False),\n",
       " 'backbone.stage2.1.blocks': Module(\n",
       "   (0): Module(\n",
       "     (conv1): Module(\n",
       "       (conv): QuantizedConv2d(\n",
       "         48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "         (param_quantizers): ModuleDict(\n",
       "           (weight): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=True)\n",
       "         )\n",
       "         (input_quantizers): ModuleList(\n",
       "           (0): None\n",
       "         )\n",
       "         (output_quantizers): ModuleList(\n",
       "           (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "         )\n",
       "       )\n",
       "       (bn): Module(\n",
       "         (module_batch_norm_13): FakeQuantizedBatchNorm(\n",
       "           (param_quantizers): ModuleDict()\n",
       "           (input_quantizers): ModuleList(\n",
       "             (0-4): 5 x QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "           )\n",
       "           (output_quantizers): ModuleList(\n",
       "             (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "           )\n",
       "         )\n",
       "       )\n",
       "       (activate): CustomSiLU(\n",
       "         (sigmoid): QuantizedSigmoid(\n",
       "           (param_quantizers): ModuleDict()\n",
       "           (input_quantizers): ModuleList(\n",
       "             (0): None\n",
       "           )\n",
       "           (output_quantizers): ModuleList(\n",
       "             (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "           )\n",
       "         )\n",
       "         (mul): QuantizedMultiply(\n",
       "           (param_quantizers): ModuleDict()\n",
       "           (input_quantizers): ModuleList(\n",
       "             (0-1): 2 x None\n",
       "           )\n",
       "           (output_quantizers): ModuleList(\n",
       "             (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "           )\n",
       "         )\n",
       "       )\n",
       "     )\n",
       "     (conv2): Module(\n",
       "       (depthwise_conv): Module(\n",
       "         (conv): QuantizedConv2d(\n",
       "           48, 48, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=48, bias=False\n",
       "           (param_quantizers): ModuleDict(\n",
       "             (weight): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=True)\n",
       "           )\n",
       "           (input_quantizers): ModuleList(\n",
       "             (0): None\n",
       "           )\n",
       "           (output_quantizers): ModuleList(\n",
       "             (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "           )\n",
       "         )\n",
       "         (bn): Module(\n",
       "           (module_batch_norm_14): BatchNorm()\n",
       "         )\n",
       "         (activate): CustomSiLU(\n",
       "           (sigmoid): QuantizedSigmoid(\n",
       "             (param_quantizers): ModuleDict()\n",
       "             (input_quantizers): ModuleList(\n",
       "               (0): None\n",
       "             )\n",
       "             (output_quantizers): ModuleList(\n",
       "               (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "             )\n",
       "           )\n",
       "           (mul): QuantizedMultiply(\n",
       "             (param_quantizers): ModuleDict()\n",
       "             (input_quantizers): ModuleList(\n",
       "               (0-1): 2 x None\n",
       "             )\n",
       "             (output_quantizers): ModuleList(\n",
       "               (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "             )\n",
       "           )\n",
       "         )\n",
       "       )\n",
       "       (pointwise_conv): Module(\n",
       "         (conv): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "         (bn): Module(\n",
       "           (module_batch_norm_15): FakeQuantizedBatchNorm(\n",
       "             (param_quantizers): ModuleDict()\n",
       "             (input_quantizers): ModuleList(\n",
       "               (0-4): 5 x QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "             )\n",
       "             (output_quantizers): ModuleList(\n",
       "               (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "             )\n",
       "           )\n",
       "         )\n",
       "         (activate): CustomSiLU(\n",
       "           (sigmoid): QuantizedSigmoid(\n",
       "             (param_quantizers): ModuleDict()\n",
       "             (input_quantizers): ModuleList(\n",
       "               (0): None\n",
       "             )\n",
       "             (output_quantizers): ModuleList(\n",
       "               (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "             )\n",
       "           )\n",
       "           (mul): QuantizedMultiply(\n",
       "             (param_quantizers): ModuleDict()\n",
       "             (input_quantizers): ModuleList(\n",
       "               (0-1): 2 x None\n",
       "             )\n",
       "             (output_quantizers): ModuleList(\n",
       "               (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "             )\n",
       "           )\n",
       "         )\n",
       "       )\n",
       "     )\n",
       "     (module_add_1): QuantizedAdd(\n",
       "       (param_quantizers): ModuleDict()\n",
       "       (input_quantizers): ModuleList(\n",
       "         (0-1): 2 x None\n",
       "       )\n",
       "       (output_quantizers): ModuleList(\n",
       "         (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "       )\n",
       "     )\n",
       "   )\n",
       " ),\n",
       " 'backbone.stage2.1.blocks.0': Module(\n",
       "   (conv1): Module(\n",
       "     (conv): QuantizedConv2d(\n",
       "       48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "       (param_quantizers): ModuleDict(\n",
       "         (weight): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=True)\n",
       "       )\n",
       "       (input_quantizers): ModuleList(\n",
       "         (0): None\n",
       "       )\n",
       "       (output_quantizers): ModuleList(\n",
       "         (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "       )\n",
       "     )\n",
       "     (bn): Module(\n",
       "       (module_batch_norm_13): FakeQuantizedBatchNorm(\n",
       "         (param_quantizers): ModuleDict()\n",
       "         (input_quantizers): ModuleList(\n",
       "           (0-4): 5 x QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "         )\n",
       "         (output_quantizers): ModuleList(\n",
       "           (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "         )\n",
       "       )\n",
       "     )\n",
       "     (activate): CustomSiLU(\n",
       "       (sigmoid): QuantizedSigmoid(\n",
       "         (param_quantizers): ModuleDict()\n",
       "         (input_quantizers): ModuleList(\n",
       "           (0): None\n",
       "         )\n",
       "         (output_quantizers): ModuleList(\n",
       "           (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "         )\n",
       "       )\n",
       "       (mul): QuantizedMultiply(\n",
       "         (param_quantizers): ModuleDict()\n",
       "         (input_quantizers): ModuleList(\n",
       "           (0-1): 2 x None\n",
       "         )\n",
       "         (output_quantizers): ModuleList(\n",
       "           (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "         )\n",
       "       )\n",
       "     )\n",
       "   )\n",
       "   (conv2): Module(\n",
       "     (depthwise_conv): Module(\n",
       "       (conv): QuantizedConv2d(\n",
       "         48, 48, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=48, bias=False\n",
       "         (param_quantizers): ModuleDict(\n",
       "           (weight): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=True)\n",
       "         )\n",
       "         (input_quantizers): ModuleList(\n",
       "           (0): None\n",
       "         )\n",
       "         (output_quantizers): ModuleList(\n",
       "           (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "         )\n",
       "       )\n",
       "       (bn): Module(\n",
       "         (module_batch_norm_14): BatchNorm()\n",
       "       )\n",
       "       (activate): CustomSiLU(\n",
       "         (sigmoid): QuantizedSigmoid(\n",
       "           (param_quantizers): ModuleDict()\n",
       "           (input_quantizers): ModuleList(\n",
       "             (0): None\n",
       "           )\n",
       "           (output_quantizers): ModuleList(\n",
       "             (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "           )\n",
       "         )\n",
       "         (mul): QuantizedMultiply(\n",
       "           (param_quantizers): ModuleDict()\n",
       "           (input_quantizers): ModuleList(\n",
       "             (0-1): 2 x None\n",
       "           )\n",
       "           (output_quantizers): ModuleList(\n",
       "             (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "           )\n",
       "         )\n",
       "       )\n",
       "     )\n",
       "     (pointwise_conv): Module(\n",
       "       (conv): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "       (bn): Module(\n",
       "         (module_batch_norm_15): FakeQuantizedBatchNorm(\n",
       "           (param_quantizers): ModuleDict()\n",
       "           (input_quantizers): ModuleList(\n",
       "             (0-4): 5 x QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "           )\n",
       "           (output_quantizers): ModuleList(\n",
       "             (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "           )\n",
       "         )\n",
       "       )\n",
       "       (activate): CustomSiLU(\n",
       "         (sigmoid): QuantizedSigmoid(\n",
       "           (param_quantizers): ModuleDict()\n",
       "           (input_quantizers): ModuleList(\n",
       "             (0): None\n",
       "           )\n",
       "           (output_quantizers): ModuleList(\n",
       "             (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "           )\n",
       "         )\n",
       "         (mul): QuantizedMultiply(\n",
       "           (param_quantizers): ModuleDict()\n",
       "           (input_quantizers): ModuleList(\n",
       "             (0-1): 2 x None\n",
       "           )\n",
       "           (output_quantizers): ModuleList(\n",
       "             (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "           )\n",
       "         )\n",
       "       )\n",
       "     )\n",
       "   )\n",
       "   (module_add_1): QuantizedAdd(\n",
       "     (param_quantizers): ModuleDict()\n",
       "     (input_quantizers): ModuleList(\n",
       "       (0-1): 2 x None\n",
       "     )\n",
       "     (output_quantizers): ModuleList(\n",
       "       (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "     )\n",
       "   )\n",
       " ),\n",
       " 'backbone.stage2.1.blocks.0.conv1': Module(\n",
       "   (conv): QuantizedConv2d(\n",
       "     48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "     (param_quantizers): ModuleDict(\n",
       "       (weight): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=True)\n",
       "     )\n",
       "     (input_quantizers): ModuleList(\n",
       "       (0): None\n",
       "     )\n",
       "     (output_quantizers): ModuleList(\n",
       "       (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "     )\n",
       "   )\n",
       "   (bn): Module(\n",
       "     (module_batch_norm_13): FakeQuantizedBatchNorm(\n",
       "       (param_quantizers): ModuleDict()\n",
       "       (input_quantizers): ModuleList(\n",
       "         (0-4): 5 x QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "       )\n",
       "       (output_quantizers): ModuleList(\n",
       "         (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "       )\n",
       "     )\n",
       "   )\n",
       "   (activate): CustomSiLU(\n",
       "     (sigmoid): QuantizedSigmoid(\n",
       "       (param_quantizers): ModuleDict()\n",
       "       (input_quantizers): ModuleList(\n",
       "         (0): None\n",
       "       )\n",
       "       (output_quantizers): ModuleList(\n",
       "         (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "       )\n",
       "     )\n",
       "     (mul): QuantizedMultiply(\n",
       "       (param_quantizers): ModuleDict()\n",
       "       (input_quantizers): ModuleList(\n",
       "         (0-1): 2 x None\n",
       "       )\n",
       "       (output_quantizers): ModuleList(\n",
       "         (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "       )\n",
       "     )\n",
       "   )\n",
       " ),\n",
       " 'backbone.stage2.1.blocks.0.conv1.conv': QuantizedConv2d(\n",
       "   48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "   (param_quantizers): ModuleDict(\n",
       "     (weight): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=True)\n",
       "   )\n",
       "   (input_quantizers): ModuleList(\n",
       "     (0): None\n",
       "   )\n",
       "   (output_quantizers): ModuleList(\n",
       "     (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "   )\n",
       " ),\n",
       " 'backbone.stage2.1.blocks.0.conv1.conv.param_quantizers': ModuleDict(\n",
       "   (weight): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=True)\n",
       " ),\n",
       " 'backbone.stage2.1.blocks.0.conv1.conv.param_quantizers.weight': QuantizeDequantize(shape=[1], bitwidth=8, symmetric=True),\n",
       " 'backbone.stage2.1.blocks.0.conv1.conv.input_quantizers': ModuleList(\n",
       "   (0): None\n",
       " ),\n",
       " 'backbone.stage2.1.blocks.0.conv1.conv.output_quantizers': ModuleList(\n",
       "   (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       " ),\n",
       " 'backbone.stage2.1.blocks.0.conv1.conv.output_quantizers.0': QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False),\n",
       " 'backbone.stage2.1.blocks.0.conv1.bn': Module(\n",
       "   (module_batch_norm_13): FakeQuantizedBatchNorm(\n",
       "     (param_quantizers): ModuleDict()\n",
       "     (input_quantizers): ModuleList(\n",
       "       (0-4): 5 x QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "     )\n",
       "     (output_quantizers): ModuleList(\n",
       "       (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "     )\n",
       "   )\n",
       " ),\n",
       " 'backbone.stage2.1.blocks.0.conv1.bn.module_batch_norm_13': FakeQuantizedBatchNorm(\n",
       "   (param_quantizers): ModuleDict()\n",
       "   (input_quantizers): ModuleList(\n",
       "     (0-4): 5 x QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "   )\n",
       "   (output_quantizers): ModuleList(\n",
       "     (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "   )\n",
       " ),\n",
       " 'backbone.stage2.1.blocks.0.conv1.bn.module_batch_norm_13.param_quantizers': ModuleDict(),\n",
       " 'backbone.stage2.1.blocks.0.conv1.bn.module_batch_norm_13.input_quantizers': ModuleList(\n",
       "   (0-4): 5 x QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       " ),\n",
       " 'backbone.stage2.1.blocks.0.conv1.bn.module_batch_norm_13.input_quantizers.0': QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False),\n",
       " 'backbone.stage2.1.blocks.0.conv1.bn.module_batch_norm_13.input_quantizers.1': QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False),\n",
       " 'backbone.stage2.1.blocks.0.conv1.bn.module_batch_norm_13.input_quantizers.2': QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False),\n",
       " 'backbone.stage2.1.blocks.0.conv1.bn.module_batch_norm_13.input_quantizers.3': QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False),\n",
       " 'backbone.stage2.1.blocks.0.conv1.bn.module_batch_norm_13.input_quantizers.4': QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False),\n",
       " 'backbone.stage2.1.blocks.0.conv1.bn.module_batch_norm_13.output_quantizers': ModuleList(\n",
       "   (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       " ),\n",
       " 'backbone.stage2.1.blocks.0.conv1.bn.module_batch_norm_13.output_quantizers.0': QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False),\n",
       " 'backbone.stage2.1.blocks.0.conv1.activate': CustomSiLU(\n",
       "   (sigmoid): QuantizedSigmoid(\n",
       "     (param_quantizers): ModuleDict()\n",
       "     (input_quantizers): ModuleList(\n",
       "       (0): None\n",
       "     )\n",
       "     (output_quantizers): ModuleList(\n",
       "       (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "     )\n",
       "   )\n",
       "   (mul): QuantizedMultiply(\n",
       "     (param_quantizers): ModuleDict()\n",
       "     (input_quantizers): ModuleList(\n",
       "       (0-1): 2 x None\n",
       "     )\n",
       "     (output_quantizers): ModuleList(\n",
       "       (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "     )\n",
       "   )\n",
       " ),\n",
       " 'backbone.stage2.1.blocks.0.conv1.activate.sigmoid': QuantizedSigmoid(\n",
       "   (param_quantizers): ModuleDict()\n",
       "   (input_quantizers): ModuleList(\n",
       "     (0): None\n",
       "   )\n",
       "   (output_quantizers): ModuleList(\n",
       "     (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "   )\n",
       " ),\n",
       " 'backbone.stage2.1.blocks.0.conv1.activate.sigmoid.param_quantizers': ModuleDict(),\n",
       " 'backbone.stage2.1.blocks.0.conv1.activate.sigmoid.input_quantizers': ModuleList(\n",
       "   (0): None\n",
       " ),\n",
       " 'backbone.stage2.1.blocks.0.conv1.activate.sigmoid.output_quantizers': ModuleList(\n",
       "   (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       " ),\n",
       " 'backbone.stage2.1.blocks.0.conv1.activate.sigmoid.output_quantizers.0': QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False),\n",
       " 'backbone.stage2.1.blocks.0.conv1.activate.mul': QuantizedMultiply(\n",
       "   (param_quantizers): ModuleDict()\n",
       "   (input_quantizers): ModuleList(\n",
       "     (0-1): 2 x None\n",
       "   )\n",
       "   (output_quantizers): ModuleList(\n",
       "     (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "   )\n",
       " ),\n",
       " 'backbone.stage2.1.blocks.0.conv1.activate.mul.param_quantizers': ModuleDict(),\n",
       " 'backbone.stage2.1.blocks.0.conv1.activate.mul.input_quantizers': ModuleList(\n",
       "   (0-1): 2 x None\n",
       " ),\n",
       " 'backbone.stage2.1.blocks.0.conv1.activate.mul.output_quantizers': ModuleList(\n",
       "   (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       " ),\n",
       " 'backbone.stage2.1.blocks.0.conv1.activate.mul.output_quantizers.0': QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False),\n",
       " 'backbone.stage2.1.blocks.0.conv2': Module(\n",
       "   (depthwise_conv): Module(\n",
       "     (conv): QuantizedConv2d(\n",
       "       48, 48, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=48, bias=False\n",
       "       (param_quantizers): ModuleDict(\n",
       "         (weight): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=True)\n",
       "       )\n",
       "       (input_quantizers): ModuleList(\n",
       "         (0): None\n",
       "       )\n",
       "       (output_quantizers): ModuleList(\n",
       "         (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "       )\n",
       "     )\n",
       "     (bn): Module(\n",
       "       (module_batch_norm_14): BatchNorm()\n",
       "     )\n",
       "     (activate): CustomSiLU(\n",
       "       (sigmoid): QuantizedSigmoid(\n",
       "         (param_quantizers): ModuleDict()\n",
       "         (input_quantizers): ModuleList(\n",
       "           (0): None\n",
       "         )\n",
       "         (output_quantizers): ModuleList(\n",
       "           (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "         )\n",
       "       )\n",
       "       (mul): QuantizedMultiply(\n",
       "         (param_quantizers): ModuleDict()\n",
       "         (input_quantizers): ModuleList(\n",
       "           (0-1): 2 x None\n",
       "         )\n",
       "         (output_quantizers): ModuleList(\n",
       "           (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "         )\n",
       "       )\n",
       "     )\n",
       "   )\n",
       "   (pointwise_conv): Module(\n",
       "     (conv): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "     (bn): Module(\n",
       "       (module_batch_norm_15): FakeQuantizedBatchNorm(\n",
       "         (param_quantizers): ModuleDict()\n",
       "         (input_quantizers): ModuleList(\n",
       "           (0-4): 5 x QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "         )\n",
       "         (output_quantizers): ModuleList(\n",
       "           (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "         )\n",
       "       )\n",
       "     )\n",
       "     (activate): CustomSiLU(\n",
       "       (sigmoid): QuantizedSigmoid(\n",
       "         (param_quantizers): ModuleDict()\n",
       "         (input_quantizers): ModuleList(\n",
       "           (0): None\n",
       "         )\n",
       "         (output_quantizers): ModuleList(\n",
       "           (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "         )\n",
       "       )\n",
       "       (mul): QuantizedMultiply(\n",
       "         (param_quantizers): ModuleDict()\n",
       "         (input_quantizers): ModuleList(\n",
       "           (0-1): 2 x None\n",
       "         )\n",
       "         (output_quantizers): ModuleList(\n",
       "           (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "         )\n",
       "       )\n",
       "     )\n",
       "   )\n",
       " ),\n",
       " 'backbone.stage2.1.blocks.0.conv2.depthwise_conv': Module(\n",
       "   (conv): QuantizedConv2d(\n",
       "     48, 48, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=48, bias=False\n",
       "     (param_quantizers): ModuleDict(\n",
       "       (weight): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=True)\n",
       "     )\n",
       "     (input_quantizers): ModuleList(\n",
       "       (0): None\n",
       "     )\n",
       "     (output_quantizers): ModuleList(\n",
       "       (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "     )\n",
       "   )\n",
       "   (bn): Module(\n",
       "     (module_batch_norm_14): BatchNorm()\n",
       "   )\n",
       "   (activate): CustomSiLU(\n",
       "     (sigmoid): QuantizedSigmoid(\n",
       "       (param_quantizers): ModuleDict()\n",
       "       (input_quantizers): ModuleList(\n",
       "         (0): None\n",
       "       )\n",
       "       (output_quantizers): ModuleList(\n",
       "         (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "       )\n",
       "     )\n",
       "     (mul): QuantizedMultiply(\n",
       "       (param_quantizers): ModuleDict()\n",
       "       (input_quantizers): ModuleList(\n",
       "         (0-1): 2 x None\n",
       "       )\n",
       "       (output_quantizers): ModuleList(\n",
       "         (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "       )\n",
       "     )\n",
       "   )\n",
       " ),\n",
       " 'backbone.stage2.1.blocks.0.conv2.depthwise_conv.conv': QuantizedConv2d(\n",
       "   48, 48, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=48, bias=False\n",
       "   (param_quantizers): ModuleDict(\n",
       "     (weight): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=True)\n",
       "   )\n",
       "   (input_quantizers): ModuleList(\n",
       "     (0): None\n",
       "   )\n",
       "   (output_quantizers): ModuleList(\n",
       "     (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "   )\n",
       " ),\n",
       " 'backbone.stage2.1.blocks.0.conv2.depthwise_conv.conv.param_quantizers': ModuleDict(\n",
       "   (weight): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=True)\n",
       " ),\n",
       " 'backbone.stage2.1.blocks.0.conv2.depthwise_conv.conv.param_quantizers.weight': QuantizeDequantize(shape=[1], bitwidth=8, symmetric=True),\n",
       " 'backbone.stage2.1.blocks.0.conv2.depthwise_conv.conv.input_quantizers': ModuleList(\n",
       "   (0): None\n",
       " ),\n",
       " 'backbone.stage2.1.blocks.0.conv2.depthwise_conv.conv.output_quantizers': ModuleList(\n",
       "   (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       " ),\n",
       " 'backbone.stage2.1.blocks.0.conv2.depthwise_conv.conv.output_quantizers.0': QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False),\n",
       " 'backbone.stage2.1.blocks.0.conv2.depthwise_conv.bn': Module(\n",
       "   (module_batch_norm_14): BatchNorm()\n",
       " ),\n",
       " 'backbone.stage2.1.blocks.0.conv2.depthwise_conv.bn.module_batch_norm_14': BatchNorm(),\n",
       " 'backbone.stage2.1.blocks.0.conv2.depthwise_conv.activate': CustomSiLU(\n",
       "   (sigmoid): QuantizedSigmoid(\n",
       "     (param_quantizers): ModuleDict()\n",
       "     (input_quantizers): ModuleList(\n",
       "       (0): None\n",
       "     )\n",
       "     (output_quantizers): ModuleList(\n",
       "       (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "     )\n",
       "   )\n",
       "   (mul): QuantizedMultiply(\n",
       "     (param_quantizers): ModuleDict()\n",
       "     (input_quantizers): ModuleList(\n",
       "       (0-1): 2 x None\n",
       "     )\n",
       "     (output_quantizers): ModuleList(\n",
       "       (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "     )\n",
       "   )\n",
       " ),\n",
       " 'backbone.stage2.1.blocks.0.conv2.depthwise_conv.activate.sigmoid': QuantizedSigmoid(\n",
       "   (param_quantizers): ModuleDict()\n",
       "   (input_quantizers): ModuleList(\n",
       "     (0): None\n",
       "   )\n",
       "   (output_quantizers): ModuleList(\n",
       "     (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "   )\n",
       " ),\n",
       " 'backbone.stage2.1.blocks.0.conv2.depthwise_conv.activate.sigmoid.param_quantizers': ModuleDict(),\n",
       " 'backbone.stage2.1.blocks.0.conv2.depthwise_conv.activate.sigmoid.input_quantizers': ModuleList(\n",
       "   (0): None\n",
       " ),\n",
       " 'backbone.stage2.1.blocks.0.conv2.depthwise_conv.activate.sigmoid.output_quantizers': ModuleList(\n",
       "   (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       " ),\n",
       " 'backbone.stage2.1.blocks.0.conv2.depthwise_conv.activate.sigmoid.output_quantizers.0': QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False),\n",
       " 'backbone.stage2.1.blocks.0.conv2.depthwise_conv.activate.mul': QuantizedMultiply(\n",
       "   (param_quantizers): ModuleDict()\n",
       "   (input_quantizers): ModuleList(\n",
       "     (0-1): 2 x None\n",
       "   )\n",
       "   (output_quantizers): ModuleList(\n",
       "     (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "   )\n",
       " ),\n",
       " 'backbone.stage2.1.blocks.0.conv2.depthwise_conv.activate.mul.param_quantizers': ModuleDict(),\n",
       " 'backbone.stage2.1.blocks.0.conv2.depthwise_conv.activate.mul.input_quantizers': ModuleList(\n",
       "   (0-1): 2 x None\n",
       " ),\n",
       " 'backbone.stage2.1.blocks.0.conv2.depthwise_conv.activate.mul.output_quantizers': ModuleList(\n",
       "   (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       " ),\n",
       " 'backbone.stage2.1.blocks.0.conv2.depthwise_conv.activate.mul.output_quantizers.0': QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False),\n",
       " 'backbone.stage2.1.blocks.0.conv2.pointwise_conv': Module(\n",
       "   (conv): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "   (bn): Module(\n",
       "     (module_batch_norm_15): FakeQuantizedBatchNorm(\n",
       "       (param_quantizers): ModuleDict()\n",
       "       (input_quantizers): ModuleList(\n",
       "         (0-4): 5 x QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "       )\n",
       "       (output_quantizers): ModuleList(\n",
       "         (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "       )\n",
       "     )\n",
       "   )\n",
       "   (activate): CustomSiLU(\n",
       "     (sigmoid): QuantizedSigmoid(\n",
       "       (param_quantizers): ModuleDict()\n",
       "       (input_quantizers): ModuleList(\n",
       "         (0): None\n",
       "       )\n",
       "       (output_quantizers): ModuleList(\n",
       "         (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "       )\n",
       "     )\n",
       "     (mul): QuantizedMultiply(\n",
       "       (param_quantizers): ModuleDict()\n",
       "       (input_quantizers): ModuleList(\n",
       "         (0-1): 2 x None\n",
       "       )\n",
       "       (output_quantizers): ModuleList(\n",
       "         (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "       )\n",
       "     )\n",
       "   )\n",
       " ),\n",
       " 'backbone.stage2.1.blocks.0.conv2.pointwise_conv.conv': Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False),\n",
       " 'backbone.stage2.1.blocks.0.conv2.pointwise_conv.bn': Module(\n",
       "   (module_batch_norm_15): FakeQuantizedBatchNorm(\n",
       "     (param_quantizers): ModuleDict()\n",
       "     (input_quantizers): ModuleList(\n",
       "       (0-4): 5 x QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "     )\n",
       "     (output_quantizers): ModuleList(\n",
       "       (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "     )\n",
       "   )\n",
       " ),\n",
       " 'backbone.stage2.1.blocks.0.conv2.pointwise_conv.bn.module_batch_norm_15': FakeQuantizedBatchNorm(\n",
       "   (param_quantizers): ModuleDict()\n",
       "   (input_quantizers): ModuleList(\n",
       "     (0-4): 5 x QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "   )\n",
       "   (output_quantizers): ModuleList(\n",
       "     (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "   )\n",
       " ),\n",
       " 'backbone.stage2.1.blocks.0.conv2.pointwise_conv.bn.module_batch_norm_15.param_quantizers': ModuleDict(),\n",
       " 'backbone.stage2.1.blocks.0.conv2.pointwise_conv.bn.module_batch_norm_15.input_quantizers': ModuleList(\n",
       "   (0-4): 5 x QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       " ),\n",
       " 'backbone.stage2.1.blocks.0.conv2.pointwise_conv.bn.module_batch_norm_15.input_quantizers.0': QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False),\n",
       " 'backbone.stage2.1.blocks.0.conv2.pointwise_conv.bn.module_batch_norm_15.input_quantizers.1': QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False),\n",
       " 'backbone.stage2.1.blocks.0.conv2.pointwise_conv.bn.module_batch_norm_15.input_quantizers.2': QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False),\n",
       " 'backbone.stage2.1.blocks.0.conv2.pointwise_conv.bn.module_batch_norm_15.input_quantizers.3': QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False),\n",
       " 'backbone.stage2.1.blocks.0.conv2.pointwise_conv.bn.module_batch_norm_15.input_quantizers.4': QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False),\n",
       " 'backbone.stage2.1.blocks.0.conv2.pointwise_conv.bn.module_batch_norm_15.output_quantizers': ModuleList(\n",
       "   (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       " ),\n",
       " 'backbone.stage2.1.blocks.0.conv2.pointwise_conv.bn.module_batch_norm_15.output_quantizers.0': QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False),\n",
       " 'backbone.stage2.1.blocks.0.conv2.pointwise_conv.activate': CustomSiLU(\n",
       "   (sigmoid): QuantizedSigmoid(\n",
       "     (param_quantizers): ModuleDict()\n",
       "     (input_quantizers): ModuleList(\n",
       "       (0): None\n",
       "     )\n",
       "     (output_quantizers): ModuleList(\n",
       "       (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "     )\n",
       "   )\n",
       "   (mul): QuantizedMultiply(\n",
       "     (param_quantizers): ModuleDict()\n",
       "     (input_quantizers): ModuleList(\n",
       "       (0-1): 2 x None\n",
       "     )\n",
       "     (output_quantizers): ModuleList(\n",
       "       (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "     )\n",
       "   )\n",
       " ),\n",
       " 'backbone.stage2.1.blocks.0.conv2.pointwise_conv.activate.sigmoid': QuantizedSigmoid(\n",
       "   (param_quantizers): ModuleDict()\n",
       "   (input_quantizers): ModuleList(\n",
       "     (0): None\n",
       "   )\n",
       "   (output_quantizers): ModuleList(\n",
       "     (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "   )\n",
       " ),\n",
       " 'backbone.stage2.1.blocks.0.conv2.pointwise_conv.activate.sigmoid.param_quantizers': ModuleDict(),\n",
       " 'backbone.stage2.1.blocks.0.conv2.pointwise_conv.activate.sigmoid.input_quantizers': ModuleList(\n",
       "   (0): None\n",
       " ),\n",
       " 'backbone.stage2.1.blocks.0.conv2.pointwise_conv.activate.sigmoid.output_quantizers': ModuleList(\n",
       "   (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       " ),\n",
       " 'backbone.stage2.1.blocks.0.conv2.pointwise_conv.activate.sigmoid.output_quantizers.0': QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False),\n",
       " 'backbone.stage2.1.blocks.0.conv2.pointwise_conv.activate.mul': QuantizedMultiply(\n",
       "   (param_quantizers): ModuleDict()\n",
       "   (input_quantizers): ModuleList(\n",
       "     (0-1): 2 x None\n",
       "   )\n",
       "   (output_quantizers): ModuleList(\n",
       "     (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "   )\n",
       " ),\n",
       " 'backbone.stage2.1.blocks.0.conv2.pointwise_conv.activate.mul.param_quantizers': ModuleDict(),\n",
       " 'backbone.stage2.1.blocks.0.conv2.pointwise_conv.activate.mul.input_quantizers': ModuleList(\n",
       "   (0-1): 2 x None\n",
       " ),\n",
       " 'backbone.stage2.1.blocks.0.conv2.pointwise_conv.activate.mul.output_quantizers': ModuleList(\n",
       "   (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       " ),\n",
       " 'backbone.stage2.1.blocks.0.conv2.pointwise_conv.activate.mul.output_quantizers.0': QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False),\n",
       " 'backbone.stage2.1.blocks.0.module_add_1': QuantizedAdd(\n",
       "   (param_quantizers): ModuleDict()\n",
       "   (input_quantizers): ModuleList(\n",
       "     (0-1): 2 x None\n",
       "   )\n",
       "   (output_quantizers): ModuleList(\n",
       "     (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "   )\n",
       " ),\n",
       " 'backbone.stage2.1.blocks.0.module_add_1.param_quantizers': ModuleDict(),\n",
       " 'backbone.stage2.1.blocks.0.module_add_1.input_quantizers': ModuleList(\n",
       "   (0-1): 2 x None\n",
       " ),\n",
       " 'backbone.stage2.1.blocks.0.module_add_1.output_quantizers': ModuleList(\n",
       "   (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       " ),\n",
       " 'backbone.stage2.1.blocks.0.module_add_1.output_quantizers.0': QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False),\n",
       " 'backbone.stage2.1.attention': Module(\n",
       "   (global_avgpool): FakeQuantizedAdaptiveAvgPool2d(\n",
       "     output_size=1\n",
       "     (param_quantizers): ModuleDict()\n",
       "     (input_quantizers): ModuleList(\n",
       "       (0): None\n",
       "     )\n",
       "     (output_quantizers): ModuleList(\n",
       "       (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "     )\n",
       "   )\n",
       "   (fc): QuantizedConv2d(\n",
       "     96, 96, kernel_size=(1, 1), stride=(1, 1)\n",
       "     (param_quantizers): ModuleDict(\n",
       "       (weight): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=True)\n",
       "       (bias): None\n",
       "     )\n",
       "     (input_quantizers): ModuleList(\n",
       "       (0): None\n",
       "     )\n",
       "     (output_quantizers): ModuleList(\n",
       "       (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "     )\n",
       "   )\n",
       "   (act): FakeQuantizedHardsigmoid(\n",
       "     (param_quantizers): ModuleDict()\n",
       "     (input_quantizers): ModuleList(\n",
       "       (0): None\n",
       "     )\n",
       "     (output_quantizers): ModuleList(\n",
       "       (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "     )\n",
       "   )\n",
       "   (module_mul_1): QuantizedMultiply(\n",
       "     (param_quantizers): ModuleDict()\n",
       "     (input_quantizers): ModuleList(\n",
       "       (0-1): 2 x None\n",
       "     )\n",
       "     (output_quantizers): ModuleList(\n",
       "       (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "     )\n",
       "   )\n",
       " ),\n",
       " 'backbone.stage2.1.attention.global_avgpool': FakeQuantizedAdaptiveAvgPool2d(\n",
       "   output_size=1\n",
       "   (param_quantizers): ModuleDict()\n",
       "   (input_quantizers): ModuleList(\n",
       "     (0): None\n",
       "   )\n",
       "   (output_quantizers): ModuleList(\n",
       "     (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "   )\n",
       " ),\n",
       " 'backbone.stage2.1.attention.global_avgpool.param_quantizers': ModuleDict(),\n",
       " 'backbone.stage2.1.attention.global_avgpool.input_quantizers': ModuleList(\n",
       "   (0): None\n",
       " ),\n",
       " 'backbone.stage2.1.attention.global_avgpool.output_quantizers': ModuleList(\n",
       "   (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       " ),\n",
       " 'backbone.stage2.1.attention.global_avgpool.output_quantizers.0': QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False),\n",
       " 'backbone.stage2.1.attention.fc': QuantizedConv2d(\n",
       "   96, 96, kernel_size=(1, 1), stride=(1, 1)\n",
       "   (param_quantizers): ModuleDict(\n",
       "     (weight): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=True)\n",
       "     (bias): None\n",
       "   )\n",
       "   (input_quantizers): ModuleList(\n",
       "     (0): None\n",
       "   )\n",
       "   (output_quantizers): ModuleList(\n",
       "     (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "   )\n",
       " ),\n",
       " 'backbone.stage2.1.attention.fc.param_quantizers': ModuleDict(\n",
       "   (weight): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=True)\n",
       "   (bias): None\n",
       " ),\n",
       " 'backbone.stage2.1.attention.fc.param_quantizers.weight': QuantizeDequantize(shape=[1], bitwidth=8, symmetric=True),\n",
       " 'backbone.stage2.1.attention.fc.input_quantizers': ModuleList(\n",
       "   (0): None\n",
       " ),\n",
       " 'backbone.stage2.1.attention.fc.output_quantizers': ModuleList(\n",
       "   (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       " ),\n",
       " 'backbone.stage2.1.attention.fc.output_quantizers.0': QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False),\n",
       " 'backbone.stage2.1.attention.act': FakeQuantizedHardsigmoid(\n",
       "   (param_quantizers): ModuleDict()\n",
       "   (input_quantizers): ModuleList(\n",
       "     (0): None\n",
       "   )\n",
       "   (output_quantizers): ModuleList(\n",
       "     (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "   )\n",
       " ),\n",
       " 'backbone.stage2.1.attention.act.param_quantizers': ModuleDict(),\n",
       " 'backbone.stage2.1.attention.act.input_quantizers': ModuleList(\n",
       "   (0): None\n",
       " ),\n",
       " 'backbone.stage2.1.attention.act.output_quantizers': ModuleList(\n",
       "   (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       " ),\n",
       " 'backbone.stage2.1.attention.act.output_quantizers.0': QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False),\n",
       " 'backbone.stage2.1.attention.module_mul_1': QuantizedMultiply(\n",
       "   (param_quantizers): ModuleDict()\n",
       "   (input_quantizers): ModuleList(\n",
       "     (0-1): 2 x None\n",
       "   )\n",
       "   (output_quantizers): ModuleList(\n",
       "     (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "   )\n",
       " ),\n",
       " 'backbone.stage2.1.attention.module_mul_1.param_quantizers': ModuleDict(),\n",
       " 'backbone.stage2.1.attention.module_mul_1.input_quantizers': ModuleList(\n",
       "   (0-1): 2 x None\n",
       " ),\n",
       " 'backbone.stage2.1.attention.module_mul_1.output_quantizers': ModuleList(\n",
       "   (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       " ),\n",
       " 'backbone.stage2.1.attention.module_mul_1.output_quantizers.0': QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False),\n",
       " 'backbone.stage2.1.final_conv': Module(\n",
       "   (conv): QuantizedConv2d(\n",
       "     96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "     (param_quantizers): ModuleDict(\n",
       "       (weight): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=True)\n",
       "     )\n",
       "     (input_quantizers): ModuleList(\n",
       "       (0): None\n",
       "     )\n",
       "     (output_quantizers): ModuleList(\n",
       "       (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "     )\n",
       "   )\n",
       "   (bn): Module(\n",
       "     (module_batch_norm_16): FakeQuantizedBatchNorm(\n",
       "       (param_quantizers): ModuleDict()\n",
       "       (input_quantizers): ModuleList(\n",
       "         (0-4): 5 x QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "       )\n",
       "       (output_quantizers): ModuleList(\n",
       "         (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "       )\n",
       "     )\n",
       "   )\n",
       "   (activate): CustomSiLU(\n",
       "     (sigmoid): QuantizedSigmoid(\n",
       "       (param_quantizers): ModuleDict()\n",
       "       (input_quantizers): ModuleList(\n",
       "         (0): None\n",
       "       )\n",
       "       (output_quantizers): ModuleList(\n",
       "         (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "       )\n",
       "     )\n",
       "     (mul): QuantizedMultiply(\n",
       "       (param_quantizers): ModuleDict()\n",
       "       (input_quantizers): ModuleList(\n",
       "         (0-1): 2 x None\n",
       "       )\n",
       "       (output_quantizers): ModuleList(\n",
       "         (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "       )\n",
       "     )\n",
       "   )\n",
       " ),\n",
       " 'backbone.stage2.1.final_conv.conv': QuantizedConv2d(\n",
       "   96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "   (param_quantizers): ModuleDict(\n",
       "     (weight): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=True)\n",
       "   )\n",
       "   (input_quantizers): ModuleList(\n",
       "     (0): None\n",
       "   )\n",
       "   (output_quantizers): ModuleList(\n",
       "     (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "   )\n",
       " ),\n",
       " 'backbone.stage2.1.final_conv.conv.param_quantizers': ModuleDict(\n",
       "   (weight): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=True)\n",
       " ),\n",
       " 'backbone.stage2.1.final_conv.conv.param_quantizers.weight': QuantizeDequantize(shape=[1], bitwidth=8, symmetric=True),\n",
       " 'backbone.stage2.1.final_conv.conv.input_quantizers': ModuleList(\n",
       "   (0): None\n",
       " ),\n",
       " 'backbone.stage2.1.final_conv.conv.output_quantizers': ModuleList(\n",
       "   (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       " ),\n",
       " 'backbone.stage2.1.final_conv.conv.output_quantizers.0': QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False),\n",
       " 'backbone.stage2.1.final_conv.bn': Module(\n",
       "   (module_batch_norm_16): FakeQuantizedBatchNorm(\n",
       "     (param_quantizers): ModuleDict()\n",
       "     (input_quantizers): ModuleList(\n",
       "       (0-4): 5 x QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "     )\n",
       "     (output_quantizers): ModuleList(\n",
       "       (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "     )\n",
       "   )\n",
       " ),\n",
       " 'backbone.stage2.1.final_conv.bn.module_batch_norm_16': FakeQuantizedBatchNorm(\n",
       "   (param_quantizers): ModuleDict()\n",
       "   (input_quantizers): ModuleList(\n",
       "     (0-4): 5 x QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "   )\n",
       "   (output_quantizers): ModuleList(\n",
       "     (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "   )\n",
       " ),\n",
       " 'backbone.stage2.1.final_conv.bn.module_batch_norm_16.param_quantizers': ModuleDict(),\n",
       " 'backbone.stage2.1.final_conv.bn.module_batch_norm_16.input_quantizers': ModuleList(\n",
       "   (0-4): 5 x QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       " ),\n",
       " 'backbone.stage2.1.final_conv.bn.module_batch_norm_16.input_quantizers.0': QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False),\n",
       " 'backbone.stage2.1.final_conv.bn.module_batch_norm_16.input_quantizers.1': QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False),\n",
       " 'backbone.stage2.1.final_conv.bn.module_batch_norm_16.input_quantizers.2': QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False),\n",
       " 'backbone.stage2.1.final_conv.bn.module_batch_norm_16.input_quantizers.3': QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False),\n",
       " 'backbone.stage2.1.final_conv.bn.module_batch_norm_16.input_quantizers.4': QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False),\n",
       " 'backbone.stage2.1.final_conv.bn.module_batch_norm_16.output_quantizers': ModuleList(\n",
       "   (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       " ),\n",
       " 'backbone.stage2.1.final_conv.bn.module_batch_norm_16.output_quantizers.0': QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False),\n",
       " 'backbone.stage2.1.final_conv.activate': CustomSiLU(\n",
       "   (sigmoid): QuantizedSigmoid(\n",
       "     (param_quantizers): ModuleDict()\n",
       "     (input_quantizers): ModuleList(\n",
       "       (0): None\n",
       "     )\n",
       "     (output_quantizers): ModuleList(\n",
       "       (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "     )\n",
       "   )\n",
       "   (mul): QuantizedMultiply(\n",
       "     (param_quantizers): ModuleDict()\n",
       "     (input_quantizers): ModuleList(\n",
       "       (0-1): 2 x None\n",
       "     )\n",
       "     (output_quantizers): ModuleList(\n",
       "       (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "     )\n",
       "   )\n",
       " ),\n",
       " 'backbone.stage2.1.final_conv.activate.sigmoid': QuantizedSigmoid(\n",
       "   (param_quantizers): ModuleDict()\n",
       "   (input_quantizers): ModuleList(\n",
       "     (0): None\n",
       "   )\n",
       "   (output_quantizers): ModuleList(\n",
       "     (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "   )\n",
       " ),\n",
       " 'backbone.stage2.1.final_conv.activate.sigmoid.param_quantizers': ModuleDict(),\n",
       " 'backbone.stage2.1.final_conv.activate.sigmoid.input_quantizers': ModuleList(\n",
       "   (0): None\n",
       " ),\n",
       " 'backbone.stage2.1.final_conv.activate.sigmoid.output_quantizers': ModuleList(\n",
       "   (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       " ),\n",
       " 'backbone.stage2.1.final_conv.activate.sigmoid.output_quantizers.0': QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False),\n",
       " 'backbone.stage2.1.final_conv.activate.mul': QuantizedMultiply(\n",
       "   (param_quantizers): ModuleDict()\n",
       "   (input_quantizers): ModuleList(\n",
       "     (0-1): 2 x None\n",
       "   )\n",
       "   (output_quantizers): ModuleList(\n",
       "     (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "   )\n",
       " ),\n",
       " 'backbone.stage2.1.final_conv.activate.mul.param_quantizers': ModuleDict(),\n",
       " 'backbone.stage2.1.final_conv.activate.mul.input_quantizers': ModuleList(\n",
       "   (0-1): 2 x None\n",
       " ),\n",
       " 'backbone.stage2.1.final_conv.activate.mul.output_quantizers': ModuleList(\n",
       "   (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       " ),\n",
       " 'backbone.stage2.1.final_conv.activate.mul.output_quantizers.0': QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False),\n",
       " 'backbone.stage2.1.module_cat_1': FakeQuantizedConcat(\n",
       "   (param_quantizers): ModuleDict()\n",
       "   (input_quantizers): ModuleList(\n",
       "     (0): None\n",
       "   )\n",
       "   (output_quantizers): ModuleList(\n",
       "     (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "   )\n",
       " ),\n",
       " 'backbone.stage2.1.module_cat_1.param_quantizers': ModuleDict(),\n",
       " 'backbone.stage2.1.module_cat_1.input_quantizers': ModuleList(\n",
       "   (0): None\n",
       " ),\n",
       " 'backbone.stage2.1.module_cat_1.output_quantizers': ModuleList(\n",
       "   (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       " ),\n",
       " 'backbone.stage2.1.module_cat_1.output_quantizers.0': QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False),\n",
       " 'backbone.stage3': Module(\n",
       "   (0): Module(\n",
       "     (conv): QuantizedConv2d(\n",
       "       96, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False\n",
       "       (param_quantizers): ModuleDict(\n",
       "         (weight): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=True)\n",
       "       )\n",
       "       (input_quantizers): ModuleList(\n",
       "         (0): None\n",
       "       )\n",
       "       (output_quantizers): ModuleList(\n",
       "         (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "       )\n",
       "     )\n",
       "     (bn): Module(\n",
       "       (module_batch_norm_17): FakeQuantizedBatchNorm(\n",
       "         (param_quantizers): ModuleDict()\n",
       "         (input_quantizers): ModuleList(\n",
       "           (0-4): 5 x QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "         )\n",
       "         (output_quantizers): ModuleList(\n",
       "           (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "         )\n",
       "       )\n",
       "     )\n",
       "     (activate): CustomSiLU(\n",
       "       (sigmoid): QuantizedSigmoid(\n",
       "         (param_quantizers): ModuleDict()\n",
       "         (input_quantizers): ModuleList(\n",
       "           (0): None\n",
       "         )\n",
       "         (output_quantizers): ModuleList(\n",
       "           (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "         )\n",
       "       )\n",
       "       (mul): QuantizedMultiply(\n",
       "         (param_quantizers): ModuleDict()\n",
       "         (input_quantizers): ModuleList(\n",
       "           (0-1): 2 x None\n",
       "         )\n",
       "         (output_quantizers): ModuleList(\n",
       "           (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "         )\n",
       "       )\n",
       "     )\n",
       "   )\n",
       "   (1): Module(\n",
       "     (short_conv): Module(\n",
       "       (conv): QuantizedConv2d(\n",
       "         192, 96, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "         (param_quantizers): ModuleDict(\n",
       "           (weight): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=True)\n",
       "         )\n",
       "         (input_quantizers): ModuleList(\n",
       "           (0): None\n",
       "         )\n",
       "         (output_quantizers): ModuleList(\n",
       "           (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "         )\n",
       "       )\n",
       "       (bn): Module(\n",
       "         (module_batch_norm_18): FakeQuantizedBatchNorm(\n",
       "           (param_quantizers): ModuleDict()\n",
       "           (input_quantizers): ModuleList(\n",
       "             (0-4): 5 x QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "           )\n",
       "           (output_quantizers): ModuleList(\n",
       "             (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "           )\n",
       "         )\n",
       "       )\n",
       "       (activate): CustomSiLU(\n",
       "         (sigmoid): QuantizedSigmoid(\n",
       "           (param_quantizers): ModuleDict()\n",
       "           (input_quantizers): ModuleList(\n",
       "             (0): None\n",
       "           )\n",
       "           (output_quantizers): ModuleList(\n",
       "             (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "           )\n",
       "         )\n",
       "         (mul): QuantizedMultiply(\n",
       "           (param_quantizers): ModuleDict()\n",
       "           (input_quantizers): ModuleList(\n",
       "             (0-1): 2 x None\n",
       "           )\n",
       "           (output_quantizers): ModuleList(\n",
       "             (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "           )\n",
       "         )\n",
       "       )\n",
       "     )\n",
       "     (main_conv): Module(\n",
       "       (conv): QuantizedConv2d(\n",
       "         192, 96, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "         (param_quantizers): ModuleDict(\n",
       "           (weight): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=True)\n",
       "         )\n",
       "         (input_quantizers): ModuleList(\n",
       "           (0): None\n",
       "         )\n",
       "         (output_quantizers): ModuleList(\n",
       "           (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "         )\n",
       "       )\n",
       "       (bn): Module(\n",
       "         (module_batch_norm_19): FakeQuantizedBatchNorm(\n",
       "           (param_quantizers): ModuleDict()\n",
       "           (input_quantizers): ModuleList(\n",
       "             (0-4): 5 x QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "           )\n",
       "           (output_quantizers): ModuleList(\n",
       "             (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "           )\n",
       "         )\n",
       "       )\n",
       "       (activate): CustomSiLU(\n",
       "         (sigmoid): QuantizedSigmoid(\n",
       "           (param_quantizers): ModuleDict()\n",
       "           (input_quantizers): ModuleList(\n",
       "             (0): None\n",
       "           )\n",
       "           (output_quantizers): ModuleList(\n",
       "             (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "           )\n",
       "         )\n",
       "         (mul): QuantizedMultiply(\n",
       "           (param_quantizers): ModuleDict()\n",
       "           (input_quantizers): ModuleList(\n",
       "             (0-1): 2 x None\n",
       "           )\n",
       "           (output_quantizers): ModuleList(\n",
       "             (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "           )\n",
       "         )\n",
       "       )\n",
       "     )\n",
       "     (blocks): Module(\n",
       "       (0): Module(\n",
       "         (conv1): Module(\n",
       "           (conv): QuantizedConv2d(\n",
       "             96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "             (param_quantizers): ModuleDict(\n",
       "               (weight): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=True)\n",
       "             )\n",
       "             (input_quantizers): ModuleList(\n",
       "               (0): None\n",
       "             )\n",
       "             (output_quantizers): ModuleList(\n",
       "               (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "             )\n",
       "           )\n",
       "           (bn): Module(\n",
       "             (module_batch_norm_20): FakeQuantizedBatchNorm(\n",
       "               (param_quantizers): ModuleDict()\n",
       "               (input_quantizers): ModuleList(\n",
       "                 (0-4): 5 x QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "               )\n",
       "               (output_quantizers): ModuleList(\n",
       "                 (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "               )\n",
       "             )\n",
       "           )\n",
       "           (activate): CustomSiLU(\n",
       "             (sigmoid): QuantizedSigmoid(\n",
       "               (param_quantizers): ModuleDict()\n",
       "               (input_quantizers): ModuleList(\n",
       "                 (0): None\n",
       "               )\n",
       "               (output_quantizers): ModuleList(\n",
       "                 (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "               )\n",
       "             )\n",
       "             (mul): QuantizedMultiply(\n",
       "               (param_quantizers): ModuleDict()\n",
       "               (input_quantizers): ModuleList(\n",
       "                 (0-1): 2 x None\n",
       "               )\n",
       "               (output_quantizers): ModuleList(\n",
       "                 (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "               )\n",
       "             )\n",
       "           )\n",
       "         )\n",
       "         (conv2): Module(\n",
       "           (depthwise_conv): Module(\n",
       "             (conv): QuantizedConv2d(\n",
       "               96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96, bias=False\n",
       "               (param_quantizers): ModuleDict(\n",
       "                 (weight): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=True)\n",
       "               )\n",
       "               (input_quantizers): ModuleList(\n",
       "                 (0): None\n",
       "               )\n",
       "               (output_quantizers): ModuleList(\n",
       "                 (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "               )\n",
       "             )\n",
       "             (bn): Module(\n",
       "               (module_batch_norm_21): BatchNorm()\n",
       "             )\n",
       "             (activate): CustomSiLU(\n",
       "               (sigmoid): QuantizedSigmoid(\n",
       "                 (param_quantizers): ModuleDict()\n",
       "                 (input_quantizers): ModuleList(\n",
       "                   (0): None\n",
       "                 )\n",
       "                 (output_quantizers): ModuleList(\n",
       "                   (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "                 )\n",
       "               )\n",
       "               (mul): QuantizedMultiply(\n",
       "                 (param_quantizers): ModuleDict()\n",
       "                 (input_quantizers): ModuleList(\n",
       "                   (0-1): 2 x None\n",
       "                 )\n",
       "                 (output_quantizers): ModuleList(\n",
       "                   (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "                 )\n",
       "               )\n",
       "             )\n",
       "           )\n",
       "           (pointwise_conv): Module(\n",
       "             (conv): QuantizedConv2d(\n",
       "               96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "               (param_quantizers): ModuleDict(\n",
       "                 (weight): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=True)\n",
       "               )\n",
       "               (input_quantizers): ModuleList(\n",
       "                 (0): None\n",
       "               )\n",
       "               (output_quantizers): ModuleList(\n",
       "                 (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "               )\n",
       "             )\n",
       "             (bn): Module(\n",
       "               (module_batch_norm_22): FakeQuantizedBatchNorm(\n",
       "                 (param_quantizers): ModuleDict()\n",
       "                 (input_quantizers): ModuleList(\n",
       "                   (0-4): 5 x QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "                 )\n",
       "                 (output_quantizers): ModuleList(\n",
       "                   (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "                 )\n",
       "               )\n",
       "             )\n",
       "             (activate): CustomSiLU(\n",
       "               (sigmoid): QuantizedSigmoid(\n",
       "                 (param_quantizers): ModuleDict()\n",
       "                 (input_quantizers): ModuleList(\n",
       "                   (0): None\n",
       "                 )\n",
       "                 (output_quantizers): ModuleList(\n",
       "                   (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "                 )\n",
       "               )\n",
       "               (mul): QuantizedMultiply(\n",
       "                 (param_quantizers): ModuleDict()\n",
       "                 (input_quantizers): ModuleList(\n",
       "                   (0-1): 2 x None\n",
       "                 )\n",
       "                 (output_quantizers): ModuleList(\n",
       "                   (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "                 )\n",
       "               )\n",
       "             )\n",
       "           )\n",
       "         )\n",
       "         (module_add_2): QuantizedAdd(\n",
       "           (param_quantizers): ModuleDict()\n",
       "           (input_quantizers): ModuleList(\n",
       "             (0-1): 2 x None\n",
       "           )\n",
       "           (output_quantizers): ModuleList(\n",
       "             (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "           )\n",
       "         )\n",
       "       )\n",
       "     )\n",
       "     (attention): Module(\n",
       "       (global_avgpool): FakeQuantizedAdaptiveAvgPool2d(\n",
       "         output_size=1\n",
       "         (param_quantizers): ModuleDict()\n",
       "         (input_quantizers): ModuleList(\n",
       "           (0): None\n",
       "         )\n",
       "         (output_quantizers): ModuleList(\n",
       "           (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "         )\n",
       "       )\n",
       "       (fc): QuantizedConv2d(\n",
       "         192, 192, kernel_size=(1, 1), stride=(1, 1)\n",
       "         (param_quantizers): ModuleDict(\n",
       "           (weight): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=True)\n",
       "           (bias): None\n",
       "         )\n",
       "         (input_quantizers): ModuleList(\n",
       "           (0): None\n",
       "         )\n",
       "         (output_quantizers): ModuleList(\n",
       "           (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "         )\n",
       "       )\n",
       "       (act): FakeQuantizedHardsigmoid(\n",
       "         (param_quantizers): ModuleDict()\n",
       "         (input_quantizers): ModuleList(\n",
       "           (0): None\n",
       "         )\n",
       "         (output_quantizers): ModuleList(\n",
       "           (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "         )\n",
       "       )\n",
       "       (module_mul_2): QuantizedMultiply(\n",
       "         (param_quantizers): ModuleDict()\n",
       "         (input_quantizers): ModuleList(\n",
       "           (0-1): 2 x None\n",
       "         )\n",
       "         (output_quantizers): ModuleList(\n",
       "           (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "         )\n",
       "       )\n",
       "     )\n",
       "     (final_conv): Module(\n",
       "       (conv): QuantizedConv2d(\n",
       "         192, 192, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "         (param_quantizers): ModuleDict(\n",
       "           (weight): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=True)\n",
       "         )\n",
       "         (input_quantizers): ModuleList(\n",
       "           (0): None\n",
       "         )\n",
       "         (output_quantizers): ModuleList(\n",
       "           (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "         )\n",
       "       )\n",
       "       (bn): Module(\n",
       "         (module_batch_norm_23): FakeQuantizedBatchNorm(\n",
       "           (param_quantizers): ModuleDict()\n",
       "           (input_quantizers): ModuleList(\n",
       "             (0-4): 5 x QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "           )\n",
       "           (output_quantizers): ModuleList(\n",
       "             (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "           )\n",
       "         )\n",
       "       )\n",
       "       (activate): CustomSiLU(\n",
       "         (sigmoid): QuantizedSigmoid(\n",
       "           (param_quantizers): ModuleDict()\n",
       "           (input_quantizers): ModuleList(\n",
       "             (0): None\n",
       "           )\n",
       "           (output_quantizers): ModuleList(\n",
       "             (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "           )\n",
       "         )\n",
       "         (mul): QuantizedMultiply(\n",
       "           (param_quantizers): ModuleDict()\n",
       "           (input_quantizers): ModuleList(\n",
       "             (0-1): 2 x None\n",
       "           )\n",
       "           (output_quantizers): ModuleList(\n",
       "             (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "           )\n",
       "         )\n",
       "       )\n",
       "     )\n",
       "     (module_cat_2): FakeQuantizedConcat(\n",
       "       (param_quantizers): ModuleDict()\n",
       "       (input_quantizers): ModuleList(\n",
       "         (0): None\n",
       "       )\n",
       "       (output_quantizers): ModuleList(\n",
       "         (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "       )\n",
       "     )\n",
       "   )\n",
       " ),\n",
       " 'backbone.stage3.0': Module(\n",
       "   (conv): QuantizedConv2d(\n",
       "     96, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False\n",
       "     (param_quantizers): ModuleDict(\n",
       "       (weight): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=True)\n",
       "     )\n",
       "     (input_quantizers): ModuleList(\n",
       "       (0): None\n",
       "     )\n",
       "     (output_quantizers): ModuleList(\n",
       "       (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "     )\n",
       "   )\n",
       "   (bn): Module(\n",
       "     (module_batch_norm_17): FakeQuantizedBatchNorm(\n",
       "       (param_quantizers): ModuleDict()\n",
       "       (input_quantizers): ModuleList(\n",
       "         (0-4): 5 x QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "       )\n",
       "       (output_quantizers): ModuleList(\n",
       "         (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "       )\n",
       "     )\n",
       "   )\n",
       "   (activate): CustomSiLU(\n",
       "     (sigmoid): QuantizedSigmoid(\n",
       "       (param_quantizers): ModuleDict()\n",
       "       (input_quantizers): ModuleList(\n",
       "         (0): None\n",
       "       )\n",
       "       (output_quantizers): ModuleList(\n",
       "         (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "       )\n",
       "     )\n",
       "     (mul): QuantizedMultiply(\n",
       "       (param_quantizers): ModuleDict()\n",
       "       (input_quantizers): ModuleList(\n",
       "         (0-1): 2 x None\n",
       "       )\n",
       "       (output_quantizers): ModuleList(\n",
       "         (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "       )\n",
       "     )\n",
       "   )\n",
       " ),\n",
       " 'backbone.stage3.0.conv': QuantizedConv2d(\n",
       "   96, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False\n",
       "   (param_quantizers): ModuleDict(\n",
       "     (weight): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=True)\n",
       "   )\n",
       "   (input_quantizers): ModuleList(\n",
       "     (0): None\n",
       "   )\n",
       "   (output_quantizers): ModuleList(\n",
       "     (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "   )\n",
       " ),\n",
       " 'backbone.stage3.0.conv.param_quantizers': ModuleDict(\n",
       "   (weight): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=True)\n",
       " ),\n",
       " 'backbone.stage3.0.conv.param_quantizers.weight': QuantizeDequantize(shape=[1], bitwidth=8, symmetric=True),\n",
       " 'backbone.stage3.0.conv.input_quantizers': ModuleList(\n",
       "   (0): None\n",
       " ),\n",
       " 'backbone.stage3.0.conv.output_quantizers': ModuleList(\n",
       "   (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       " ),\n",
       " 'backbone.stage3.0.conv.output_quantizers.0': QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False),\n",
       " 'backbone.stage3.0.bn': Module(\n",
       "   (module_batch_norm_17): FakeQuantizedBatchNorm(\n",
       "     (param_quantizers): ModuleDict()\n",
       "     (input_quantizers): ModuleList(\n",
       "       (0-4): 5 x QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "     )\n",
       "     (output_quantizers): ModuleList(\n",
       "       (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "     )\n",
       "   )\n",
       " ),\n",
       " 'backbone.stage3.0.bn.module_batch_norm_17': FakeQuantizedBatchNorm(\n",
       "   (param_quantizers): ModuleDict()\n",
       "   (input_quantizers): ModuleList(\n",
       "     (0-4): 5 x QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "   )\n",
       "   (output_quantizers): ModuleList(\n",
       "     (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "   )\n",
       " ),\n",
       " 'backbone.stage3.0.bn.module_batch_norm_17.param_quantizers': ModuleDict(),\n",
       " 'backbone.stage3.0.bn.module_batch_norm_17.input_quantizers': ModuleList(\n",
       "   (0-4): 5 x QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       " ),\n",
       " 'backbone.stage3.0.bn.module_batch_norm_17.input_quantizers.0': QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False),\n",
       " 'backbone.stage3.0.bn.module_batch_norm_17.input_quantizers.1': QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False),\n",
       " 'backbone.stage3.0.bn.module_batch_norm_17.input_quantizers.2': QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False),\n",
       " 'backbone.stage3.0.bn.module_batch_norm_17.input_quantizers.3': QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False),\n",
       " 'backbone.stage3.0.bn.module_batch_norm_17.input_quantizers.4': QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False),\n",
       " 'backbone.stage3.0.bn.module_batch_norm_17.output_quantizers': ModuleList(\n",
       "   (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       " ),\n",
       " 'backbone.stage3.0.bn.module_batch_norm_17.output_quantizers.0': QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False),\n",
       " 'backbone.stage3.0.activate': CustomSiLU(\n",
       "   (sigmoid): QuantizedSigmoid(\n",
       "     (param_quantizers): ModuleDict()\n",
       "     (input_quantizers): ModuleList(\n",
       "       (0): None\n",
       "     )\n",
       "     (output_quantizers): ModuleList(\n",
       "       (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "     )\n",
       "   )\n",
       "   (mul): QuantizedMultiply(\n",
       "     (param_quantizers): ModuleDict()\n",
       "     (input_quantizers): ModuleList(\n",
       "       (0-1): 2 x None\n",
       "     )\n",
       "     (output_quantizers): ModuleList(\n",
       "       (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "     )\n",
       "   )\n",
       " ),\n",
       " 'backbone.stage3.0.activate.sigmoid': QuantizedSigmoid(\n",
       "   (param_quantizers): ModuleDict()\n",
       "   (input_quantizers): ModuleList(\n",
       "     (0): None\n",
       "   )\n",
       "   (output_quantizers): ModuleList(\n",
       "     (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "   )\n",
       " ),\n",
       " 'backbone.stage3.0.activate.sigmoid.param_quantizers': ModuleDict(),\n",
       " 'backbone.stage3.0.activate.sigmoid.input_quantizers': ModuleList(\n",
       "   (0): None\n",
       " ),\n",
       " 'backbone.stage3.0.activate.sigmoid.output_quantizers': ModuleList(\n",
       "   (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       " ),\n",
       " 'backbone.stage3.0.activate.sigmoid.output_quantizers.0': QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False),\n",
       " 'backbone.stage3.0.activate.mul': QuantizedMultiply(\n",
       "   (param_quantizers): ModuleDict()\n",
       "   (input_quantizers): ModuleList(\n",
       "     (0-1): 2 x None\n",
       "   )\n",
       "   (output_quantizers): ModuleList(\n",
       "     (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "   )\n",
       " ),\n",
       " 'backbone.stage3.0.activate.mul.param_quantizers': ModuleDict(),\n",
       " 'backbone.stage3.0.activate.mul.input_quantizers': ModuleList(\n",
       "   (0-1): 2 x None\n",
       " ),\n",
       " 'backbone.stage3.0.activate.mul.output_quantizers': ModuleList(\n",
       "   (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       " ),\n",
       " 'backbone.stage3.0.activate.mul.output_quantizers.0': QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False),\n",
       " 'backbone.stage3.1': Module(\n",
       "   (short_conv): Module(\n",
       "     (conv): QuantizedConv2d(\n",
       "       192, 96, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "       (param_quantizers): ModuleDict(\n",
       "         (weight): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=True)\n",
       "       )\n",
       "       (input_quantizers): ModuleList(\n",
       "         (0): None\n",
       "       )\n",
       "       (output_quantizers): ModuleList(\n",
       "         (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "       )\n",
       "     )\n",
       "     (bn): Module(\n",
       "       (module_batch_norm_18): FakeQuantizedBatchNorm(\n",
       "         (param_quantizers): ModuleDict()\n",
       "         (input_quantizers): ModuleList(\n",
       "           (0-4): 5 x QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "         )\n",
       "         (output_quantizers): ModuleList(\n",
       "           (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "         )\n",
       "       )\n",
       "     )\n",
       "     (activate): CustomSiLU(\n",
       "       (sigmoid): QuantizedSigmoid(\n",
       "         (param_quantizers): ModuleDict()\n",
       "         (input_quantizers): ModuleList(\n",
       "           (0): None\n",
       "         )\n",
       "         (output_quantizers): ModuleList(\n",
       "           (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "         )\n",
       "       )\n",
       "       (mul): QuantizedMultiply(\n",
       "         (param_quantizers): ModuleDict()\n",
       "         (input_quantizers): ModuleList(\n",
       "           (0-1): 2 x None\n",
       "         )\n",
       "         (output_quantizers): ModuleList(\n",
       "           (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "         )\n",
       "       )\n",
       "     )\n",
       "   )\n",
       "   (main_conv): Module(\n",
       "     (conv): QuantizedConv2d(\n",
       "       192, 96, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "       (param_quantizers): ModuleDict(\n",
       "         (weight): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=True)\n",
       "       )\n",
       "       (input_quantizers): ModuleList(\n",
       "         (0): None\n",
       "       )\n",
       "       (output_quantizers): ModuleList(\n",
       "         (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "       )\n",
       "     )\n",
       "     (bn): Module(\n",
       "       (module_batch_norm_19): FakeQuantizedBatchNorm(\n",
       "         (param_quantizers): ModuleDict()\n",
       "         (input_quantizers): ModuleList(\n",
       "           (0-4): 5 x QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "         )\n",
       "         (output_quantizers): ModuleList(\n",
       "           (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "         )\n",
       "       )\n",
       "     )\n",
       "     (activate): CustomSiLU(\n",
       "       (sigmoid): QuantizedSigmoid(\n",
       "         (param_quantizers): ModuleDict()\n",
       "         (input_quantizers): ModuleList(\n",
       "           (0): None\n",
       "         )\n",
       "         (output_quantizers): ModuleList(\n",
       "           (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "         )\n",
       "       )\n",
       "       (mul): QuantizedMultiply(\n",
       "         (param_quantizers): ModuleDict()\n",
       "         (input_quantizers): ModuleList(\n",
       "           (0-1): 2 x None\n",
       "         )\n",
       "         (output_quantizers): ModuleList(\n",
       "           (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "         )\n",
       "       )\n",
       "     )\n",
       "   )\n",
       "   (blocks): Module(\n",
       "     (0): Module(\n",
       "       (conv1): Module(\n",
       "         (conv): QuantizedConv2d(\n",
       "           96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "           (param_quantizers): ModuleDict(\n",
       "             (weight): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=True)\n",
       "           )\n",
       "           (input_quantizers): ModuleList(\n",
       "             (0): None\n",
       "           )\n",
       "           (output_quantizers): ModuleList(\n",
       "             (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "           )\n",
       "         )\n",
       "         (bn): Module(\n",
       "           (module_batch_norm_20): FakeQuantizedBatchNorm(\n",
       "             (param_quantizers): ModuleDict()\n",
       "             (input_quantizers): ModuleList(\n",
       "               (0-4): 5 x QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "             )\n",
       "             (output_quantizers): ModuleList(\n",
       "               (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "             )\n",
       "           )\n",
       "         )\n",
       "         (activate): CustomSiLU(\n",
       "           (sigmoid): QuantizedSigmoid(\n",
       "             (param_quantizers): ModuleDict()\n",
       "             (input_quantizers): ModuleList(\n",
       "               (0): None\n",
       "             )\n",
       "             (output_quantizers): ModuleList(\n",
       "               (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "             )\n",
       "           )\n",
       "           (mul): QuantizedMultiply(\n",
       "             (param_quantizers): ModuleDict()\n",
       "             (input_quantizers): ModuleList(\n",
       "               (0-1): 2 x None\n",
       "             )\n",
       "             (output_quantizers): ModuleList(\n",
       "               (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "             )\n",
       "           )\n",
       "         )\n",
       "       )\n",
       "       (conv2): Module(\n",
       "         (depthwise_conv): Module(\n",
       "           (conv): QuantizedConv2d(\n",
       "             96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96, bias=False\n",
       "             (param_quantizers): ModuleDict(\n",
       "               (weight): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=True)\n",
       "             )\n",
       "             (input_quantizers): ModuleList(\n",
       "               (0): None\n",
       "             )\n",
       "             (output_quantizers): ModuleList(\n",
       "               (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "             )\n",
       "           )\n",
       "           (bn): Module(\n",
       "             (module_batch_norm_21): BatchNorm()\n",
       "           )\n",
       "           (activate): CustomSiLU(\n",
       "             (sigmoid): QuantizedSigmoid(\n",
       "               (param_quantizers): ModuleDict()\n",
       "               (input_quantizers): ModuleList(\n",
       "                 (0): None\n",
       "               )\n",
       "               (output_quantizers): ModuleList(\n",
       "                 (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "               )\n",
       "             )\n",
       "             (mul): QuantizedMultiply(\n",
       "               (param_quantizers): ModuleDict()\n",
       "               (input_quantizers): ModuleList(\n",
       "                 (0-1): 2 x None\n",
       "               )\n",
       "               (output_quantizers): ModuleList(\n",
       "                 (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "               )\n",
       "             )\n",
       "           )\n",
       "         )\n",
       "         (pointwise_conv): Module(\n",
       "           (conv): QuantizedConv2d(\n",
       "             96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "             (param_quantizers): ModuleDict(\n",
       "               (weight): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=True)\n",
       "             )\n",
       "             (input_quantizers): ModuleList(\n",
       "               (0): None\n",
       "             )\n",
       "             (output_quantizers): ModuleList(\n",
       "               (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "             )\n",
       "           )\n",
       "           (bn): Module(\n",
       "             (module_batch_norm_22): FakeQuantizedBatchNorm(\n",
       "               (param_quantizers): ModuleDict()\n",
       "               (input_quantizers): ModuleList(\n",
       "                 (0-4): 5 x QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "               )\n",
       "               (output_quantizers): ModuleList(\n",
       "                 (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "               )\n",
       "             )\n",
       "           )\n",
       "           (activate): CustomSiLU(\n",
       "             (sigmoid): QuantizedSigmoid(\n",
       "               (param_quantizers): ModuleDict()\n",
       "               (input_quantizers): ModuleList(\n",
       "                 (0): None\n",
       "               )\n",
       "               (output_quantizers): ModuleList(\n",
       "                 (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "               )\n",
       "             )\n",
       "             (mul): QuantizedMultiply(\n",
       "               (param_quantizers): ModuleDict()\n",
       "               (input_quantizers): ModuleList(\n",
       "                 (0-1): 2 x None\n",
       "               )\n",
       "               (output_quantizers): ModuleList(\n",
       "                 (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "               )\n",
       "             )\n",
       "           )\n",
       "         )\n",
       "       )\n",
       "       (module_add_2): QuantizedAdd(\n",
       "         (param_quantizers): ModuleDict()\n",
       "         (input_quantizers): ModuleList(\n",
       "           (0-1): 2 x None\n",
       "         )\n",
       "         (output_quantizers): ModuleList(\n",
       "           (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "         )\n",
       "       )\n",
       "     )\n",
       "   )\n",
       "   (attention): Module(\n",
       "     (global_avgpool): FakeQuantizedAdaptiveAvgPool2d(\n",
       "       output_size=1\n",
       "       (param_quantizers): ModuleDict()\n",
       "       (input_quantizers): ModuleList(\n",
       "         (0): None\n",
       "       )\n",
       "       (output_quantizers): ModuleList(\n",
       "         (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "       )\n",
       "     )\n",
       "     (fc): QuantizedConv2d(\n",
       "       192, 192, kernel_size=(1, 1), stride=(1, 1)\n",
       "       (param_quantizers): ModuleDict(\n",
       "         (weight): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=True)\n",
       "         (bias): None\n",
       "       )\n",
       "       (input_quantizers): ModuleList(\n",
       "         (0): None\n",
       "       )\n",
       "       (output_quantizers): ModuleList(\n",
       "         (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "       )\n",
       "     )\n",
       "     (act): FakeQuantizedHardsigmoid(\n",
       "       (param_quantizers): ModuleDict()\n",
       "       (input_quantizers): ModuleList(\n",
       "         (0): None\n",
       "       )\n",
       "       (output_quantizers): ModuleList(\n",
       "         (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "       )\n",
       "     )\n",
       "     (module_mul_2): QuantizedMultiply(\n",
       "       (param_quantizers): ModuleDict()\n",
       "       (input_quantizers): ModuleList(\n",
       "         (0-1): 2 x None\n",
       "       )\n",
       "       (output_quantizers): ModuleList(\n",
       "         (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "       )\n",
       "     )\n",
       "   )\n",
       "   (final_conv): Module(\n",
       "     (conv): QuantizedConv2d(\n",
       "       192, 192, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "       (param_quantizers): ModuleDict(\n",
       "         (weight): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=True)\n",
       "       )\n",
       "       (input_quantizers): ModuleList(\n",
       "         (0): None\n",
       "       )\n",
       "       (output_quantizers): ModuleList(\n",
       "         (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "       )\n",
       "     )\n",
       "     (bn): Module(\n",
       "       (module_batch_norm_23): FakeQuantizedBatchNorm(\n",
       "         (param_quantizers): ModuleDict()\n",
       "         (input_quantizers): ModuleList(\n",
       "           (0-4): 5 x QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "         )\n",
       "         (output_quantizers): ModuleList(\n",
       "           (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "         )\n",
       "       )\n",
       "     )\n",
       "     (activate): CustomSiLU(\n",
       "       (sigmoid): QuantizedSigmoid(\n",
       "         (param_quantizers): ModuleDict()\n",
       "         (input_quantizers): ModuleList(\n",
       "           (0): None\n",
       "         )\n",
       "         (output_quantizers): ModuleList(\n",
       "           (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "         )\n",
       "       )\n",
       "       (mul): QuantizedMultiply(\n",
       "         (param_quantizers): ModuleDict()\n",
       "         (input_quantizers): ModuleList(\n",
       "           (0-1): 2 x None\n",
       "         )\n",
       "         (output_quantizers): ModuleList(\n",
       "           (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "         )\n",
       "       )\n",
       "     )\n",
       "   )\n",
       "   (module_cat_2): FakeQuantizedConcat(\n",
       "     (param_quantizers): ModuleDict()\n",
       "     (input_quantizers): ModuleList(\n",
       "       (0): None\n",
       "     )\n",
       "     (output_quantizers): ModuleList(\n",
       "       (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "     )\n",
       "   )\n",
       " ),\n",
       " 'backbone.stage3.1.short_conv': Module(\n",
       "   (conv): QuantizedConv2d(\n",
       "     192, 96, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "     (param_quantizers): ModuleDict(\n",
       "       (weight): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=True)\n",
       "     )\n",
       "     (input_quantizers): ModuleList(\n",
       "       (0): None\n",
       "     )\n",
       "     (output_quantizers): ModuleList(\n",
       "       (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "     )\n",
       "   )\n",
       "   (bn): Module(\n",
       "     (module_batch_norm_18): FakeQuantizedBatchNorm(\n",
       "       (param_quantizers): ModuleDict()\n",
       "       (input_quantizers): ModuleList(\n",
       "         (0-4): 5 x QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "       )\n",
       "       (output_quantizers): ModuleList(\n",
       "         (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "       )\n",
       "     )\n",
       "   )\n",
       "   (activate): CustomSiLU(\n",
       "     (sigmoid): QuantizedSigmoid(\n",
       "       (param_quantizers): ModuleDict()\n",
       "       (input_quantizers): ModuleList(\n",
       "         (0): None\n",
       "       )\n",
       "       (output_quantizers): ModuleList(\n",
       "         (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "       )\n",
       "     )\n",
       "     (mul): QuantizedMultiply(\n",
       "       (param_quantizers): ModuleDict()\n",
       "       (input_quantizers): ModuleList(\n",
       "         (0-1): 2 x None\n",
       "       )\n",
       "       (output_quantizers): ModuleList(\n",
       "         (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "       )\n",
       "     )\n",
       "   )\n",
       " ),\n",
       " 'backbone.stage3.1.short_conv.conv': QuantizedConv2d(\n",
       "   192, 96, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "   (param_quantizers): ModuleDict(\n",
       "     (weight): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=True)\n",
       "   )\n",
       "   (input_quantizers): ModuleList(\n",
       "     (0): None\n",
       "   )\n",
       "   (output_quantizers): ModuleList(\n",
       "     (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "   )\n",
       " ),\n",
       " 'backbone.stage3.1.short_conv.conv.param_quantizers': ModuleDict(\n",
       "   (weight): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=True)\n",
       " ),\n",
       " 'backbone.stage3.1.short_conv.conv.param_quantizers.weight': QuantizeDequantize(shape=[1], bitwidth=8, symmetric=True),\n",
       " 'backbone.stage3.1.short_conv.conv.input_quantizers': ModuleList(\n",
       "   (0): None\n",
       " ),\n",
       " 'backbone.stage3.1.short_conv.conv.output_quantizers': ModuleList(\n",
       "   (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       " ),\n",
       " 'backbone.stage3.1.short_conv.conv.output_quantizers.0': QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False),\n",
       " 'backbone.stage3.1.short_conv.bn': Module(\n",
       "   (module_batch_norm_18): FakeQuantizedBatchNorm(\n",
       "     (param_quantizers): ModuleDict()\n",
       "     (input_quantizers): ModuleList(\n",
       "       (0-4): 5 x QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "     )\n",
       "     (output_quantizers): ModuleList(\n",
       "       (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "     )\n",
       "   )\n",
       " ),\n",
       " 'backbone.stage3.1.short_conv.bn.module_batch_norm_18': FakeQuantizedBatchNorm(\n",
       "   (param_quantizers): ModuleDict()\n",
       "   (input_quantizers): ModuleList(\n",
       "     (0-4): 5 x QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "   )\n",
       "   (output_quantizers): ModuleList(\n",
       "     (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "   )\n",
       " ),\n",
       " 'backbone.stage3.1.short_conv.bn.module_batch_norm_18.param_quantizers': ModuleDict(),\n",
       " 'backbone.stage3.1.short_conv.bn.module_batch_norm_18.input_quantizers': ModuleList(\n",
       "   (0-4): 5 x QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       " ),\n",
       " 'backbone.stage3.1.short_conv.bn.module_batch_norm_18.input_quantizers.0': QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False),\n",
       " 'backbone.stage3.1.short_conv.bn.module_batch_norm_18.input_quantizers.1': QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False),\n",
       " 'backbone.stage3.1.short_conv.bn.module_batch_norm_18.input_quantizers.2': QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False),\n",
       " 'backbone.stage3.1.short_conv.bn.module_batch_norm_18.input_quantizers.3': QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False),\n",
       " 'backbone.stage3.1.short_conv.bn.module_batch_norm_18.input_quantizers.4': QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False),\n",
       " 'backbone.stage3.1.short_conv.bn.module_batch_norm_18.output_quantizers': ModuleList(\n",
       "   (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       " ),\n",
       " 'backbone.stage3.1.short_conv.bn.module_batch_norm_18.output_quantizers.0': QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False),\n",
       " 'backbone.stage3.1.short_conv.activate': CustomSiLU(\n",
       "   (sigmoid): QuantizedSigmoid(\n",
       "     (param_quantizers): ModuleDict()\n",
       "     (input_quantizers): ModuleList(\n",
       "       (0): None\n",
       "     )\n",
       "     (output_quantizers): ModuleList(\n",
       "       (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "     )\n",
       "   )\n",
       "   (mul): QuantizedMultiply(\n",
       "     (param_quantizers): ModuleDict()\n",
       "     (input_quantizers): ModuleList(\n",
       "       (0-1): 2 x None\n",
       "     )\n",
       "     (output_quantizers): ModuleList(\n",
       "       (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "     )\n",
       "   )\n",
       " ),\n",
       " 'backbone.stage3.1.short_conv.activate.sigmoid': QuantizedSigmoid(\n",
       "   (param_quantizers): ModuleDict()\n",
       "   (input_quantizers): ModuleList(\n",
       "     (0): None\n",
       "   )\n",
       "   (output_quantizers): ModuleList(\n",
       "     (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "   )\n",
       " ),\n",
       " 'backbone.stage3.1.short_conv.activate.sigmoid.param_quantizers': ModuleDict(),\n",
       " 'backbone.stage3.1.short_conv.activate.sigmoid.input_quantizers': ModuleList(\n",
       "   (0): None\n",
       " ),\n",
       " 'backbone.stage3.1.short_conv.activate.sigmoid.output_quantizers': ModuleList(\n",
       "   (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       " ),\n",
       " 'backbone.stage3.1.short_conv.activate.sigmoid.output_quantizers.0': QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False),\n",
       " 'backbone.stage3.1.short_conv.activate.mul': QuantizedMultiply(\n",
       "   (param_quantizers): ModuleDict()\n",
       "   (input_quantizers): ModuleList(\n",
       "     (0-1): 2 x None\n",
       "   )\n",
       "   (output_quantizers): ModuleList(\n",
       "     (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "   )\n",
       " ),\n",
       " 'backbone.stage3.1.short_conv.activate.mul.param_quantizers': ModuleDict(),\n",
       " 'backbone.stage3.1.short_conv.activate.mul.input_quantizers': ModuleList(\n",
       "   (0-1): 2 x None\n",
       " ),\n",
       " 'backbone.stage3.1.short_conv.activate.mul.output_quantizers': ModuleList(\n",
       "   (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       " ),\n",
       " 'backbone.stage3.1.short_conv.activate.mul.output_quantizers.0': QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False),\n",
       " 'backbone.stage3.1.main_conv': Module(\n",
       "   (conv): QuantizedConv2d(\n",
       "     192, 96, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "     (param_quantizers): ModuleDict(\n",
       "       (weight): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=True)\n",
       "     )\n",
       "     (input_quantizers): ModuleList(\n",
       "       (0): None\n",
       "     )\n",
       "     (output_quantizers): ModuleList(\n",
       "       (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "     )\n",
       "   )\n",
       "   (bn): Module(\n",
       "     (module_batch_norm_19): FakeQuantizedBatchNorm(\n",
       "       (param_quantizers): ModuleDict()\n",
       "       (input_quantizers): ModuleList(\n",
       "         (0-4): 5 x QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "       )\n",
       "       (output_quantizers): ModuleList(\n",
       "         (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "       )\n",
       "     )\n",
       "   )\n",
       "   (activate): CustomSiLU(\n",
       "     (sigmoid): QuantizedSigmoid(\n",
       "       (param_quantizers): ModuleDict()\n",
       "       (input_quantizers): ModuleList(\n",
       "         (0): None\n",
       "       )\n",
       "       (output_quantizers): ModuleList(\n",
       "         (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "       )\n",
       "     )\n",
       "     (mul): QuantizedMultiply(\n",
       "       (param_quantizers): ModuleDict()\n",
       "       (input_quantizers): ModuleList(\n",
       "         (0-1): 2 x None\n",
       "       )\n",
       "       (output_quantizers): ModuleList(\n",
       "         (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "       )\n",
       "     )\n",
       "   )\n",
       " ),\n",
       " 'backbone.stage3.1.main_conv.conv': QuantizedConv2d(\n",
       "   192, 96, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "   (param_quantizers): ModuleDict(\n",
       "     (weight): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=True)\n",
       "   )\n",
       "   (input_quantizers): ModuleList(\n",
       "     (0): None\n",
       "   )\n",
       "   (output_quantizers): ModuleList(\n",
       "     (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "   )\n",
       " ),\n",
       " 'backbone.stage3.1.main_conv.conv.param_quantizers': ModuleDict(\n",
       "   (weight): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=True)\n",
       " ),\n",
       " 'backbone.stage3.1.main_conv.conv.param_quantizers.weight': QuantizeDequantize(shape=[1], bitwidth=8, symmetric=True),\n",
       " 'backbone.stage3.1.main_conv.conv.input_quantizers': ModuleList(\n",
       "   (0): None\n",
       " ),\n",
       " 'backbone.stage3.1.main_conv.conv.output_quantizers': ModuleList(\n",
       "   (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       " ),\n",
       " 'backbone.stage3.1.main_conv.conv.output_quantizers.0': QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False),\n",
       " 'backbone.stage3.1.main_conv.bn': Module(\n",
       "   (module_batch_norm_19): FakeQuantizedBatchNorm(\n",
       "     (param_quantizers): ModuleDict()\n",
       "     (input_quantizers): ModuleList(\n",
       "       (0-4): 5 x QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "     )\n",
       "     (output_quantizers): ModuleList(\n",
       "       (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "     )\n",
       "   )\n",
       " ),\n",
       " 'backbone.stage3.1.main_conv.bn.module_batch_norm_19': FakeQuantizedBatchNorm(\n",
       "   (param_quantizers): ModuleDict()\n",
       "   (input_quantizers): ModuleList(\n",
       "     (0-4): 5 x QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "   )\n",
       "   (output_quantizers): ModuleList(\n",
       "     (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "   )\n",
       " ),\n",
       " 'backbone.stage3.1.main_conv.bn.module_batch_norm_19.param_quantizers': ModuleDict(),\n",
       " 'backbone.stage3.1.main_conv.bn.module_batch_norm_19.input_quantizers': ModuleList(\n",
       "   (0-4): 5 x QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       " ),\n",
       " 'backbone.stage3.1.main_conv.bn.module_batch_norm_19.input_quantizers.0': QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False),\n",
       " 'backbone.stage3.1.main_conv.bn.module_batch_norm_19.input_quantizers.1': QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False),\n",
       " 'backbone.stage3.1.main_conv.bn.module_batch_norm_19.input_quantizers.2': QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False),\n",
       " 'backbone.stage3.1.main_conv.bn.module_batch_norm_19.input_quantizers.3': QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False),\n",
       " 'backbone.stage3.1.main_conv.bn.module_batch_norm_19.input_quantizers.4': QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False),\n",
       " 'backbone.stage3.1.main_conv.bn.module_batch_norm_19.output_quantizers': ModuleList(\n",
       "   (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       " ),\n",
       " 'backbone.stage3.1.main_conv.bn.module_batch_norm_19.output_quantizers.0': QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False),\n",
       " 'backbone.stage3.1.main_conv.activate': CustomSiLU(\n",
       "   (sigmoid): QuantizedSigmoid(\n",
       "     (param_quantizers): ModuleDict()\n",
       "     (input_quantizers): ModuleList(\n",
       "       (0): None\n",
       "     )\n",
       "     (output_quantizers): ModuleList(\n",
       "       (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "     )\n",
       "   )\n",
       "   (mul): QuantizedMultiply(\n",
       "     (param_quantizers): ModuleDict()\n",
       "     (input_quantizers): ModuleList(\n",
       "       (0-1): 2 x None\n",
       "     )\n",
       "     (output_quantizers): ModuleList(\n",
       "       (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "     )\n",
       "   )\n",
       " ),\n",
       " 'backbone.stage3.1.main_conv.activate.sigmoid': QuantizedSigmoid(\n",
       "   (param_quantizers): ModuleDict()\n",
       "   (input_quantizers): ModuleList(\n",
       "     (0): None\n",
       "   )\n",
       "   (output_quantizers): ModuleList(\n",
       "     (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "   )\n",
       " ),\n",
       " 'backbone.stage3.1.main_conv.activate.sigmoid.param_quantizers': ModuleDict(),\n",
       " 'backbone.stage3.1.main_conv.activate.sigmoid.input_quantizers': ModuleList(\n",
       "   (0): None\n",
       " ),\n",
       " 'backbone.stage3.1.main_conv.activate.sigmoid.output_quantizers': ModuleList(\n",
       "   (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       " ),\n",
       " 'backbone.stage3.1.main_conv.activate.sigmoid.output_quantizers.0': QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False),\n",
       " 'backbone.stage3.1.main_conv.activate.mul': QuantizedMultiply(\n",
       "   (param_quantizers): ModuleDict()\n",
       "   (input_quantizers): ModuleList(\n",
       "     (0-1): 2 x None\n",
       "   )\n",
       "   (output_quantizers): ModuleList(\n",
       "     (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "   )\n",
       " ),\n",
       " 'backbone.stage3.1.main_conv.activate.mul.param_quantizers': ModuleDict(),\n",
       " 'backbone.stage3.1.main_conv.activate.mul.input_quantizers': ModuleList(\n",
       "   (0-1): 2 x None\n",
       " ),\n",
       " 'backbone.stage3.1.main_conv.activate.mul.output_quantizers': ModuleList(\n",
       "   (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       " ),\n",
       " 'backbone.stage3.1.main_conv.activate.mul.output_quantizers.0': QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False),\n",
       " 'backbone.stage3.1.blocks': Module(\n",
       "   (0): Module(\n",
       "     (conv1): Module(\n",
       "       (conv): QuantizedConv2d(\n",
       "         96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "         (param_quantizers): ModuleDict(\n",
       "           (weight): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=True)\n",
       "         )\n",
       "         (input_quantizers): ModuleList(\n",
       "           (0): None\n",
       "         )\n",
       "         (output_quantizers): ModuleList(\n",
       "           (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "         )\n",
       "       )\n",
       "       (bn): Module(\n",
       "         (module_batch_norm_20): FakeQuantizedBatchNorm(\n",
       "           (param_quantizers): ModuleDict()\n",
       "           (input_quantizers): ModuleList(\n",
       "             (0-4): 5 x QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "           )\n",
       "           (output_quantizers): ModuleList(\n",
       "             (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "           )\n",
       "         )\n",
       "       )\n",
       "       (activate): CustomSiLU(\n",
       "         (sigmoid): QuantizedSigmoid(\n",
       "           (param_quantizers): ModuleDict()\n",
       "           (input_quantizers): ModuleList(\n",
       "             (0): None\n",
       "           )\n",
       "           (output_quantizers): ModuleList(\n",
       "             (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "           )\n",
       "         )\n",
       "         (mul): QuantizedMultiply(\n",
       "           (param_quantizers): ModuleDict()\n",
       "           (input_quantizers): ModuleList(\n",
       "             (0-1): 2 x None\n",
       "           )\n",
       "           (output_quantizers): ModuleList(\n",
       "             (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "           )\n",
       "         )\n",
       "       )\n",
       "     )\n",
       "     (conv2): Module(\n",
       "       (depthwise_conv): Module(\n",
       "         (conv): QuantizedConv2d(\n",
       "           96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96, bias=False\n",
       "           (param_quantizers): ModuleDict(\n",
       "             (weight): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=True)\n",
       "           )\n",
       "           (input_quantizers): ModuleList(\n",
       "             (0): None\n",
       "           )\n",
       "           (output_quantizers): ModuleList(\n",
       "             (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "           )\n",
       "         )\n",
       "         (bn): Module(\n",
       "           (module_batch_norm_21): BatchNorm()\n",
       "         )\n",
       "         (activate): CustomSiLU(\n",
       "           (sigmoid): QuantizedSigmoid(\n",
       "             (param_quantizers): ModuleDict()\n",
       "             (input_quantizers): ModuleList(\n",
       "               (0): None\n",
       "             )\n",
       "             (output_quantizers): ModuleList(\n",
       "               (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "             )\n",
       "           )\n",
       "           (mul): QuantizedMultiply(\n",
       "             (param_quantizers): ModuleDict()\n",
       "             (input_quantizers): ModuleList(\n",
       "               (0-1): 2 x None\n",
       "             )\n",
       "             (output_quantizers): ModuleList(\n",
       "               (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "             )\n",
       "           )\n",
       "         )\n",
       "       )\n",
       "       (pointwise_conv): Module(\n",
       "         (conv): QuantizedConv2d(\n",
       "           96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "           (param_quantizers): ModuleDict(\n",
       "             (weight): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=True)\n",
       "           )\n",
       "           (input_quantizers): ModuleList(\n",
       "             (0): None\n",
       "           )\n",
       "           (output_quantizers): ModuleList(\n",
       "             (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "           )\n",
       "         )\n",
       "         (bn): Module(\n",
       "           (module_batch_norm_22): FakeQuantizedBatchNorm(\n",
       "             (param_quantizers): ModuleDict()\n",
       "             (input_quantizers): ModuleList(\n",
       "               (0-4): 5 x QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "             )\n",
       "             (output_quantizers): ModuleList(\n",
       "               (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "             )\n",
       "           )\n",
       "         )\n",
       "         (activate): CustomSiLU(\n",
       "           (sigmoid): QuantizedSigmoid(\n",
       "             (param_quantizers): ModuleDict()\n",
       "             (input_quantizers): ModuleList(\n",
       "               (0): None\n",
       "             )\n",
       "             (output_quantizers): ModuleList(\n",
       "               (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "             )\n",
       "           )\n",
       "           (mul): QuantizedMultiply(\n",
       "             (param_quantizers): ModuleDict()\n",
       "             (input_quantizers): ModuleList(\n",
       "               (0-1): 2 x None\n",
       "             )\n",
       "             (output_quantizers): ModuleList(\n",
       "               (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "             )\n",
       "           )\n",
       "         )\n",
       "       )\n",
       "     )\n",
       "     (module_add_2): QuantizedAdd(\n",
       "       (param_quantizers): ModuleDict()\n",
       "       (input_quantizers): ModuleList(\n",
       "         (0-1): 2 x None\n",
       "       )\n",
       "       (output_quantizers): ModuleList(\n",
       "         (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "       )\n",
       "     )\n",
       "   )\n",
       " ),\n",
       " 'backbone.stage3.1.blocks.0': Module(\n",
       "   (conv1): Module(\n",
       "     (conv): QuantizedConv2d(\n",
       "       96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "       (param_quantizers): ModuleDict(\n",
       "         (weight): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=True)\n",
       "       )\n",
       "       (input_quantizers): ModuleList(\n",
       "         (0): None\n",
       "       )\n",
       "       (output_quantizers): ModuleList(\n",
       "         (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "       )\n",
       "     )\n",
       "     (bn): Module(\n",
       "       (module_batch_norm_20): FakeQuantizedBatchNorm(\n",
       "         (param_quantizers): ModuleDict()\n",
       "         (input_quantizers): ModuleList(\n",
       "           (0-4): 5 x QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "         )\n",
       "         (output_quantizers): ModuleList(\n",
       "           (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "         )\n",
       "       )\n",
       "     )\n",
       "     (activate): CustomSiLU(\n",
       "       (sigmoid): QuantizedSigmoid(\n",
       "         (param_quantizers): ModuleDict()\n",
       "         (input_quantizers): ModuleList(\n",
       "           (0): None\n",
       "         )\n",
       "         (output_quantizers): ModuleList(\n",
       "           (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "         )\n",
       "       )\n",
       "       (mul): QuantizedMultiply(\n",
       "         (param_quantizers): ModuleDict()\n",
       "         (input_quantizers): ModuleList(\n",
       "           (0-1): 2 x None\n",
       "         )\n",
       "         (output_quantizers): ModuleList(\n",
       "           (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "         )\n",
       "       )\n",
       "     )\n",
       "   )\n",
       "   (conv2): Module(\n",
       "     (depthwise_conv): Module(\n",
       "       (conv): QuantizedConv2d(\n",
       "         96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96, bias=False\n",
       "         (param_quantizers): ModuleDict(\n",
       "           (weight): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=True)\n",
       "         )\n",
       "         (input_quantizers): ModuleList(\n",
       "           (0): None\n",
       "         )\n",
       "         (output_quantizers): ModuleList(\n",
       "           (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "         )\n",
       "       )\n",
       "       (bn): Module(\n",
       "         (module_batch_norm_21): BatchNorm()\n",
       "       )\n",
       "       (activate): CustomSiLU(\n",
       "         (sigmoid): QuantizedSigmoid(\n",
       "           (param_quantizers): ModuleDict()\n",
       "           (input_quantizers): ModuleList(\n",
       "             (0): None\n",
       "           )\n",
       "           (output_quantizers): ModuleList(\n",
       "             (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "           )\n",
       "         )\n",
       "         (mul): QuantizedMultiply(\n",
       "           (param_quantizers): ModuleDict()\n",
       "           (input_quantizers): ModuleList(\n",
       "             (0-1): 2 x None\n",
       "           )\n",
       "           (output_quantizers): ModuleList(\n",
       "             (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "           )\n",
       "         )\n",
       "       )\n",
       "     )\n",
       "     (pointwise_conv): Module(\n",
       "       (conv): QuantizedConv2d(\n",
       "         96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "         (param_quantizers): ModuleDict(\n",
       "           (weight): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=True)\n",
       "         )\n",
       "         (input_quantizers): ModuleList(\n",
       "           (0): None\n",
       "         )\n",
       "         (output_quantizers): ModuleList(\n",
       "           (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "         )\n",
       "       )\n",
       "       (bn): Module(\n",
       "         (module_batch_norm_22): FakeQuantizedBatchNorm(\n",
       "           (param_quantizers): ModuleDict()\n",
       "           (input_quantizers): ModuleList(\n",
       "             (0-4): 5 x QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "           )\n",
       "           (output_quantizers): ModuleList(\n",
       "             (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "           )\n",
       "         )\n",
       "       )\n",
       "       (activate): CustomSiLU(\n",
       "         (sigmoid): QuantizedSigmoid(\n",
       "           (param_quantizers): ModuleDict()\n",
       "           (input_quantizers): ModuleList(\n",
       "             (0): None\n",
       "           )\n",
       "           (output_quantizers): ModuleList(\n",
       "             (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "           )\n",
       "         )\n",
       "         (mul): QuantizedMultiply(\n",
       "           (param_quantizers): ModuleDict()\n",
       "           (input_quantizers): ModuleList(\n",
       "             (0-1): 2 x None\n",
       "           )\n",
       "           (output_quantizers): ModuleList(\n",
       "             (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "           )\n",
       "         )\n",
       "       )\n",
       "     )\n",
       "   )\n",
       "   (module_add_2): QuantizedAdd(\n",
       "     (param_quantizers): ModuleDict()\n",
       "     (input_quantizers): ModuleList(\n",
       "       (0-1): 2 x None\n",
       "     )\n",
       "     (output_quantizers): ModuleList(\n",
       "       (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "     )\n",
       "   )\n",
       " ),\n",
       " 'backbone.stage3.1.blocks.0.conv1': Module(\n",
       "   (conv): QuantizedConv2d(\n",
       "     96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "     (param_quantizers): ModuleDict(\n",
       "       (weight): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=True)\n",
       "     )\n",
       "     (input_quantizers): ModuleList(\n",
       "       (0): None\n",
       "     )\n",
       "     (output_quantizers): ModuleList(\n",
       "       (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "     )\n",
       "   )\n",
       "   (bn): Module(\n",
       "     (module_batch_norm_20): FakeQuantizedBatchNorm(\n",
       "       (param_quantizers): ModuleDict()\n",
       "       (input_quantizers): ModuleList(\n",
       "         (0-4): 5 x QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "       )\n",
       "       (output_quantizers): ModuleList(\n",
       "         (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "       )\n",
       "     )\n",
       "   )\n",
       "   (activate): CustomSiLU(\n",
       "     (sigmoid): QuantizedSigmoid(\n",
       "       (param_quantizers): ModuleDict()\n",
       "       (input_quantizers): ModuleList(\n",
       "         (0): None\n",
       "       )\n",
       "       (output_quantizers): ModuleList(\n",
       "         (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "       )\n",
       "     )\n",
       "     (mul): QuantizedMultiply(\n",
       "       (param_quantizers): ModuleDict()\n",
       "       (input_quantizers): ModuleList(\n",
       "         (0-1): 2 x None\n",
       "       )\n",
       "       (output_quantizers): ModuleList(\n",
       "         (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "       )\n",
       "     )\n",
       "   )\n",
       " ),\n",
       " 'backbone.stage3.1.blocks.0.conv1.conv': QuantizedConv2d(\n",
       "   96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "   (param_quantizers): ModuleDict(\n",
       "     (weight): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=True)\n",
       "   )\n",
       "   (input_quantizers): ModuleList(\n",
       "     (0): None\n",
       "   )\n",
       "   (output_quantizers): ModuleList(\n",
       "     (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "   )\n",
       " ),\n",
       " 'backbone.stage3.1.blocks.0.conv1.conv.param_quantizers': ModuleDict(\n",
       "   (weight): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=True)\n",
       " ),\n",
       " 'backbone.stage3.1.blocks.0.conv1.conv.param_quantizers.weight': QuantizeDequantize(shape=[1], bitwidth=8, symmetric=True),\n",
       " 'backbone.stage3.1.blocks.0.conv1.conv.input_quantizers': ModuleList(\n",
       "   (0): None\n",
       " ),\n",
       " 'backbone.stage3.1.blocks.0.conv1.conv.output_quantizers': ModuleList(\n",
       "   (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       " ),\n",
       " 'backbone.stage3.1.blocks.0.conv1.conv.output_quantizers.0': QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False),\n",
       " 'backbone.stage3.1.blocks.0.conv1.bn': Module(\n",
       "   (module_batch_norm_20): FakeQuantizedBatchNorm(\n",
       "     (param_quantizers): ModuleDict()\n",
       "     (input_quantizers): ModuleList(\n",
       "       (0-4): 5 x QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "     )\n",
       "     (output_quantizers): ModuleList(\n",
       "       (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "     )\n",
       "   )\n",
       " ),\n",
       " 'backbone.stage3.1.blocks.0.conv1.bn.module_batch_norm_20': FakeQuantizedBatchNorm(\n",
       "   (param_quantizers): ModuleDict()\n",
       "   (input_quantizers): ModuleList(\n",
       "     (0-4): 5 x QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "   )\n",
       "   (output_quantizers): ModuleList(\n",
       "     (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "   )\n",
       " ),\n",
       " 'backbone.stage3.1.blocks.0.conv1.bn.module_batch_norm_20.param_quantizers': ModuleDict(),\n",
       " 'backbone.stage3.1.blocks.0.conv1.bn.module_batch_norm_20.input_quantizers': ModuleList(\n",
       "   (0-4): 5 x QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       " ),\n",
       " 'backbone.stage3.1.blocks.0.conv1.bn.module_batch_norm_20.input_quantizers.0': QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False),\n",
       " 'backbone.stage3.1.blocks.0.conv1.bn.module_batch_norm_20.input_quantizers.1': QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False),\n",
       " 'backbone.stage3.1.blocks.0.conv1.bn.module_batch_norm_20.input_quantizers.2': QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False),\n",
       " 'backbone.stage3.1.blocks.0.conv1.bn.module_batch_norm_20.input_quantizers.3': QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False),\n",
       " 'backbone.stage3.1.blocks.0.conv1.bn.module_batch_norm_20.input_quantizers.4': QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False),\n",
       " 'backbone.stage3.1.blocks.0.conv1.bn.module_batch_norm_20.output_quantizers': ModuleList(\n",
       "   (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       " ),\n",
       " 'backbone.stage3.1.blocks.0.conv1.bn.module_batch_norm_20.output_quantizers.0': QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False),\n",
       " 'backbone.stage3.1.blocks.0.conv1.activate': CustomSiLU(\n",
       "   (sigmoid): QuantizedSigmoid(\n",
       "     (param_quantizers): ModuleDict()\n",
       "     (input_quantizers): ModuleList(\n",
       "       (0): None\n",
       "     )\n",
       "     (output_quantizers): ModuleList(\n",
       "       (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "     )\n",
       "   )\n",
       "   (mul): QuantizedMultiply(\n",
       "     (param_quantizers): ModuleDict()\n",
       "     (input_quantizers): ModuleList(\n",
       "       (0-1): 2 x None\n",
       "     )\n",
       "     (output_quantizers): ModuleList(\n",
       "       (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "     )\n",
       "   )\n",
       " ),\n",
       " 'backbone.stage3.1.blocks.0.conv1.activate.sigmoid': QuantizedSigmoid(\n",
       "   (param_quantizers): ModuleDict()\n",
       "   (input_quantizers): ModuleList(\n",
       "     (0): None\n",
       "   )\n",
       "   (output_quantizers): ModuleList(\n",
       "     (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "   )\n",
       " ),\n",
       " 'backbone.stage3.1.blocks.0.conv1.activate.sigmoid.param_quantizers': ModuleDict(),\n",
       " 'backbone.stage3.1.blocks.0.conv1.activate.sigmoid.input_quantizers': ModuleList(\n",
       "   (0): None\n",
       " ),\n",
       " 'backbone.stage3.1.blocks.0.conv1.activate.sigmoid.output_quantizers': ModuleList(\n",
       "   (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       " ),\n",
       " 'backbone.stage3.1.blocks.0.conv1.activate.sigmoid.output_quantizers.0': QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False),\n",
       " 'backbone.stage3.1.blocks.0.conv1.activate.mul': QuantizedMultiply(\n",
       "   (param_quantizers): ModuleDict()\n",
       "   (input_quantizers): ModuleList(\n",
       "     (0-1): 2 x None\n",
       "   )\n",
       "   (output_quantizers): ModuleList(\n",
       "     (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "   )\n",
       " ),\n",
       " 'backbone.stage3.1.blocks.0.conv1.activate.mul.param_quantizers': ModuleDict(),\n",
       " 'backbone.stage3.1.blocks.0.conv1.activate.mul.input_quantizers': ModuleList(\n",
       "   (0-1): 2 x None\n",
       " ),\n",
       " 'backbone.stage3.1.blocks.0.conv1.activate.mul.output_quantizers': ModuleList(\n",
       "   (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       " ),\n",
       " 'backbone.stage3.1.blocks.0.conv1.activate.mul.output_quantizers.0': QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False),\n",
       " 'backbone.stage3.1.blocks.0.conv2': Module(\n",
       "   (depthwise_conv): Module(\n",
       "     (conv): QuantizedConv2d(\n",
       "       96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96, bias=False\n",
       "       (param_quantizers): ModuleDict(\n",
       "         (weight): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=True)\n",
       "       )\n",
       "       (input_quantizers): ModuleList(\n",
       "         (0): None\n",
       "       )\n",
       "       (output_quantizers): ModuleList(\n",
       "         (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "       )\n",
       "     )\n",
       "     (bn): Module(\n",
       "       (module_batch_norm_21): BatchNorm()\n",
       "     )\n",
       "     (activate): CustomSiLU(\n",
       "       (sigmoid): QuantizedSigmoid(\n",
       "         (param_quantizers): ModuleDict()\n",
       "         (input_quantizers): ModuleList(\n",
       "           (0): None\n",
       "         )\n",
       "         (output_quantizers): ModuleList(\n",
       "           (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "         )\n",
       "       )\n",
       "       (mul): QuantizedMultiply(\n",
       "         (param_quantizers): ModuleDict()\n",
       "         (input_quantizers): ModuleList(\n",
       "           (0-1): 2 x None\n",
       "         )\n",
       "         (output_quantizers): ModuleList(\n",
       "           (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "         )\n",
       "       )\n",
       "     )\n",
       "   )\n",
       "   (pointwise_conv): Module(\n",
       "     (conv): QuantizedConv2d(\n",
       "       96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "       (param_quantizers): ModuleDict(\n",
       "         (weight): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=True)\n",
       "       )\n",
       "       (input_quantizers): ModuleList(\n",
       "         (0): None\n",
       "       )\n",
       "       (output_quantizers): ModuleList(\n",
       "         (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "       )\n",
       "     )\n",
       "     (bn): Module(\n",
       "       (module_batch_norm_22): FakeQuantizedBatchNorm(\n",
       "         (param_quantizers): ModuleDict()\n",
       "         (input_quantizers): ModuleList(\n",
       "           (0-4): 5 x QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "         )\n",
       "         (output_quantizers): ModuleList(\n",
       "           (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "         )\n",
       "       )\n",
       "     )\n",
       "     (activate): CustomSiLU(\n",
       "       (sigmoid): QuantizedSigmoid(\n",
       "         (param_quantizers): ModuleDict()\n",
       "         (input_quantizers): ModuleList(\n",
       "           (0): None\n",
       "         )\n",
       "         (output_quantizers): ModuleList(\n",
       "           (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "         )\n",
       "       )\n",
       "       (mul): QuantizedMultiply(\n",
       "         (param_quantizers): ModuleDict()\n",
       "         (input_quantizers): ModuleList(\n",
       "           (0-1): 2 x None\n",
       "         )\n",
       "         (output_quantizers): ModuleList(\n",
       "           (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "         )\n",
       "       )\n",
       "     )\n",
       "   )\n",
       " ),\n",
       " 'backbone.stage3.1.blocks.0.conv2.depthwise_conv': Module(\n",
       "   (conv): QuantizedConv2d(\n",
       "     96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96, bias=False\n",
       "     (param_quantizers): ModuleDict(\n",
       "       (weight): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=True)\n",
       "     )\n",
       "     (input_quantizers): ModuleList(\n",
       "       (0): None\n",
       "     )\n",
       "     (output_quantizers): ModuleList(\n",
       "       (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "     )\n",
       "   )\n",
       "   (bn): Module(\n",
       "     (module_batch_norm_21): BatchNorm()\n",
       "   )\n",
       "   (activate): CustomSiLU(\n",
       "     (sigmoid): QuantizedSigmoid(\n",
       "       (param_quantizers): ModuleDict()\n",
       "       (input_quantizers): ModuleList(\n",
       "         (0): None\n",
       "       )\n",
       "       (output_quantizers): ModuleList(\n",
       "         (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "       )\n",
       "     )\n",
       "     (mul): QuantizedMultiply(\n",
       "       (param_quantizers): ModuleDict()\n",
       "       (input_quantizers): ModuleList(\n",
       "         (0-1): 2 x None\n",
       "       )\n",
       "       (output_quantizers): ModuleList(\n",
       "         (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "       )\n",
       "     )\n",
       "   )\n",
       " ),\n",
       " 'backbone.stage3.1.blocks.0.conv2.depthwise_conv.conv': QuantizedConv2d(\n",
       "   96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96, bias=False\n",
       "   (param_quantizers): ModuleDict(\n",
       "     (weight): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=True)\n",
       "   )\n",
       "   (input_quantizers): ModuleList(\n",
       "     (0): None\n",
       "   )\n",
       "   (output_quantizers): ModuleList(\n",
       "     (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "   )\n",
       " ),\n",
       " 'backbone.stage3.1.blocks.0.conv2.depthwise_conv.conv.param_quantizers': ModuleDict(\n",
       "   (weight): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=True)\n",
       " ),\n",
       " 'backbone.stage3.1.blocks.0.conv2.depthwise_conv.conv.param_quantizers.weight': QuantizeDequantize(shape=[1], bitwidth=8, symmetric=True),\n",
       " 'backbone.stage3.1.blocks.0.conv2.depthwise_conv.conv.input_quantizers': ModuleList(\n",
       "   (0): None\n",
       " ),\n",
       " 'backbone.stage3.1.blocks.0.conv2.depthwise_conv.conv.output_quantizers': ModuleList(\n",
       "   (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       " ),\n",
       " 'backbone.stage3.1.blocks.0.conv2.depthwise_conv.conv.output_quantizers.0': QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False),\n",
       " 'backbone.stage3.1.blocks.0.conv2.depthwise_conv.bn': Module(\n",
       "   (module_batch_norm_21): BatchNorm()\n",
       " ),\n",
       " 'backbone.stage3.1.blocks.0.conv2.depthwise_conv.bn.module_batch_norm_21': BatchNorm(),\n",
       " 'backbone.stage3.1.blocks.0.conv2.depthwise_conv.activate': CustomSiLU(\n",
       "   (sigmoid): QuantizedSigmoid(\n",
       "     (param_quantizers): ModuleDict()\n",
       "     (input_quantizers): ModuleList(\n",
       "       (0): None\n",
       "     )\n",
       "     (output_quantizers): ModuleList(\n",
       "       (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "     )\n",
       "   )\n",
       "   (mul): QuantizedMultiply(\n",
       "     (param_quantizers): ModuleDict()\n",
       "     (input_quantizers): ModuleList(\n",
       "       (0-1): 2 x None\n",
       "     )\n",
       "     (output_quantizers): ModuleList(\n",
       "       (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "     )\n",
       "   )\n",
       " ),\n",
       " 'backbone.stage3.1.blocks.0.conv2.depthwise_conv.activate.sigmoid': QuantizedSigmoid(\n",
       "   (param_quantizers): ModuleDict()\n",
       "   (input_quantizers): ModuleList(\n",
       "     (0): None\n",
       "   )\n",
       "   (output_quantizers): ModuleList(\n",
       "     (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "   )\n",
       " ),\n",
       " 'backbone.stage3.1.blocks.0.conv2.depthwise_conv.activate.sigmoid.param_quantizers': ModuleDict(),\n",
       " 'backbone.stage3.1.blocks.0.conv2.depthwise_conv.activate.sigmoid.input_quantizers': ModuleList(\n",
       "   (0): None\n",
       " ),\n",
       " 'backbone.stage3.1.blocks.0.conv2.depthwise_conv.activate.sigmoid.output_quantizers': ModuleList(\n",
       "   (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       " ),\n",
       " 'backbone.stage3.1.blocks.0.conv2.depthwise_conv.activate.sigmoid.output_quantizers.0': QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False),\n",
       " 'backbone.stage3.1.blocks.0.conv2.depthwise_conv.activate.mul': QuantizedMultiply(\n",
       "   (param_quantizers): ModuleDict()\n",
       "   (input_quantizers): ModuleList(\n",
       "     (0-1): 2 x None\n",
       "   )\n",
       "   (output_quantizers): ModuleList(\n",
       "     (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "   )\n",
       " ),\n",
       " 'backbone.stage3.1.blocks.0.conv2.depthwise_conv.activate.mul.param_quantizers': ModuleDict(),\n",
       " 'backbone.stage3.1.blocks.0.conv2.depthwise_conv.activate.mul.input_quantizers': ModuleList(\n",
       "   (0-1): 2 x None\n",
       " ),\n",
       " 'backbone.stage3.1.blocks.0.conv2.depthwise_conv.activate.mul.output_quantizers': ModuleList(\n",
       "   (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       " ),\n",
       " 'backbone.stage3.1.blocks.0.conv2.depthwise_conv.activate.mul.output_quantizers.0': QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False),\n",
       " 'backbone.stage3.1.blocks.0.conv2.pointwise_conv': Module(\n",
       "   (conv): QuantizedConv2d(\n",
       "     96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "     (param_quantizers): ModuleDict(\n",
       "       (weight): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=True)\n",
       "     )\n",
       "     (input_quantizers): ModuleList(\n",
       "       (0): None\n",
       "     )\n",
       "     (output_quantizers): ModuleList(\n",
       "       (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "     )\n",
       "   )\n",
       "   (bn): Module(\n",
       "     (module_batch_norm_22): FakeQuantizedBatchNorm(\n",
       "       (param_quantizers): ModuleDict()\n",
       "       (input_quantizers): ModuleList(\n",
       "         (0-4): 5 x QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "       )\n",
       "       (output_quantizers): ModuleList(\n",
       "         (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "       )\n",
       "     )\n",
       "   )\n",
       "   (activate): CustomSiLU(\n",
       "     (sigmoid): QuantizedSigmoid(\n",
       "       (param_quantizers): ModuleDict()\n",
       "       (input_quantizers): ModuleList(\n",
       "         (0): None\n",
       "       )\n",
       "       (output_quantizers): ModuleList(\n",
       "         (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "       )\n",
       "     )\n",
       "     (mul): QuantizedMultiply(\n",
       "       (param_quantizers): ModuleDict()\n",
       "       (input_quantizers): ModuleList(\n",
       "         (0-1): 2 x None\n",
       "       )\n",
       "       (output_quantizers): ModuleList(\n",
       "         (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "       )\n",
       "     )\n",
       "   )\n",
       " ),\n",
       " 'backbone.stage3.1.blocks.0.conv2.pointwise_conv.conv': QuantizedConv2d(\n",
       "   96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "   (param_quantizers): ModuleDict(\n",
       "     (weight): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=True)\n",
       "   )\n",
       "   (input_quantizers): ModuleList(\n",
       "     (0): None\n",
       "   )\n",
       "   (output_quantizers): ModuleList(\n",
       "     (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "   )\n",
       " ),\n",
       " 'backbone.stage3.1.blocks.0.conv2.pointwise_conv.conv.param_quantizers': ModuleDict(\n",
       "   (weight): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=True)\n",
       " ),\n",
       " 'backbone.stage3.1.blocks.0.conv2.pointwise_conv.conv.param_quantizers.weight': QuantizeDequantize(shape=[1], bitwidth=8, symmetric=True),\n",
       " 'backbone.stage3.1.blocks.0.conv2.pointwise_conv.conv.input_quantizers': ModuleList(\n",
       "   (0): None\n",
       " ),\n",
       " 'backbone.stage3.1.blocks.0.conv2.pointwise_conv.conv.output_quantizers': ModuleList(\n",
       "   (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       " ),\n",
       " 'backbone.stage3.1.blocks.0.conv2.pointwise_conv.conv.output_quantizers.0': QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False),\n",
       " 'backbone.stage3.1.blocks.0.conv2.pointwise_conv.bn': Module(\n",
       "   (module_batch_norm_22): FakeQuantizedBatchNorm(\n",
       "     (param_quantizers): ModuleDict()\n",
       "     (input_quantizers): ModuleList(\n",
       "       (0-4): 5 x QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "     )\n",
       "     (output_quantizers): ModuleList(\n",
       "       (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "     )\n",
       "   )\n",
       " ),\n",
       " 'backbone.stage3.1.blocks.0.conv2.pointwise_conv.bn.module_batch_norm_22': FakeQuantizedBatchNorm(\n",
       "   (param_quantizers): ModuleDict()\n",
       "   (input_quantizers): ModuleList(\n",
       "     (0-4): 5 x QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "   )\n",
       "   (output_quantizers): ModuleList(\n",
       "     (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "   )\n",
       " ),\n",
       " 'backbone.stage3.1.blocks.0.conv2.pointwise_conv.bn.module_batch_norm_22.param_quantizers': ModuleDict(),\n",
       " 'backbone.stage3.1.blocks.0.conv2.pointwise_conv.bn.module_batch_norm_22.input_quantizers': ModuleList(\n",
       "   (0-4): 5 x QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       " ),\n",
       " 'backbone.stage3.1.blocks.0.conv2.pointwise_conv.bn.module_batch_norm_22.input_quantizers.0': QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False),\n",
       " 'backbone.stage3.1.blocks.0.conv2.pointwise_conv.bn.module_batch_norm_22.input_quantizers.1': QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False),\n",
       " 'backbone.stage3.1.blocks.0.conv2.pointwise_conv.bn.module_batch_norm_22.input_quantizers.2': QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False),\n",
       " 'backbone.stage3.1.blocks.0.conv2.pointwise_conv.bn.module_batch_norm_22.input_quantizers.3': QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False),\n",
       " 'backbone.stage3.1.blocks.0.conv2.pointwise_conv.bn.module_batch_norm_22.input_quantizers.4': QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False),\n",
       " 'backbone.stage3.1.blocks.0.conv2.pointwise_conv.bn.module_batch_norm_22.output_quantizers': ModuleList(\n",
       "   (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       " ),\n",
       " 'backbone.stage3.1.blocks.0.conv2.pointwise_conv.bn.module_batch_norm_22.output_quantizers.0': QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False),\n",
       " 'backbone.stage3.1.blocks.0.conv2.pointwise_conv.activate': CustomSiLU(\n",
       "   (sigmoid): QuantizedSigmoid(\n",
       "     (param_quantizers): ModuleDict()\n",
       "     (input_quantizers): ModuleList(\n",
       "       (0): None\n",
       "     )\n",
       "     (output_quantizers): ModuleList(\n",
       "       (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "     )\n",
       "   )\n",
       "   (mul): QuantizedMultiply(\n",
       "     (param_quantizers): ModuleDict()\n",
       "     (input_quantizers): ModuleList(\n",
       "       (0-1): 2 x None\n",
       "     )\n",
       "     (output_quantizers): ModuleList(\n",
       "       (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "     )\n",
       "   )\n",
       " ),\n",
       " 'backbone.stage3.1.blocks.0.conv2.pointwise_conv.activate.sigmoid': QuantizedSigmoid(\n",
       "   (param_quantizers): ModuleDict()\n",
       "   (input_quantizers): ModuleList(\n",
       "     (0): None\n",
       "   )\n",
       "   (output_quantizers): ModuleList(\n",
       "     (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "   )\n",
       " ),\n",
       " 'backbone.stage3.1.blocks.0.conv2.pointwise_conv.activate.sigmoid.param_quantizers': ModuleDict(),\n",
       " 'backbone.stage3.1.blocks.0.conv2.pointwise_conv.activate.sigmoid.input_quantizers': ModuleList(\n",
       "   (0): None\n",
       " ),\n",
       " 'backbone.stage3.1.blocks.0.conv2.pointwise_conv.activate.sigmoid.output_quantizers': ModuleList(\n",
       "   (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       " ),\n",
       " 'backbone.stage3.1.blocks.0.conv2.pointwise_conv.activate.sigmoid.output_quantizers.0': QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False),\n",
       " 'backbone.stage3.1.blocks.0.conv2.pointwise_conv.activate.mul': QuantizedMultiply(\n",
       "   (param_quantizers): ModuleDict()\n",
       "   (input_quantizers): ModuleList(\n",
       "     (0-1): 2 x None\n",
       "   )\n",
       "   (output_quantizers): ModuleList(\n",
       "     (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "   )\n",
       " ),\n",
       " 'backbone.stage3.1.blocks.0.conv2.pointwise_conv.activate.mul.param_quantizers': ModuleDict(),\n",
       " 'backbone.stage3.1.blocks.0.conv2.pointwise_conv.activate.mul.input_quantizers': ModuleList(\n",
       "   (0-1): 2 x None\n",
       " ),\n",
       " 'backbone.stage3.1.blocks.0.conv2.pointwise_conv.activate.mul.output_quantizers': ModuleList(\n",
       "   (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       " ),\n",
       " 'backbone.stage3.1.blocks.0.conv2.pointwise_conv.activate.mul.output_quantizers.0': QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False),\n",
       " 'backbone.stage3.1.blocks.0.module_add_2': QuantizedAdd(\n",
       "   (param_quantizers): ModuleDict()\n",
       "   (input_quantizers): ModuleList(\n",
       "     (0-1): 2 x None\n",
       "   )\n",
       "   (output_quantizers): ModuleList(\n",
       "     (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "   )\n",
       " ),\n",
       " 'backbone.stage3.1.blocks.0.module_add_2.param_quantizers': ModuleDict(),\n",
       " 'backbone.stage3.1.blocks.0.module_add_2.input_quantizers': ModuleList(\n",
       "   (0-1): 2 x None\n",
       " ),\n",
       " 'backbone.stage3.1.blocks.0.module_add_2.output_quantizers': ModuleList(\n",
       "   (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       " ),\n",
       " 'backbone.stage3.1.blocks.0.module_add_2.output_quantizers.0': QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False),\n",
       " 'backbone.stage3.1.attention': Module(\n",
       "   (global_avgpool): FakeQuantizedAdaptiveAvgPool2d(\n",
       "     output_size=1\n",
       "     (param_quantizers): ModuleDict()\n",
       "     (input_quantizers): ModuleList(\n",
       "       (0): None\n",
       "     )\n",
       "     (output_quantizers): ModuleList(\n",
       "       (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "     )\n",
       "   )\n",
       "   (fc): QuantizedConv2d(\n",
       "     192, 192, kernel_size=(1, 1), stride=(1, 1)\n",
       "     (param_quantizers): ModuleDict(\n",
       "       (weight): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=True)\n",
       "       (bias): None\n",
       "     )\n",
       "     (input_quantizers): ModuleList(\n",
       "       (0): None\n",
       "     )\n",
       "     (output_quantizers): ModuleList(\n",
       "       (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "     )\n",
       "   )\n",
       "   (act): FakeQuantizedHardsigmoid(\n",
       "     (param_quantizers): ModuleDict()\n",
       "     (input_quantizers): ModuleList(\n",
       "       (0): None\n",
       "     )\n",
       "     (output_quantizers): ModuleList(\n",
       "       (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "     )\n",
       "   )\n",
       "   (module_mul_2): QuantizedMultiply(\n",
       "     (param_quantizers): ModuleDict()\n",
       "     (input_quantizers): ModuleList(\n",
       "       (0-1): 2 x None\n",
       "     )\n",
       "     (output_quantizers): ModuleList(\n",
       "       (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "     )\n",
       "   )\n",
       " ),\n",
       " 'backbone.stage3.1.attention.global_avgpool': FakeQuantizedAdaptiveAvgPool2d(\n",
       "   output_size=1\n",
       "   (param_quantizers): ModuleDict()\n",
       "   (input_quantizers): ModuleList(\n",
       "     (0): None\n",
       "   )\n",
       "   (output_quantizers): ModuleList(\n",
       "     (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "   )\n",
       " ),\n",
       " 'backbone.stage3.1.attention.global_avgpool.param_quantizers': ModuleDict(),\n",
       " 'backbone.stage3.1.attention.global_avgpool.input_quantizers': ModuleList(\n",
       "   (0): None\n",
       " ),\n",
       " 'backbone.stage3.1.attention.global_avgpool.output_quantizers': ModuleList(\n",
       "   (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       " ),\n",
       " 'backbone.stage3.1.attention.global_avgpool.output_quantizers.0': QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False),\n",
       " 'backbone.stage3.1.attention.fc': QuantizedConv2d(\n",
       "   192, 192, kernel_size=(1, 1), stride=(1, 1)\n",
       "   (param_quantizers): ModuleDict(\n",
       "     (weight): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=True)\n",
       "     (bias): None\n",
       "   )\n",
       "   (input_quantizers): ModuleList(\n",
       "     (0): None\n",
       "   )\n",
       "   (output_quantizers): ModuleList(\n",
       "     (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "   )\n",
       " ),\n",
       " 'backbone.stage3.1.attention.fc.param_quantizers': ModuleDict(\n",
       "   (weight): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=True)\n",
       "   (bias): None\n",
       " ),\n",
       " 'backbone.stage3.1.attention.fc.param_quantizers.weight': QuantizeDequantize(shape=[1], bitwidth=8, symmetric=True),\n",
       " 'backbone.stage3.1.attention.fc.input_quantizers': ModuleList(\n",
       "   (0): None\n",
       " ),\n",
       " 'backbone.stage3.1.attention.fc.output_quantizers': ModuleList(\n",
       "   (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       " ),\n",
       " 'backbone.stage3.1.attention.fc.output_quantizers.0': QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False),\n",
       " 'backbone.stage3.1.attention.act': FakeQuantizedHardsigmoid(\n",
       "   (param_quantizers): ModuleDict()\n",
       "   (input_quantizers): ModuleList(\n",
       "     (0): None\n",
       "   )\n",
       "   (output_quantizers): ModuleList(\n",
       "     (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "   )\n",
       " ),\n",
       " 'backbone.stage3.1.attention.act.param_quantizers': ModuleDict(),\n",
       " 'backbone.stage3.1.attention.act.input_quantizers': ModuleList(\n",
       "   (0): None\n",
       " ),\n",
       " 'backbone.stage3.1.attention.act.output_quantizers': ModuleList(\n",
       "   (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       " ),\n",
       " 'backbone.stage3.1.attention.act.output_quantizers.0': QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False),\n",
       " 'backbone.stage3.1.attention.module_mul_2': QuantizedMultiply(\n",
       "   (param_quantizers): ModuleDict()\n",
       "   (input_quantizers): ModuleList(\n",
       "     (0-1): 2 x None\n",
       "   )\n",
       "   (output_quantizers): ModuleList(\n",
       "     (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "   )\n",
       " ),\n",
       " 'backbone.stage3.1.attention.module_mul_2.param_quantizers': ModuleDict(),\n",
       " 'backbone.stage3.1.attention.module_mul_2.input_quantizers': ModuleList(\n",
       "   (0-1): 2 x None\n",
       " ),\n",
       " 'backbone.stage3.1.attention.module_mul_2.output_quantizers': ModuleList(\n",
       "   (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       " ),\n",
       " 'backbone.stage3.1.attention.module_mul_2.output_quantizers.0': QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False),\n",
       " 'backbone.stage3.1.final_conv': Module(\n",
       "   (conv): QuantizedConv2d(\n",
       "     192, 192, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "     (param_quantizers): ModuleDict(\n",
       "       (weight): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=True)\n",
       "     )\n",
       "     (input_quantizers): ModuleList(\n",
       "       (0): None\n",
       "     )\n",
       "     (output_quantizers): ModuleList(\n",
       "       (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "     )\n",
       "   )\n",
       "   (bn): Module(\n",
       "     (module_batch_norm_23): FakeQuantizedBatchNorm(\n",
       "       (param_quantizers): ModuleDict()\n",
       "       (input_quantizers): ModuleList(\n",
       "         (0-4): 5 x QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "       )\n",
       "       (output_quantizers): ModuleList(\n",
       "         (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "       )\n",
       "     )\n",
       "   )\n",
       "   (activate): CustomSiLU(\n",
       "     (sigmoid): QuantizedSigmoid(\n",
       "       (param_quantizers): ModuleDict()\n",
       "       (input_quantizers): ModuleList(\n",
       "         (0): None\n",
       "       )\n",
       "       (output_quantizers): ModuleList(\n",
       "         (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "       )\n",
       "     )\n",
       "     (mul): QuantizedMultiply(\n",
       "       (param_quantizers): ModuleDict()\n",
       "       (input_quantizers): ModuleList(\n",
       "         (0-1): 2 x None\n",
       "       )\n",
       "       (output_quantizers): ModuleList(\n",
       "         (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "       )\n",
       "     )\n",
       "   )\n",
       " ),\n",
       " 'backbone.stage3.1.final_conv.conv': QuantizedConv2d(\n",
       "   192, 192, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "   (param_quantizers): ModuleDict(\n",
       "     (weight): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=True)\n",
       "   )\n",
       "   (input_quantizers): ModuleList(\n",
       "     (0): None\n",
       "   )\n",
       "   (output_quantizers): ModuleList(\n",
       "     (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "   )\n",
       " ),\n",
       " 'backbone.stage3.1.final_conv.conv.param_quantizers': ModuleDict(\n",
       "   (weight): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=True)\n",
       " ),\n",
       " 'backbone.stage3.1.final_conv.conv.param_quantizers.weight': QuantizeDequantize(shape=[1], bitwidth=8, symmetric=True),\n",
       " 'backbone.stage3.1.final_conv.conv.input_quantizers': ModuleList(\n",
       "   (0): None\n",
       " ),\n",
       " 'backbone.stage3.1.final_conv.conv.output_quantizers': ModuleList(\n",
       "   (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       " ),\n",
       " 'backbone.stage3.1.final_conv.conv.output_quantizers.0': QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False),\n",
       " 'backbone.stage3.1.final_conv.bn': Module(\n",
       "   (module_batch_norm_23): FakeQuantizedBatchNorm(\n",
       "     (param_quantizers): ModuleDict()\n",
       "     (input_quantizers): ModuleList(\n",
       "       (0-4): 5 x QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "     )\n",
       "     (output_quantizers): ModuleList(\n",
       "       (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "     )\n",
       "   )\n",
       " ),\n",
       " 'backbone.stage3.1.final_conv.bn.module_batch_norm_23': FakeQuantizedBatchNorm(\n",
       "   (param_quantizers): ModuleDict()\n",
       "   (input_quantizers): ModuleList(\n",
       "     (0-4): 5 x QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "   )\n",
       "   (output_quantizers): ModuleList(\n",
       "     (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "   )\n",
       " ),\n",
       " 'backbone.stage3.1.final_conv.bn.module_batch_norm_23.param_quantizers': ModuleDict(),\n",
       " 'backbone.stage3.1.final_conv.bn.module_batch_norm_23.input_quantizers': ModuleList(\n",
       "   (0-4): 5 x QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       " ),\n",
       " 'backbone.stage3.1.final_conv.bn.module_batch_norm_23.input_quantizers.0': QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False),\n",
       " 'backbone.stage3.1.final_conv.bn.module_batch_norm_23.input_quantizers.1': QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False),\n",
       " 'backbone.stage3.1.final_conv.bn.module_batch_norm_23.input_quantizers.2': QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False),\n",
       " 'backbone.stage3.1.final_conv.bn.module_batch_norm_23.input_quantizers.3': QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False),\n",
       " 'backbone.stage3.1.final_conv.bn.module_batch_norm_23.input_quantizers.4': QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False),\n",
       " 'backbone.stage3.1.final_conv.bn.module_batch_norm_23.output_quantizers': ModuleList(\n",
       "   (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       " ),\n",
       " 'backbone.stage3.1.final_conv.bn.module_batch_norm_23.output_quantizers.0': QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False),\n",
       " 'backbone.stage3.1.final_conv.activate': CustomSiLU(\n",
       "   (sigmoid): QuantizedSigmoid(\n",
       "     (param_quantizers): ModuleDict()\n",
       "     (input_quantizers): ModuleList(\n",
       "       (0): None\n",
       "     )\n",
       "     (output_quantizers): ModuleList(\n",
       "       (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "     )\n",
       "   )\n",
       "   (mul): QuantizedMultiply(\n",
       "     (param_quantizers): ModuleDict()\n",
       "     (input_quantizers): ModuleList(\n",
       "       (0-1): 2 x None\n",
       "     )\n",
       "     (output_quantizers): ModuleList(\n",
       "       (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "     )\n",
       "   )\n",
       " ),\n",
       " 'backbone.stage3.1.final_conv.activate.sigmoid': QuantizedSigmoid(\n",
       "   (param_quantizers): ModuleDict()\n",
       "   (input_quantizers): ModuleList(\n",
       "     (0): None\n",
       "   )\n",
       "   (output_quantizers): ModuleList(\n",
       "     (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "   )\n",
       " ),\n",
       " 'backbone.stage3.1.final_conv.activate.sigmoid.param_quantizers': ModuleDict(),\n",
       " 'backbone.stage3.1.final_conv.activate.sigmoid.input_quantizers': ModuleList(\n",
       "   (0): None\n",
       " ),\n",
       " 'backbone.stage3.1.final_conv.activate.sigmoid.output_quantizers': ModuleList(\n",
       "   (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       " ),\n",
       " 'backbone.stage3.1.final_conv.activate.sigmoid.output_quantizers.0': QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False),\n",
       " 'backbone.stage3.1.final_conv.activate.mul': QuantizedMultiply(\n",
       "   (param_quantizers): ModuleDict()\n",
       "   (input_quantizers): ModuleList(\n",
       "     (0-1): 2 x None\n",
       "   )\n",
       "   (output_quantizers): ModuleList(\n",
       "     (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "   )\n",
       " ),\n",
       " 'backbone.stage3.1.final_conv.activate.mul.param_quantizers': ModuleDict(),\n",
       " 'backbone.stage3.1.final_conv.activate.mul.input_quantizers': ModuleList(\n",
       "   (0-1): 2 x None\n",
       " ),\n",
       " 'backbone.stage3.1.final_conv.activate.mul.output_quantizers': ModuleList(\n",
       "   (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       " ),\n",
       " 'backbone.stage3.1.final_conv.activate.mul.output_quantizers.0': QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False),\n",
       " 'backbone.stage3.1.module_cat_2': FakeQuantizedConcat(\n",
       "   (param_quantizers): ModuleDict()\n",
       "   (input_quantizers): ModuleList(\n",
       "     (0): None\n",
       "   )\n",
       "   (output_quantizers): ModuleList(\n",
       "     (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "   )\n",
       " ),\n",
       " 'backbone.stage3.1.module_cat_2.param_quantizers': ModuleDict(),\n",
       " 'backbone.stage3.1.module_cat_2.input_quantizers': ModuleList(\n",
       "   (0): None\n",
       " ),\n",
       " 'backbone.stage3.1.module_cat_2.output_quantizers': ModuleList(\n",
       "   (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       " ),\n",
       " 'backbone.stage3.1.module_cat_2.output_quantizers.0': QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False),\n",
       " 'backbone.stage4': Module(\n",
       "   (0): Module(\n",
       "     (conv): QuantizedConv2d(\n",
       "       192, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False\n",
       "       (param_quantizers): ModuleDict(\n",
       "         (weight): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=True)\n",
       "       )\n",
       "       (input_quantizers): ModuleList(\n",
       "         (0): None\n",
       "       )\n",
       "       (output_quantizers): ModuleList(\n",
       "         (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "       )\n",
       "     )\n",
       "     (bn): Module(\n",
       "       (module_batch_norm_24): FakeQuantizedBatchNorm(\n",
       "         (param_quantizers): ModuleDict()\n",
       "         (input_quantizers): ModuleList(\n",
       "           (0-4): 5 x QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "         )\n",
       "         (output_quantizers): ModuleList(\n",
       "           (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "         )\n",
       "       )\n",
       "     )\n",
       "     (activate): CustomSiLU(\n",
       "       (sigmoid): QuantizedSigmoid(\n",
       "         (param_quantizers): ModuleDict()\n",
       "         (input_quantizers): ModuleList(\n",
       "           (0): None\n",
       "         )\n",
       "         (output_quantizers): ModuleList(\n",
       "           (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "         )\n",
       "       )\n",
       "       (mul): QuantizedMultiply(\n",
       "         (param_quantizers): ModuleDict()\n",
       "         (input_quantizers): ModuleList(\n",
       "           (0-1): 2 x None\n",
       "         )\n",
       "         (output_quantizers): ModuleList(\n",
       "           (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "         )\n",
       "       )\n",
       "     )\n",
       "   )\n",
       "   (1): Module(\n",
       "     (conv1): Module(\n",
       "       (conv): QuantizedConv2d(\n",
       "         384, 192, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "         (param_quantizers): ModuleDict(\n",
       "           (weight): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=True)\n",
       "         )\n",
       "         (input_quantizers): ModuleList(\n",
       "           (0): None\n",
       "         )\n",
       "         (output_quantizers): ModuleList(\n",
       "           (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "         )\n",
       "       )\n",
       "       (bn): Module(\n",
       "         (module_batch_norm_25): FakeQuantizedBatchNorm(\n",
       "           (param_quantizers): ModuleDict()\n",
       "           (input_quantizers): ModuleList(\n",
       "             (0-4): 5 x QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "           )\n",
       "           (output_quantizers): ModuleList(\n",
       "             (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "           )\n",
       "         )\n",
       "       )\n",
       "       (activate): CustomSiLU(\n",
       "         (sigmoid): QuantizedSigmoid(\n",
       "           (param_quantizers): ModuleDict()\n",
       "           (input_quantizers): ModuleList(\n",
       "             (0): None\n",
       "           )\n",
       "           (output_quantizers): ModuleList(\n",
       "             (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "           )\n",
       "         )\n",
       "         (mul): QuantizedMultiply(\n",
       "           (param_quantizers): ModuleDict()\n",
       "           (input_quantizers): ModuleList(\n",
       "             (0-1): 2 x None\n",
       "           )\n",
       "           (output_quantizers): ModuleList(\n",
       "             (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "           )\n",
       "         )\n",
       "       )\n",
       "     )\n",
       "     (poolings): Module(\n",
       "       (0): FakeQuantizedMaxPool2d(\n",
       "         kernel_size=5, stride=1, padding=2, dilation=1, ceil_mode=False\n",
       "         (param_quantizers): ModuleDict()\n",
       "         (input_quantizers): ModuleList(\n",
       "           (0): None\n",
       "         )\n",
       "         (output_quantizers): ModuleList(\n",
       "           (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "         )\n",
       "       )\n",
       "       (1): FakeQuantizedMaxPool2d(\n",
       "         kernel_size=9, stride=1, padding=4, dilation=1, ceil_mode=False\n",
       "         (param_quantizers): ModuleDict()\n",
       "         (input_quantizers): ModuleList(\n",
       "           (0): None\n",
       "         )\n",
       "         (output_quantizers): ModuleList(\n",
       "           (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "         )\n",
       "       )\n",
       "       (2): FakeQuantizedMaxPool2d(\n",
       "         kernel_size=13, stride=1, padding=6, dilation=1, ceil_mode=False\n",
       "         (param_quantizers): ModuleDict()\n",
       "         (input_quantizers): ModuleList(\n",
       "           (0): None\n",
       "         )\n",
       "         (output_quantizers): ModuleList(\n",
       "           (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "         )\n",
       "       )\n",
       "     )\n",
       "     (conv2): Module(\n",
       "       (conv): QuantizedConv2d(\n",
       "         768, 384, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "         (param_quantizers): ModuleDict(\n",
       "           (weight): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=True)\n",
       "         )\n",
       "         (input_quantizers): ModuleList(\n",
       "           (0): None\n",
       "         )\n",
       "         (output_quantizers): ModuleList(\n",
       "           (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "         )\n",
       "       )\n",
       "       (bn): Module(\n",
       "         (module_batch_norm_26): FakeQuantizedBatchNorm(\n",
       "           (param_quantizers): ModuleDict()\n",
       "           (input_quantizers): ModuleList(\n",
       "             (0-4): 5 x QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "           )\n",
       "           (output_quantizers): ModuleList(\n",
       "             (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "           )\n",
       "         )\n",
       "       )\n",
       "       (activate): CustomSiLU(\n",
       "         (sigmoid): QuantizedSigmoid(\n",
       "           (param_quantizers): ModuleDict()\n",
       "           (input_quantizers): ModuleList(\n",
       "             (0): None\n",
       "           )\n",
       "           (output_quantizers): ModuleList(\n",
       "             (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "           )\n",
       "         )\n",
       "         (mul): QuantizedMultiply(\n",
       "           (param_quantizers): ModuleDict()\n",
       "           (input_quantizers): ModuleList(\n",
       "             (0-1): 2 x None\n",
       "           )\n",
       "           (output_quantizers): ModuleList(\n",
       "             (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "           )\n",
       "         )\n",
       "       )\n",
       "     )\n",
       "     (module_cat_3): FakeQuantizedConcat(\n",
       "       (param_quantizers): ModuleDict()\n",
       "       (input_quantizers): ModuleList(\n",
       "         (0): None\n",
       "       )\n",
       "       (output_quantizers): ModuleList(\n",
       "         (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "       )\n",
       "     )\n",
       "   )\n",
       "   (2): Module(\n",
       "     (short_conv): Module(\n",
       "       (conv): QuantizedConv2d(\n",
       "         384, 192, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "         (param_quantizers): ModuleDict(\n",
       "           (weight): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=True)\n",
       "         )\n",
       "         (input_quantizers): ModuleList(\n",
       "           (0): None\n",
       "         )\n",
       "         (output_quantizers): ModuleList(\n",
       "           (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "         )\n",
       "       )\n",
       "       (bn): Module(\n",
       "         (module_batch_norm_27): FakeQuantizedBatchNorm(\n",
       "           (param_quantizers): ModuleDict()\n",
       "           (input_quantizers): ModuleList(\n",
       "             (0-4): 5 x QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "           )\n",
       "           (output_quantizers): ModuleList(\n",
       "             (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "           )\n",
       "         )\n",
       "       )\n",
       "       (activate): CustomSiLU(\n",
       "         (sigmoid): QuantizedSigmoid(\n",
       "           (param_quantizers): ModuleDict()\n",
       "           (input_quantizers): ModuleList(\n",
       "             (0): None\n",
       "           )\n",
       "           (output_quantizers): ModuleList(\n",
       "             (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "           )\n",
       "         )\n",
       "         (mul): QuantizedMultiply(\n",
       "           (param_quantizers): ModuleDict()\n",
       "           (input_quantizers): ModuleList(\n",
       "             (0-1): 2 x None\n",
       "           )\n",
       "           (output_quantizers): ModuleList(\n",
       "             (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "           )\n",
       "         )\n",
       "       )\n",
       "     )\n",
       "     (main_conv): Module(\n",
       "       (conv): QuantizedConv2d(\n",
       "         384, 192, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "         (param_quantizers): ModuleDict(\n",
       "           (weight): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=True)\n",
       "         )\n",
       "         (input_quantizers): ModuleList(\n",
       "           (0): None\n",
       "         )\n",
       "         (output_quantizers): ModuleList(\n",
       "           (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "         )\n",
       "       )\n",
       "       (bn): Module(\n",
       "         (module_batch_norm_28): FakeQuantizedBatchNorm(\n",
       "           (param_quantizers): ModuleDict()\n",
       "           (input_quantizers): ModuleList(\n",
       "             (0-4): 5 x QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "           )\n",
       "           (output_quantizers): ModuleList(\n",
       "             (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "           )\n",
       "         )\n",
       "       )\n",
       "       (activate): CustomSiLU(\n",
       "         (sigmoid): QuantizedSigmoid(\n",
       "           (param_quantizers): ModuleDict()\n",
       "           (input_quantizers): ModuleList(\n",
       "             (0): None\n",
       "           )\n",
       "           (output_quantizers): ModuleList(\n",
       "             (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "           )\n",
       "         )\n",
       "         (mul): QuantizedMultiply(\n",
       "           (param_quantizers): ModuleDict()\n",
       "           (input_quantizers): ModuleList(\n",
       "             (0-1): 2 x None\n",
       "           )\n",
       "           (output_quantizers): ModuleList(\n",
       "             (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "           )\n",
       "         )\n",
       "       )\n",
       "     )\n",
       "     (blocks): Module(\n",
       "       (0): Module(\n",
       "         (conv1): Module(\n",
       "           (conv): QuantizedConv2d(\n",
       "             192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "             (param_quantizers): ModuleDict(\n",
       "               (weight): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=True)\n",
       "             )\n",
       "             (input_quantizers): ModuleList(\n",
       "               (0): None\n",
       "             )\n",
       "             (output_quantizers): ModuleList(\n",
       "               (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "             )\n",
       "           )\n",
       "           (bn): Module(\n",
       "             (module_batch_norm_29): FakeQuantizedBatchNorm(\n",
       "               (param_quantizers): ModuleDict()\n",
       "               (input_quantizers): ModuleList(\n",
       "                 (0-4): 5 x QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "               )\n",
       "               (output_quantizers): ModuleList(\n",
       "                 (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "               )\n",
       "             )\n",
       "           )\n",
       "           (activate): CustomSiLU(\n",
       "             (sigmoid): QuantizedSigmoid(\n",
       "               (param_quantizers): ModuleDict()\n",
       "               (input_quantizers): ModuleList(\n",
       "                 (0): None\n",
       "               )\n",
       "               (output_quantizers): ModuleList(\n",
       "                 (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "               )\n",
       "             )\n",
       "             (mul): QuantizedMultiply(\n",
       "               (param_quantizers): ModuleDict()\n",
       "               (input_quantizers): ModuleList(\n",
       "                 (0-1): 2 x None\n",
       "               )\n",
       "               (output_quantizers): ModuleList(\n",
       "                 (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "               )\n",
       "             )\n",
       "           )\n",
       "         )\n",
       "         (conv2): Module(\n",
       "           (depthwise_conv): Module(\n",
       "             (conv): QuantizedConv2d(\n",
       "               192, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=192, bias=False\n",
       "               (param_quantizers): ModuleDict(\n",
       "                 (weight): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=True)\n",
       "               )\n",
       "               (input_quantizers): ModuleList(\n",
       "                 (0): None\n",
       "               )\n",
       "               (output_quantizers): ModuleList(\n",
       "                 (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "               )\n",
       "             )\n",
       "             (bn): Module(\n",
       "               (module_batch_norm_30): BatchNorm()\n",
       "             )\n",
       "             (activate): CustomSiLU(\n",
       "               (sigmoid): QuantizedSigmoid(\n",
       "                 (param_quantizers): ModuleDict()\n",
       "                 (input_quantizers): ModuleList(\n",
       "                   (0): None\n",
       "                 )\n",
       "                 (output_quantizers): ModuleList(\n",
       "                   (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "                 )\n",
       "               )\n",
       "               (mul): QuantizedMultiply(\n",
       "                 (param_quantizers): ModuleDict()\n",
       "                 (input_quantizers): ModuleList(\n",
       "                   (0-1): 2 x None\n",
       "                 )\n",
       "                 (output_quantizers): ModuleList(\n",
       "                   (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "                 )\n",
       "               )\n",
       "             )\n",
       "           )\n",
       "           (pointwise_conv): Module(\n",
       "             (conv): QuantizedConv2d(\n",
       "               192, 192, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "               (param_quantizers): ModuleDict(\n",
       "                 (weight): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=True)\n",
       "               )\n",
       "               (input_quantizers): ModuleList(\n",
       "                 (0): None\n",
       "               )\n",
       "               (output_quantizers): ModuleList(\n",
       "                 (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "               )\n",
       "             )\n",
       "             (bn): Module(\n",
       "               (module_batch_norm_31): FakeQuantizedBatchNorm(\n",
       "                 (param_quantizers): ModuleDict()\n",
       "                 (input_quantizers): ModuleList(\n",
       "                   (0-4): 5 x QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "                 )\n",
       "                 (output_quantizers): ModuleList(\n",
       "                   (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "                 )\n",
       "               )\n",
       "             )\n",
       "             (activate): CustomSiLU(\n",
       "               (sigmoid): QuantizedSigmoid(\n",
       "                 (param_quantizers): ModuleDict()\n",
       "                 (input_quantizers): ModuleList(\n",
       "                   (0): None\n",
       "                 )\n",
       "                 (output_quantizers): ModuleList(\n",
       "                   (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "                 )\n",
       "               )\n",
       "               (mul): QuantizedMultiply(\n",
       "                 (param_quantizers): ModuleDict()\n",
       "                 (input_quantizers): ModuleList(\n",
       "                   (0-1): 2 x None\n",
       "                 )\n",
       "                 (output_quantizers): ModuleList(\n",
       "                   (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "                 )\n",
       "               )\n",
       "             )\n",
       "           )\n",
       "         )\n",
       "       )\n",
       "     )\n",
       "     (attention): Module(\n",
       "       (global_avgpool): FakeQuantizedAdaptiveAvgPool2d(\n",
       "         output_size=1\n",
       "         (param_quantizers): ModuleDict()\n",
       "         (input_quantizers): ModuleList(\n",
       "           (0): None\n",
       "         )\n",
       "         (output_quantizers): ModuleList(\n",
       "           (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "         )\n",
       "       )\n",
       "       (fc): QuantizedConv2d(\n",
       "         384, 384, kernel_size=(1, 1), stride=(1, 1)\n",
       "         (param_quantizers): ModuleDict(\n",
       "           (weight): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=True)\n",
       "           (bias): None\n",
       "         )\n",
       "         (input_quantizers): ModuleList(\n",
       "           (0): None\n",
       "         )\n",
       "         (output_quantizers): ModuleList(\n",
       "           (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "         )\n",
       "       )\n",
       "       (act): FakeQuantizedHardsigmoid(\n",
       "         (param_quantizers): ModuleDict()\n",
       "         (input_quantizers): ModuleList(\n",
       "           (0): None\n",
       "         )\n",
       "         (output_quantizers): ModuleList(\n",
       "           (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "         )\n",
       "       )\n",
       "       (module_mul_3): QuantizedMultiply(\n",
       "         (param_quantizers): ModuleDict()\n",
       "         (input_quantizers): ModuleList(\n",
       "           (0-1): 2 x None\n",
       "         )\n",
       "         (output_quantizers): ModuleList(\n",
       "           (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "         )\n",
       "       )\n",
       "     )\n",
       "     (final_conv): Module(\n",
       "       (conv): QuantizedConv2d(\n",
       "         384, 384, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "         (param_quantizers): ModuleDict(\n",
       "           (weight): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=True)\n",
       "         )\n",
       "         (input_quantizers): ModuleList(\n",
       "           (0): None\n",
       "         )\n",
       "         (output_quantizers): ModuleList(\n",
       "           (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "         )\n",
       "       )\n",
       "       (bn): Module(\n",
       "         (module_batch_norm_32): FakeQuantizedBatchNorm(\n",
       "           (param_quantizers): ModuleDict()\n",
       "           (input_quantizers): ModuleList(\n",
       "             (0-4): 5 x QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "           )\n",
       "           (output_quantizers): ModuleList(\n",
       "             (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "           )\n",
       "         )\n",
       "       )\n",
       "       (activate): CustomSiLU(\n",
       "         (sigmoid): QuantizedSigmoid(\n",
       "           (param_quantizers): ModuleDict()\n",
       "           (input_quantizers): ModuleList(\n",
       "             (0): None\n",
       "           )\n",
       "           (output_quantizers): ModuleList(\n",
       "             (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "           )\n",
       "         )\n",
       "         (mul): QuantizedMultiply(\n",
       "           (param_quantizers): ModuleDict()\n",
       "           (input_quantizers): ModuleList(\n",
       "             (0-1): 2 x None\n",
       "           )\n",
       "           (output_quantizers): ModuleList(\n",
       "             (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "           )\n",
       "         )\n",
       "       )\n",
       "     )\n",
       "     (module_cat_4): FakeQuantizedConcat(\n",
       "       (param_quantizers): ModuleDict()\n",
       "       (input_quantizers): ModuleList(\n",
       "         (0): None\n",
       "       )\n",
       "       (output_quantizers): ModuleList(\n",
       "         (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "       )\n",
       "     )\n",
       "   )\n",
       " ),\n",
       " 'backbone.stage4.0': Module(\n",
       "   (conv): QuantizedConv2d(\n",
       "     192, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False\n",
       "     (param_quantizers): ModuleDict(\n",
       "       (weight): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=True)\n",
       "     )\n",
       "     (input_quantizers): ModuleList(\n",
       "       (0): None\n",
       "     )\n",
       "     (output_quantizers): ModuleList(\n",
       "       (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "     )\n",
       "   )\n",
       "   (bn): Module(\n",
       "     (module_batch_norm_24): FakeQuantizedBatchNorm(\n",
       "       (param_quantizers): ModuleDict()\n",
       "       (input_quantizers): ModuleList(\n",
       "         (0-4): 5 x QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "       )\n",
       "       (output_quantizers): ModuleList(\n",
       "         (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "       )\n",
       "     )\n",
       "   )\n",
       "   (activate): CustomSiLU(\n",
       "     (sigmoid): QuantizedSigmoid(\n",
       "       (param_quantizers): ModuleDict()\n",
       "       (input_quantizers): ModuleList(\n",
       "         (0): None\n",
       "       )\n",
       "       (output_quantizers): ModuleList(\n",
       "         (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "       )\n",
       "     )\n",
       "     (mul): QuantizedMultiply(\n",
       "       (param_quantizers): ModuleDict()\n",
       "       (input_quantizers): ModuleList(\n",
       "         (0-1): 2 x None\n",
       "       )\n",
       "       (output_quantizers): ModuleList(\n",
       "         (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "       )\n",
       "     )\n",
       "   )\n",
       " ),\n",
       " 'backbone.stage4.0.conv': QuantizedConv2d(\n",
       "   192, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False\n",
       "   (param_quantizers): ModuleDict(\n",
       "     (weight): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=True)\n",
       "   )\n",
       "   (input_quantizers): ModuleList(\n",
       "     (0): None\n",
       "   )\n",
       "   (output_quantizers): ModuleList(\n",
       "     (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "   )\n",
       " ),\n",
       " 'backbone.stage4.0.conv.param_quantizers': ModuleDict(\n",
       "   (weight): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=True)\n",
       " ),\n",
       " 'backbone.stage4.0.conv.param_quantizers.weight': QuantizeDequantize(shape=[1], bitwidth=8, symmetric=True),\n",
       " 'backbone.stage4.0.conv.input_quantizers': ModuleList(\n",
       "   (0): None\n",
       " ),\n",
       " 'backbone.stage4.0.conv.output_quantizers': ModuleList(\n",
       "   (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       " ),\n",
       " 'backbone.stage4.0.conv.output_quantizers.0': QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False),\n",
       " 'backbone.stage4.0.bn': Module(\n",
       "   (module_batch_norm_24): FakeQuantizedBatchNorm(\n",
       "     (param_quantizers): ModuleDict()\n",
       "     (input_quantizers): ModuleList(\n",
       "       (0-4): 5 x QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "     )\n",
       "     (output_quantizers): ModuleList(\n",
       "       (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "     )\n",
       "   )\n",
       " ),\n",
       " 'backbone.stage4.0.bn.module_batch_norm_24': FakeQuantizedBatchNorm(\n",
       "   (param_quantizers): ModuleDict()\n",
       "   (input_quantizers): ModuleList(\n",
       "     (0-4): 5 x QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "   )\n",
       "   (output_quantizers): ModuleList(\n",
       "     (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "   )\n",
       " ),\n",
       " 'backbone.stage4.0.bn.module_batch_norm_24.param_quantizers': ModuleDict(),\n",
       " 'backbone.stage4.0.bn.module_batch_norm_24.input_quantizers': ModuleList(\n",
       "   (0-4): 5 x QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       " ),\n",
       " 'backbone.stage4.0.bn.module_batch_norm_24.input_quantizers.0': QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False),\n",
       " 'backbone.stage4.0.bn.module_batch_norm_24.input_quantizers.1': QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False),\n",
       " 'backbone.stage4.0.bn.module_batch_norm_24.input_quantizers.2': QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False),\n",
       " 'backbone.stage4.0.bn.module_batch_norm_24.input_quantizers.3': QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False),\n",
       " 'backbone.stage4.0.bn.module_batch_norm_24.input_quantizers.4': QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False),\n",
       " 'backbone.stage4.0.bn.module_batch_norm_24.output_quantizers': ModuleList(\n",
       "   (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       " ),\n",
       " 'backbone.stage4.0.bn.module_batch_norm_24.output_quantizers.0': QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False),\n",
       " 'backbone.stage4.0.activate': CustomSiLU(\n",
       "   (sigmoid): QuantizedSigmoid(\n",
       "     (param_quantizers): ModuleDict()\n",
       "     (input_quantizers): ModuleList(\n",
       "       (0): None\n",
       "     )\n",
       "     (output_quantizers): ModuleList(\n",
       "       (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "     )\n",
       "   )\n",
       "   (mul): QuantizedMultiply(\n",
       "     (param_quantizers): ModuleDict()\n",
       "     (input_quantizers): ModuleList(\n",
       "       (0-1): 2 x None\n",
       "     )\n",
       "     (output_quantizers): ModuleList(\n",
       "       (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "     )\n",
       "   )\n",
       " ),\n",
       " 'backbone.stage4.0.activate.sigmoid': QuantizedSigmoid(\n",
       "   (param_quantizers): ModuleDict()\n",
       "   (input_quantizers): ModuleList(\n",
       "     (0): None\n",
       "   )\n",
       "   (output_quantizers): ModuleList(\n",
       "     (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "   )\n",
       " ),\n",
       " 'backbone.stage4.0.activate.sigmoid.param_quantizers': ModuleDict(),\n",
       " 'backbone.stage4.0.activate.sigmoid.input_quantizers': ModuleList(\n",
       "   (0): None\n",
       " ),\n",
       " 'backbone.stage4.0.activate.sigmoid.output_quantizers': ModuleList(\n",
       "   (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       " ),\n",
       " 'backbone.stage4.0.activate.sigmoid.output_quantizers.0': QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False),\n",
       " 'backbone.stage4.0.activate.mul': QuantizedMultiply(\n",
       "   (param_quantizers): ModuleDict()\n",
       "   (input_quantizers): ModuleList(\n",
       "     (0-1): 2 x None\n",
       "   )\n",
       "   (output_quantizers): ModuleList(\n",
       "     (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "   )\n",
       " ),\n",
       " 'backbone.stage4.0.activate.mul.param_quantizers': ModuleDict(),\n",
       " 'backbone.stage4.0.activate.mul.input_quantizers': ModuleList(\n",
       "   (0-1): 2 x None\n",
       " ),\n",
       " 'backbone.stage4.0.activate.mul.output_quantizers': ModuleList(\n",
       "   (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       " ),\n",
       " 'backbone.stage4.0.activate.mul.output_quantizers.0': QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False),\n",
       " 'backbone.stage4.1': Module(\n",
       "   (conv1): Module(\n",
       "     (conv): QuantizedConv2d(\n",
       "       384, 192, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "       (param_quantizers): ModuleDict(\n",
       "         (weight): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=True)\n",
       "       )\n",
       "       (input_quantizers): ModuleList(\n",
       "         (0): None\n",
       "       )\n",
       "       (output_quantizers): ModuleList(\n",
       "         (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "       )\n",
       "     )\n",
       "     (bn): Module(\n",
       "       (module_batch_norm_25): FakeQuantizedBatchNorm(\n",
       "         (param_quantizers): ModuleDict()\n",
       "         (input_quantizers): ModuleList(\n",
       "           (0-4): 5 x QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "         )\n",
       "         (output_quantizers): ModuleList(\n",
       "           (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "         )\n",
       "       )\n",
       "     )\n",
       "     (activate): CustomSiLU(\n",
       "       (sigmoid): QuantizedSigmoid(\n",
       "         (param_quantizers): ModuleDict()\n",
       "         (input_quantizers): ModuleList(\n",
       "           (0): None\n",
       "         )\n",
       "         (output_quantizers): ModuleList(\n",
       "           (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "         )\n",
       "       )\n",
       "       (mul): QuantizedMultiply(\n",
       "         (param_quantizers): ModuleDict()\n",
       "         (input_quantizers): ModuleList(\n",
       "           (0-1): 2 x None\n",
       "         )\n",
       "         (output_quantizers): ModuleList(\n",
       "           (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "         )\n",
       "       )\n",
       "     )\n",
       "   )\n",
       "   (poolings): Module(\n",
       "     (0): FakeQuantizedMaxPool2d(\n",
       "       kernel_size=5, stride=1, padding=2, dilation=1, ceil_mode=False\n",
       "       (param_quantizers): ModuleDict()\n",
       "       (input_quantizers): ModuleList(\n",
       "         (0): None\n",
       "       )\n",
       "       (output_quantizers): ModuleList(\n",
       "         (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "       )\n",
       "     )\n",
       "     (1): FakeQuantizedMaxPool2d(\n",
       "       kernel_size=9, stride=1, padding=4, dilation=1, ceil_mode=False\n",
       "       (param_quantizers): ModuleDict()\n",
       "       (input_quantizers): ModuleList(\n",
       "         (0): None\n",
       "       )\n",
       "       (output_quantizers): ModuleList(\n",
       "         (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "       )\n",
       "     )\n",
       "     (2): FakeQuantizedMaxPool2d(\n",
       "       kernel_size=13, stride=1, padding=6, dilation=1, ceil_mode=False\n",
       "       (param_quantizers): ModuleDict()\n",
       "       (input_quantizers): ModuleList(\n",
       "         (0): None\n",
       "       )\n",
       "       (output_quantizers): ModuleList(\n",
       "         (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "       )\n",
       "     )\n",
       "   )\n",
       "   (conv2): Module(\n",
       "     (conv): QuantizedConv2d(\n",
       "       768, 384, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "       (param_quantizers): ModuleDict(\n",
       "         (weight): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=True)\n",
       "       )\n",
       "       (input_quantizers): ModuleList(\n",
       "         (0): None\n",
       "       )\n",
       "       (output_quantizers): ModuleList(\n",
       "         (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "       )\n",
       "     )\n",
       "     (bn): Module(\n",
       "       (module_batch_norm_26): FakeQuantizedBatchNorm(\n",
       "         (param_quantizers): ModuleDict()\n",
       "         (input_quantizers): ModuleList(\n",
       "           (0-4): 5 x QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "         )\n",
       "         (output_quantizers): ModuleList(\n",
       "           (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "         )\n",
       "       )\n",
       "     )\n",
       "     (activate): CustomSiLU(\n",
       "       (sigmoid): QuantizedSigmoid(\n",
       "         (param_quantizers): ModuleDict()\n",
       "         (input_quantizers): ModuleList(\n",
       "           (0): None\n",
       "         )\n",
       "         (output_quantizers): ModuleList(\n",
       "           (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "         )\n",
       "       )\n",
       "       (mul): QuantizedMultiply(\n",
       "         (param_quantizers): ModuleDict()\n",
       "         (input_quantizers): ModuleList(\n",
       "           (0-1): 2 x None\n",
       "         )\n",
       "         (output_quantizers): ModuleList(\n",
       "           (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "         )\n",
       "       )\n",
       "     )\n",
       "   )\n",
       "   (module_cat_3): FakeQuantizedConcat(\n",
       "     (param_quantizers): ModuleDict()\n",
       "     (input_quantizers): ModuleList(\n",
       "       (0): None\n",
       "     )\n",
       "     (output_quantizers): ModuleList(\n",
       "       (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "     )\n",
       "   )\n",
       " ),\n",
       " 'backbone.stage4.1.conv1': Module(\n",
       "   (conv): QuantizedConv2d(\n",
       "     384, 192, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "     (param_quantizers): ModuleDict(\n",
       "       (weight): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=True)\n",
       "     )\n",
       "     (input_quantizers): ModuleList(\n",
       "       (0): None\n",
       "     )\n",
       "     (output_quantizers): ModuleList(\n",
       "       (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "     )\n",
       "   )\n",
       "   (bn): Module(\n",
       "     (module_batch_norm_25): FakeQuantizedBatchNorm(\n",
       "       (param_quantizers): ModuleDict()\n",
       "       (input_quantizers): ModuleList(\n",
       "         (0-4): 5 x QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "       )\n",
       "       (output_quantizers): ModuleList(\n",
       "         (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "       )\n",
       "     )\n",
       "   )\n",
       "   (activate): CustomSiLU(\n",
       "     (sigmoid): QuantizedSigmoid(\n",
       "       (param_quantizers): ModuleDict()\n",
       "       (input_quantizers): ModuleList(\n",
       "         (0): None\n",
       "       )\n",
       "       (output_quantizers): ModuleList(\n",
       "         (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "       )\n",
       "     )\n",
       "     (mul): QuantizedMultiply(\n",
       "       (param_quantizers): ModuleDict()\n",
       "       (input_quantizers): ModuleList(\n",
       "         (0-1): 2 x None\n",
       "       )\n",
       "       (output_quantizers): ModuleList(\n",
       "         (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "       )\n",
       "     )\n",
       "   )\n",
       " ),\n",
       " 'backbone.stage4.1.conv1.conv': QuantizedConv2d(\n",
       "   384, 192, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "   (param_quantizers): ModuleDict(\n",
       "     (weight): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=True)\n",
       "   )\n",
       "   (input_quantizers): ModuleList(\n",
       "     (0): None\n",
       "   )\n",
       "   (output_quantizers): ModuleList(\n",
       "     (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "   )\n",
       " ),\n",
       " 'backbone.stage4.1.conv1.conv.param_quantizers': ModuleDict(\n",
       "   (weight): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=True)\n",
       " ),\n",
       " 'backbone.stage4.1.conv1.conv.param_quantizers.weight': QuantizeDequantize(shape=[1], bitwidth=8, symmetric=True),\n",
       " 'backbone.stage4.1.conv1.conv.input_quantizers': ModuleList(\n",
       "   (0): None\n",
       " ),\n",
       " 'backbone.stage4.1.conv1.conv.output_quantizers': ModuleList(\n",
       "   (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       " ),\n",
       " 'backbone.stage4.1.conv1.conv.output_quantizers.0': QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False),\n",
       " 'backbone.stage4.1.conv1.bn': Module(\n",
       "   (module_batch_norm_25): FakeQuantizedBatchNorm(\n",
       "     (param_quantizers): ModuleDict()\n",
       "     (input_quantizers): ModuleList(\n",
       "       (0-4): 5 x QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "     )\n",
       "     (output_quantizers): ModuleList(\n",
       "       (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "     )\n",
       "   )\n",
       " ),\n",
       " 'backbone.stage4.1.conv1.bn.module_batch_norm_25': FakeQuantizedBatchNorm(\n",
       "   (param_quantizers): ModuleDict()\n",
       "   (input_quantizers): ModuleList(\n",
       "     (0-4): 5 x QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "   )\n",
       "   (output_quantizers): ModuleList(\n",
       "     (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "   )\n",
       " ),\n",
       " 'backbone.stage4.1.conv1.bn.module_batch_norm_25.param_quantizers': ModuleDict(),\n",
       " 'backbone.stage4.1.conv1.bn.module_batch_norm_25.input_quantizers': ModuleList(\n",
       "   (0-4): 5 x QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       " ),\n",
       " 'backbone.stage4.1.conv1.bn.module_batch_norm_25.input_quantizers.0': QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False),\n",
       " 'backbone.stage4.1.conv1.bn.module_batch_norm_25.input_quantizers.1': QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False),\n",
       " 'backbone.stage4.1.conv1.bn.module_batch_norm_25.input_quantizers.2': QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False),\n",
       " 'backbone.stage4.1.conv1.bn.module_batch_norm_25.input_quantizers.3': QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False),\n",
       " 'backbone.stage4.1.conv1.bn.module_batch_norm_25.input_quantizers.4': QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False),\n",
       " 'backbone.stage4.1.conv1.bn.module_batch_norm_25.output_quantizers': ModuleList(\n",
       "   (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       " ),\n",
       " 'backbone.stage4.1.conv1.bn.module_batch_norm_25.output_quantizers.0': QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False),\n",
       " 'backbone.stage4.1.conv1.activate': CustomSiLU(\n",
       "   (sigmoid): QuantizedSigmoid(\n",
       "     (param_quantizers): ModuleDict()\n",
       "     (input_quantizers): ModuleList(\n",
       "       (0): None\n",
       "     )\n",
       "     (output_quantizers): ModuleList(\n",
       "       (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "     )\n",
       "   )\n",
       "   (mul): QuantizedMultiply(\n",
       "     (param_quantizers): ModuleDict()\n",
       "     (input_quantizers): ModuleList(\n",
       "       (0-1): 2 x None\n",
       "     )\n",
       "     (output_quantizers): ModuleList(\n",
       "       (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "     )\n",
       "   )\n",
       " ),\n",
       " 'backbone.stage4.1.conv1.activate.sigmoid': QuantizedSigmoid(\n",
       "   (param_quantizers): ModuleDict()\n",
       "   (input_quantizers): ModuleList(\n",
       "     (0): None\n",
       "   )\n",
       "   (output_quantizers): ModuleList(\n",
       "     (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "   )\n",
       " ),\n",
       " 'backbone.stage4.1.conv1.activate.sigmoid.param_quantizers': ModuleDict(),\n",
       " 'backbone.stage4.1.conv1.activate.sigmoid.input_quantizers': ModuleList(\n",
       "   (0): None\n",
       " ),\n",
       " 'backbone.stage4.1.conv1.activate.sigmoid.output_quantizers': ModuleList(\n",
       "   (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       " ),\n",
       " 'backbone.stage4.1.conv1.activate.sigmoid.output_quantizers.0': QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False),\n",
       " 'backbone.stage4.1.conv1.activate.mul': QuantizedMultiply(\n",
       "   (param_quantizers): ModuleDict()\n",
       "   (input_quantizers): ModuleList(\n",
       "     (0-1): 2 x None\n",
       "   )\n",
       "   (output_quantizers): ModuleList(\n",
       "     (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "   )\n",
       " ),\n",
       " 'backbone.stage4.1.conv1.activate.mul.param_quantizers': ModuleDict(),\n",
       " 'backbone.stage4.1.conv1.activate.mul.input_quantizers': ModuleList(\n",
       "   (0-1): 2 x None\n",
       " ),\n",
       " 'backbone.stage4.1.conv1.activate.mul.output_quantizers': ModuleList(\n",
       "   (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       " ),\n",
       " 'backbone.stage4.1.conv1.activate.mul.output_quantizers.0': QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False),\n",
       " 'backbone.stage4.1.poolings': Module(\n",
       "   (0): FakeQuantizedMaxPool2d(\n",
       "     kernel_size=5, stride=1, padding=2, dilation=1, ceil_mode=False\n",
       "     (param_quantizers): ModuleDict()\n",
       "     (input_quantizers): ModuleList(\n",
       "       (0): None\n",
       "     )\n",
       "     (output_quantizers): ModuleList(\n",
       "       (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "     )\n",
       "   )\n",
       "   (1): FakeQuantizedMaxPool2d(\n",
       "     kernel_size=9, stride=1, padding=4, dilation=1, ceil_mode=False\n",
       "     (param_quantizers): ModuleDict()\n",
       "     (input_quantizers): ModuleList(\n",
       "       (0): None\n",
       "     )\n",
       "     (output_quantizers): ModuleList(\n",
       "       (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "     )\n",
       "   )\n",
       "   (2): FakeQuantizedMaxPool2d(\n",
       "     kernel_size=13, stride=1, padding=6, dilation=1, ceil_mode=False\n",
       "     (param_quantizers): ModuleDict()\n",
       "     (input_quantizers): ModuleList(\n",
       "       (0): None\n",
       "     )\n",
       "     (output_quantizers): ModuleList(\n",
       "       (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "     )\n",
       "   )\n",
       " ),\n",
       " 'backbone.stage4.1.poolings.0': FakeQuantizedMaxPool2d(\n",
       "   kernel_size=5, stride=1, padding=2, dilation=1, ceil_mode=False\n",
       "   (param_quantizers): ModuleDict()\n",
       "   (input_quantizers): ModuleList(\n",
       "     (0): None\n",
       "   )\n",
       "   (output_quantizers): ModuleList(\n",
       "     (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "   )\n",
       " ),\n",
       " 'backbone.stage4.1.poolings.0.param_quantizers': ModuleDict(),\n",
       " 'backbone.stage4.1.poolings.0.input_quantizers': ModuleList(\n",
       "   (0): None\n",
       " ),\n",
       " 'backbone.stage4.1.poolings.0.output_quantizers': ModuleList(\n",
       "   (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       " ),\n",
       " 'backbone.stage4.1.poolings.0.output_quantizers.0': QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False),\n",
       " 'backbone.stage4.1.poolings.1': FakeQuantizedMaxPool2d(\n",
       "   kernel_size=9, stride=1, padding=4, dilation=1, ceil_mode=False\n",
       "   (param_quantizers): ModuleDict()\n",
       "   (input_quantizers): ModuleList(\n",
       "     (0): None\n",
       "   )\n",
       "   (output_quantizers): ModuleList(\n",
       "     (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "   )\n",
       " ),\n",
       " 'backbone.stage4.1.poolings.1.param_quantizers': ModuleDict(),\n",
       " 'backbone.stage4.1.poolings.1.input_quantizers': ModuleList(\n",
       "   (0): None\n",
       " ),\n",
       " 'backbone.stage4.1.poolings.1.output_quantizers': ModuleList(\n",
       "   (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       " ),\n",
       " 'backbone.stage4.1.poolings.1.output_quantizers.0': QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False),\n",
       " 'backbone.stage4.1.poolings.2': FakeQuantizedMaxPool2d(\n",
       "   kernel_size=13, stride=1, padding=6, dilation=1, ceil_mode=False\n",
       "   (param_quantizers): ModuleDict()\n",
       "   (input_quantizers): ModuleList(\n",
       "     (0): None\n",
       "   )\n",
       "   (output_quantizers): ModuleList(\n",
       "     (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "   )\n",
       " ),\n",
       " 'backbone.stage4.1.poolings.2.param_quantizers': ModuleDict(),\n",
       " 'backbone.stage4.1.poolings.2.input_quantizers': ModuleList(\n",
       "   (0): None\n",
       " ),\n",
       " 'backbone.stage4.1.poolings.2.output_quantizers': ModuleList(\n",
       "   (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       " ),\n",
       " 'backbone.stage4.1.poolings.2.output_quantizers.0': QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False),\n",
       " 'backbone.stage4.1.conv2': Module(\n",
       "   (conv): QuantizedConv2d(\n",
       "     768, 384, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "     (param_quantizers): ModuleDict(\n",
       "       (weight): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=True)\n",
       "     )\n",
       "     (input_quantizers): ModuleList(\n",
       "       (0): None\n",
       "     )\n",
       "     (output_quantizers): ModuleList(\n",
       "       (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "     )\n",
       "   )\n",
       "   (bn): Module(\n",
       "     (module_batch_norm_26): FakeQuantizedBatchNorm(\n",
       "       (param_quantizers): ModuleDict()\n",
       "       (input_quantizers): ModuleList(\n",
       "         (0-4): 5 x QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "       )\n",
       "       (output_quantizers): ModuleList(\n",
       "         (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "       )\n",
       "     )\n",
       "   )\n",
       "   (activate): CustomSiLU(\n",
       "     (sigmoid): QuantizedSigmoid(\n",
       "       (param_quantizers): ModuleDict()\n",
       "       (input_quantizers): ModuleList(\n",
       "         (0): None\n",
       "       )\n",
       "       (output_quantizers): ModuleList(\n",
       "         (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "       )\n",
       "     )\n",
       "     (mul): QuantizedMultiply(\n",
       "       (param_quantizers): ModuleDict()\n",
       "       (input_quantizers): ModuleList(\n",
       "         (0-1): 2 x None\n",
       "       )\n",
       "       (output_quantizers): ModuleList(\n",
       "         (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "       )\n",
       "     )\n",
       "   )\n",
       " ),\n",
       " 'backbone.stage4.1.conv2.conv': QuantizedConv2d(\n",
       "   768, 384, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "   (param_quantizers): ModuleDict(\n",
       "     (weight): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=True)\n",
       "   )\n",
       "   (input_quantizers): ModuleList(\n",
       "     (0): None\n",
       "   )\n",
       "   (output_quantizers): ModuleList(\n",
       "     (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "   )\n",
       " ),\n",
       " 'backbone.stage4.1.conv2.conv.param_quantizers': ModuleDict(\n",
       "   (weight): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=True)\n",
       " ),\n",
       " 'backbone.stage4.1.conv2.conv.param_quantizers.weight': QuantizeDequantize(shape=[1], bitwidth=8, symmetric=True),\n",
       " 'backbone.stage4.1.conv2.conv.input_quantizers': ModuleList(\n",
       "   (0): None\n",
       " ),\n",
       " 'backbone.stage4.1.conv2.conv.output_quantizers': ModuleList(\n",
       "   (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       " ),\n",
       " 'backbone.stage4.1.conv2.conv.output_quantizers.0': QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False),\n",
       " 'backbone.stage4.1.conv2.bn': Module(\n",
       "   (module_batch_norm_26): FakeQuantizedBatchNorm(\n",
       "     (param_quantizers): ModuleDict()\n",
       "     (input_quantizers): ModuleList(\n",
       "       (0-4): 5 x QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "     )\n",
       "     (output_quantizers): ModuleList(\n",
       "       (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "     )\n",
       "   )\n",
       " ),\n",
       " 'backbone.stage4.1.conv2.bn.module_batch_norm_26': FakeQuantizedBatchNorm(\n",
       "   (param_quantizers): ModuleDict()\n",
       "   (input_quantizers): ModuleList(\n",
       "     (0-4): 5 x QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "   )\n",
       "   (output_quantizers): ModuleList(\n",
       "     (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "   )\n",
       " ),\n",
       " 'backbone.stage4.1.conv2.bn.module_batch_norm_26.param_quantizers': ModuleDict(),\n",
       " 'backbone.stage4.1.conv2.bn.module_batch_norm_26.input_quantizers': ModuleList(\n",
       "   (0-4): 5 x QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       " ),\n",
       " 'backbone.stage4.1.conv2.bn.module_batch_norm_26.input_quantizers.0': QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False),\n",
       " 'backbone.stage4.1.conv2.bn.module_batch_norm_26.input_quantizers.1': QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False),\n",
       " 'backbone.stage4.1.conv2.bn.module_batch_norm_26.input_quantizers.2': QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False),\n",
       " 'backbone.stage4.1.conv2.bn.module_batch_norm_26.input_quantizers.3': QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False),\n",
       " 'backbone.stage4.1.conv2.bn.module_batch_norm_26.input_quantizers.4': QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False),\n",
       " 'backbone.stage4.1.conv2.bn.module_batch_norm_26.output_quantizers': ModuleList(\n",
       "   (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       " ),\n",
       " 'backbone.stage4.1.conv2.bn.module_batch_norm_26.output_quantizers.0': QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False),\n",
       " 'backbone.stage4.1.conv2.activate': CustomSiLU(\n",
       "   (sigmoid): QuantizedSigmoid(\n",
       "     (param_quantizers): ModuleDict()\n",
       "     (input_quantizers): ModuleList(\n",
       "       (0): None\n",
       "     )\n",
       "     (output_quantizers): ModuleList(\n",
       "       (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "     )\n",
       "   )\n",
       "   (mul): QuantizedMultiply(\n",
       "     (param_quantizers): ModuleDict()\n",
       "     (input_quantizers): ModuleList(\n",
       "       (0-1): 2 x None\n",
       "     )\n",
       "     (output_quantizers): ModuleList(\n",
       "       (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "     )\n",
       "   )\n",
       " ),\n",
       " 'backbone.stage4.1.conv2.activate.sigmoid': QuantizedSigmoid(\n",
       "   (param_quantizers): ModuleDict()\n",
       "   (input_quantizers): ModuleList(\n",
       "     (0): None\n",
       "   )\n",
       "   (output_quantizers): ModuleList(\n",
       "     (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "   )\n",
       " ),\n",
       " 'backbone.stage4.1.conv2.activate.sigmoid.param_quantizers': ModuleDict(),\n",
       " 'backbone.stage4.1.conv2.activate.sigmoid.input_quantizers': ModuleList(\n",
       "   (0): None\n",
       " ),\n",
       " 'backbone.stage4.1.conv2.activate.sigmoid.output_quantizers': ModuleList(\n",
       "   (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       " ),\n",
       " 'backbone.stage4.1.conv2.activate.sigmoid.output_quantizers.0': QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False),\n",
       " 'backbone.stage4.1.conv2.activate.mul': QuantizedMultiply(\n",
       "   (param_quantizers): ModuleDict()\n",
       "   (input_quantizers): ModuleList(\n",
       "     (0-1): 2 x None\n",
       "   )\n",
       "   (output_quantizers): ModuleList(\n",
       "     (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "   )\n",
       " ),\n",
       " 'backbone.stage4.1.conv2.activate.mul.param_quantizers': ModuleDict(),\n",
       " 'backbone.stage4.1.conv2.activate.mul.input_quantizers': ModuleList(\n",
       "   (0-1): 2 x None\n",
       " ),\n",
       " 'backbone.stage4.1.conv2.activate.mul.output_quantizers': ModuleList(\n",
       "   (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       " ),\n",
       " 'backbone.stage4.1.conv2.activate.mul.output_quantizers.0': QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False),\n",
       " 'backbone.stage4.1.module_cat_3': FakeQuantizedConcat(\n",
       "   (param_quantizers): ModuleDict()\n",
       "   (input_quantizers): ModuleList(\n",
       "     (0): None\n",
       "   )\n",
       "   (output_quantizers): ModuleList(\n",
       "     (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "   )\n",
       " ),\n",
       " 'backbone.stage4.1.module_cat_3.param_quantizers': ModuleDict(),\n",
       " 'backbone.stage4.1.module_cat_3.input_quantizers': ModuleList(\n",
       "   (0): None\n",
       " ),\n",
       " 'backbone.stage4.1.module_cat_3.output_quantizers': ModuleList(\n",
       "   (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       " ),\n",
       " 'backbone.stage4.1.module_cat_3.output_quantizers.0': QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False),\n",
       " 'backbone.stage4.2': Module(\n",
       "   (short_conv): Module(\n",
       "     (conv): QuantizedConv2d(\n",
       "       384, 192, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "       (param_quantizers): ModuleDict(\n",
       "         (weight): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=True)\n",
       "       )\n",
       "       (input_quantizers): ModuleList(\n",
       "         (0): None\n",
       "       )\n",
       "       (output_quantizers): ModuleList(\n",
       "         (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "       )\n",
       "     )\n",
       "     (bn): Module(\n",
       "       (module_batch_norm_27): FakeQuantizedBatchNorm(\n",
       "         (param_quantizers): ModuleDict()\n",
       "         (input_quantizers): ModuleList(\n",
       "           (0-4): 5 x QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "         )\n",
       "         (output_quantizers): ModuleList(\n",
       "           (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "         )\n",
       "       )\n",
       "     )\n",
       "     (activate): CustomSiLU(\n",
       "       (sigmoid): QuantizedSigmoid(\n",
       "         (param_quantizers): ModuleDict()\n",
       "         (input_quantizers): ModuleList(\n",
       "           (0): None\n",
       "         )\n",
       "         (output_quantizers): ModuleList(\n",
       "           (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "         )\n",
       "       )\n",
       "       (mul): QuantizedMultiply(\n",
       "         (param_quantizers): ModuleDict()\n",
       "         (input_quantizers): ModuleList(\n",
       "           (0-1): 2 x None\n",
       "         )\n",
       "         (output_quantizers): ModuleList(\n",
       "           (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "         )\n",
       "       )\n",
       "     )\n",
       "   )\n",
       "   (main_conv): Module(\n",
       "     (conv): QuantizedConv2d(\n",
       "       384, 192, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "       (param_quantizers): ModuleDict(\n",
       "         (weight): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=True)\n",
       "       )\n",
       "       (input_quantizers): ModuleList(\n",
       "         (0): None\n",
       "       )\n",
       "       (output_quantizers): ModuleList(\n",
       "         (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "       )\n",
       "     )\n",
       "     (bn): Module(\n",
       "       (module_batch_norm_28): FakeQuantizedBatchNorm(\n",
       "         (param_quantizers): ModuleDict()\n",
       "         (input_quantizers): ModuleList(\n",
       "           (0-4): 5 x QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "         )\n",
       "         (output_quantizers): ModuleList(\n",
       "           (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "         )\n",
       "       )\n",
       "     )\n",
       "     (activate): CustomSiLU(\n",
       "       (sigmoid): QuantizedSigmoid(\n",
       "         (param_quantizers): ModuleDict()\n",
       "         (input_quantizers): ModuleList(\n",
       "           (0): None\n",
       "         )\n",
       "         (output_quantizers): ModuleList(\n",
       "           (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "         )\n",
       "       )\n",
       "       (mul): QuantizedMultiply(\n",
       "         (param_quantizers): ModuleDict()\n",
       "         (input_quantizers): ModuleList(\n",
       "           (0-1): 2 x None\n",
       "         )\n",
       "         (output_quantizers): ModuleList(\n",
       "           (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "         )\n",
       "       )\n",
       "     )\n",
       "   )\n",
       "   (blocks): Module(\n",
       "     (0): Module(\n",
       "       (conv1): Module(\n",
       "         (conv): QuantizedConv2d(\n",
       "           192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "           (param_quantizers): ModuleDict(\n",
       "             (weight): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=True)\n",
       "           )\n",
       "           (input_quantizers): ModuleList(\n",
       "             (0): None\n",
       "           )\n",
       "           (output_quantizers): ModuleList(\n",
       "             (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "           )\n",
       "         )\n",
       "         (bn): Module(\n",
       "           (module_batch_norm_29): FakeQuantizedBatchNorm(\n",
       "             (param_quantizers): ModuleDict()\n",
       "             (input_quantizers): ModuleList(\n",
       "               (0-4): 5 x QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "             )\n",
       "             (output_quantizers): ModuleList(\n",
       "               (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "             )\n",
       "           )\n",
       "         )\n",
       "         (activate): CustomSiLU(\n",
       "           (sigmoid): QuantizedSigmoid(\n",
       "             (param_quantizers): ModuleDict()\n",
       "             (input_quantizers): ModuleList(\n",
       "               (0): None\n",
       "             )\n",
       "             (output_quantizers): ModuleList(\n",
       "               (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "             )\n",
       "           )\n",
       "           (mul): QuantizedMultiply(\n",
       "             (param_quantizers): ModuleDict()\n",
       "             (input_quantizers): ModuleList(\n",
       "               (0-1): 2 x None\n",
       "             )\n",
       "             (output_quantizers): ModuleList(\n",
       "               (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "             )\n",
       "           )\n",
       "         )\n",
       "       )\n",
       "       (conv2): Module(\n",
       "         (depthwise_conv): Module(\n",
       "           (conv): QuantizedConv2d(\n",
       "             192, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=192, bias=False\n",
       "             (param_quantizers): ModuleDict(\n",
       "               (weight): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=True)\n",
       "             )\n",
       "             (input_quantizers): ModuleList(\n",
       "               (0): None\n",
       "             )\n",
       "             (output_quantizers): ModuleList(\n",
       "               (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "             )\n",
       "           )\n",
       "           (bn): Module(\n",
       "             (module_batch_norm_30): BatchNorm()\n",
       "           )\n",
       "           (activate): CustomSiLU(\n",
       "             (sigmoid): QuantizedSigmoid(\n",
       "               (param_quantizers): ModuleDict()\n",
       "               (input_quantizers): ModuleList(\n",
       "                 (0): None\n",
       "               )\n",
       "               (output_quantizers): ModuleList(\n",
       "                 (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "               )\n",
       "             )\n",
       "             (mul): QuantizedMultiply(\n",
       "               (param_quantizers): ModuleDict()\n",
       "               (input_quantizers): ModuleList(\n",
       "                 (0-1): 2 x None\n",
       "               )\n",
       "               (output_quantizers): ModuleList(\n",
       "                 (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "               )\n",
       "             )\n",
       "           )\n",
       "         )\n",
       "         (pointwise_conv): Module(\n",
       "           (conv): QuantizedConv2d(\n",
       "             192, 192, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "             (param_quantizers): ModuleDict(\n",
       "               (weight): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=True)\n",
       "             )\n",
       "             (input_quantizers): ModuleList(\n",
       "               (0): None\n",
       "             )\n",
       "             (output_quantizers): ModuleList(\n",
       "               (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "             )\n",
       "           )\n",
       "           (bn): Module(\n",
       "             (module_batch_norm_31): FakeQuantizedBatchNorm(\n",
       "               (param_quantizers): ModuleDict()\n",
       "               (input_quantizers): ModuleList(\n",
       "                 (0-4): 5 x QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "               )\n",
       "               (output_quantizers): ModuleList(\n",
       "                 (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "               )\n",
       "             )\n",
       "           )\n",
       "           (activate): CustomSiLU(\n",
       "             (sigmoid): QuantizedSigmoid(\n",
       "               (param_quantizers): ModuleDict()\n",
       "               (input_quantizers): ModuleList(\n",
       "                 (0): None\n",
       "               )\n",
       "               (output_quantizers): ModuleList(\n",
       "                 (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "               )\n",
       "             )\n",
       "             (mul): QuantizedMultiply(\n",
       "               (param_quantizers): ModuleDict()\n",
       "               (input_quantizers): ModuleList(\n",
       "                 (0-1): 2 x None\n",
       "               )\n",
       "               (output_quantizers): ModuleList(\n",
       "                 (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "               )\n",
       "             )\n",
       "           )\n",
       "         )\n",
       "       )\n",
       "     )\n",
       "   )\n",
       "   (attention): Module(\n",
       "     (global_avgpool): FakeQuantizedAdaptiveAvgPool2d(\n",
       "       output_size=1\n",
       "       (param_quantizers): ModuleDict()\n",
       "       (input_quantizers): ModuleList(\n",
       "         (0): None\n",
       "       )\n",
       "       (output_quantizers): ModuleList(\n",
       "         (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "       )\n",
       "     )\n",
       "     (fc): QuantizedConv2d(\n",
       "       384, 384, kernel_size=(1, 1), stride=(1, 1)\n",
       "       (param_quantizers): ModuleDict(\n",
       "         (weight): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=True)\n",
       "         (bias): None\n",
       "       )\n",
       "       (input_quantizers): ModuleList(\n",
       "         (0): None\n",
       "       )\n",
       "       (output_quantizers): ModuleList(\n",
       "         (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "       )\n",
       "     )\n",
       "     (act): FakeQuantizedHardsigmoid(\n",
       "       (param_quantizers): ModuleDict()\n",
       "       (input_quantizers): ModuleList(\n",
       "         (0): None\n",
       "       )\n",
       "       (output_quantizers): ModuleList(\n",
       "         (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "       )\n",
       "     )\n",
       "     (module_mul_3): QuantizedMultiply(\n",
       "       (param_quantizers): ModuleDict()\n",
       "       (input_quantizers): ModuleList(\n",
       "         (0-1): 2 x None\n",
       "       )\n",
       "       (output_quantizers): ModuleList(\n",
       "         (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "       )\n",
       "     )\n",
       "   )\n",
       "   (final_conv): Module(\n",
       "     (conv): QuantizedConv2d(\n",
       "       384, 384, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "       (param_quantizers): ModuleDict(\n",
       "         (weight): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=True)\n",
       "       )\n",
       "       (input_quantizers): ModuleList(\n",
       "         (0): None\n",
       "       )\n",
       "       (output_quantizers): ModuleList(\n",
       "         (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "       )\n",
       "     )\n",
       "     (bn): Module(\n",
       "       (module_batch_norm_32): FakeQuantizedBatchNorm(\n",
       "         (param_quantizers): ModuleDict()\n",
       "         (input_quantizers): ModuleList(\n",
       "           (0-4): 5 x QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "         )\n",
       "         (output_quantizers): ModuleList(\n",
       "           (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "         )\n",
       "       )\n",
       "     )\n",
       "     (activate): CustomSiLU(\n",
       "       (sigmoid): QuantizedSigmoid(\n",
       "         (param_quantizers): ModuleDict()\n",
       "         (input_quantizers): ModuleList(\n",
       "           (0): None\n",
       "         )\n",
       "         (output_quantizers): ModuleList(\n",
       "           (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "         )\n",
       "       )\n",
       "       (mul): QuantizedMultiply(\n",
       "         (param_quantizers): ModuleDict()\n",
       "         (input_quantizers): ModuleList(\n",
       "           (0-1): 2 x None\n",
       "         )\n",
       "         (output_quantizers): ModuleList(\n",
       "           (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "         )\n",
       "       )\n",
       "     )\n",
       "   )\n",
       "   (module_cat_4): FakeQuantizedConcat(\n",
       "     (param_quantizers): ModuleDict()\n",
       "     (input_quantizers): ModuleList(\n",
       "       (0): None\n",
       "     )\n",
       "     (output_quantizers): ModuleList(\n",
       "       (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "     )\n",
       "   )\n",
       " ),\n",
       " 'backbone.stage4.2.short_conv': Module(\n",
       "   (conv): QuantizedConv2d(\n",
       "     384, 192, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "     (param_quantizers): ModuleDict(\n",
       "       (weight): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=True)\n",
       "     )\n",
       "     (input_quantizers): ModuleList(\n",
       "       (0): None\n",
       "     )\n",
       "     (output_quantizers): ModuleList(\n",
       "       (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "     )\n",
       "   )\n",
       "   (bn): Module(\n",
       "     (module_batch_norm_27): FakeQuantizedBatchNorm(\n",
       "       (param_quantizers): ModuleDict()\n",
       "       (input_quantizers): ModuleList(\n",
       "         (0-4): 5 x QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "       )\n",
       "       (output_quantizers): ModuleList(\n",
       "         (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "       )\n",
       "     )\n",
       "   )\n",
       "   (activate): CustomSiLU(\n",
       "     (sigmoid): QuantizedSigmoid(\n",
       "       (param_quantizers): ModuleDict()\n",
       "       (input_quantizers): ModuleList(\n",
       "         (0): None\n",
       "       )\n",
       "       (output_quantizers): ModuleList(\n",
       "         (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "       )\n",
       "     )\n",
       "     (mul): QuantizedMultiply(\n",
       "       (param_quantizers): ModuleDict()\n",
       "       (input_quantizers): ModuleList(\n",
       "         (0-1): 2 x None\n",
       "       )\n",
       "       (output_quantizers): ModuleList(\n",
       "         (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "       )\n",
       "     )\n",
       "   )\n",
       " ),\n",
       " 'backbone.stage4.2.short_conv.conv': QuantizedConv2d(\n",
       "   384, 192, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "   (param_quantizers): ModuleDict(\n",
       "     (weight): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=True)\n",
       "   )\n",
       "   (input_quantizers): ModuleList(\n",
       "     (0): None\n",
       "   )\n",
       "   (output_quantizers): ModuleList(\n",
       "     (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "   )\n",
       " ),\n",
       " 'backbone.stage4.2.short_conv.conv.param_quantizers': ModuleDict(\n",
       "   (weight): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=True)\n",
       " ),\n",
       " 'backbone.stage4.2.short_conv.conv.param_quantizers.weight': QuantizeDequantize(shape=[1], bitwidth=8, symmetric=True),\n",
       " 'backbone.stage4.2.short_conv.conv.input_quantizers': ModuleList(\n",
       "   (0): None\n",
       " ),\n",
       " 'backbone.stage4.2.short_conv.conv.output_quantizers': ModuleList(\n",
       "   (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       " ),\n",
       " 'backbone.stage4.2.short_conv.conv.output_quantizers.0': QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False),\n",
       " 'backbone.stage4.2.short_conv.bn': Module(\n",
       "   (module_batch_norm_27): FakeQuantizedBatchNorm(\n",
       "     (param_quantizers): ModuleDict()\n",
       "     (input_quantizers): ModuleList(\n",
       "       (0-4): 5 x QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "     )\n",
       "     (output_quantizers): ModuleList(\n",
       "       (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "     )\n",
       "   )\n",
       " ),\n",
       " 'backbone.stage4.2.short_conv.bn.module_batch_norm_27': FakeQuantizedBatchNorm(\n",
       "   (param_quantizers): ModuleDict()\n",
       "   (input_quantizers): ModuleList(\n",
       "     (0-4): 5 x QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "   )\n",
       "   (output_quantizers): ModuleList(\n",
       "     (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "   )\n",
       " ),\n",
       " 'backbone.stage4.2.short_conv.bn.module_batch_norm_27.param_quantizers': ModuleDict(),\n",
       " 'backbone.stage4.2.short_conv.bn.module_batch_norm_27.input_quantizers': ModuleList(\n",
       "   (0-4): 5 x QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       " ),\n",
       " 'backbone.stage4.2.short_conv.bn.module_batch_norm_27.input_quantizers.0': QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False),\n",
       " 'backbone.stage4.2.short_conv.bn.module_batch_norm_27.input_quantizers.1': QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False),\n",
       " 'backbone.stage4.2.short_conv.bn.module_batch_norm_27.input_quantizers.2': QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False),\n",
       " 'backbone.stage4.2.short_conv.bn.module_batch_norm_27.input_quantizers.3': QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False),\n",
       " 'backbone.stage4.2.short_conv.bn.module_batch_norm_27.input_quantizers.4': QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False),\n",
       " 'backbone.stage4.2.short_conv.bn.module_batch_norm_27.output_quantizers': ModuleList(\n",
       "   (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       " ),\n",
       " 'backbone.stage4.2.short_conv.bn.module_batch_norm_27.output_quantizers.0': QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False),\n",
       " 'backbone.stage4.2.short_conv.activate': CustomSiLU(\n",
       "   (sigmoid): QuantizedSigmoid(\n",
       "     (param_quantizers): ModuleDict()\n",
       "     (input_quantizers): ModuleList(\n",
       "       (0): None\n",
       "     )\n",
       "     (output_quantizers): ModuleList(\n",
       "       (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "     )\n",
       "   )\n",
       "   (mul): QuantizedMultiply(\n",
       "     (param_quantizers): ModuleDict()\n",
       "     (input_quantizers): ModuleList(\n",
       "       (0-1): 2 x None\n",
       "     )\n",
       "     (output_quantizers): ModuleList(\n",
       "       (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "     )\n",
       "   )\n",
       " ),\n",
       " 'backbone.stage4.2.short_conv.activate.sigmoid': QuantizedSigmoid(\n",
       "   (param_quantizers): ModuleDict()\n",
       "   (input_quantizers): ModuleList(\n",
       "     (0): None\n",
       "   )\n",
       "   (output_quantizers): ModuleList(\n",
       "     (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "   )\n",
       " ),\n",
       " 'backbone.stage4.2.short_conv.activate.sigmoid.param_quantizers': ModuleDict(),\n",
       " 'backbone.stage4.2.short_conv.activate.sigmoid.input_quantizers': ModuleList(\n",
       "   (0): None\n",
       " ),\n",
       " 'backbone.stage4.2.short_conv.activate.sigmoid.output_quantizers': ModuleList(\n",
       "   (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       " ),\n",
       " 'backbone.stage4.2.short_conv.activate.sigmoid.output_quantizers.0': QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False),\n",
       " 'backbone.stage4.2.short_conv.activate.mul': QuantizedMultiply(\n",
       "   (param_quantizers): ModuleDict()\n",
       "   (input_quantizers): ModuleList(\n",
       "     (0-1): 2 x None\n",
       "   )\n",
       "   (output_quantizers): ModuleList(\n",
       "     (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "   )\n",
       " ),\n",
       " 'backbone.stage4.2.short_conv.activate.mul.param_quantizers': ModuleDict(),\n",
       " 'backbone.stage4.2.short_conv.activate.mul.input_quantizers': ModuleList(\n",
       "   (0-1): 2 x None\n",
       " ),\n",
       " 'backbone.stage4.2.short_conv.activate.mul.output_quantizers': ModuleList(\n",
       "   (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       " ),\n",
       " 'backbone.stage4.2.short_conv.activate.mul.output_quantizers.0': QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False),\n",
       " 'backbone.stage4.2.main_conv': Module(\n",
       "   (conv): QuantizedConv2d(\n",
       "     384, 192, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "     (param_quantizers): ModuleDict(\n",
       "       (weight): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=True)\n",
       "     )\n",
       "     (input_quantizers): ModuleList(\n",
       "       (0): None\n",
       "     )\n",
       "     (output_quantizers): ModuleList(\n",
       "       (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "     )\n",
       "   )\n",
       "   (bn): Module(\n",
       "     (module_batch_norm_28): FakeQuantizedBatchNorm(\n",
       "       (param_quantizers): ModuleDict()\n",
       "       (input_quantizers): ModuleList(\n",
       "         (0-4): 5 x QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "       )\n",
       "       (output_quantizers): ModuleList(\n",
       "         (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "       )\n",
       "     )\n",
       "   )\n",
       "   (activate): CustomSiLU(\n",
       "     (sigmoid): QuantizedSigmoid(\n",
       "       (param_quantizers): ModuleDict()\n",
       "       (input_quantizers): ModuleList(\n",
       "         (0): None\n",
       "       )\n",
       "       (output_quantizers): ModuleList(\n",
       "         (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "       )\n",
       "     )\n",
       "     (mul): QuantizedMultiply(\n",
       "       (param_quantizers): ModuleDict()\n",
       "       (input_quantizers): ModuleList(\n",
       "         (0-1): 2 x None\n",
       "       )\n",
       "       (output_quantizers): ModuleList(\n",
       "         (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "       )\n",
       "     )\n",
       "   )\n",
       " ),\n",
       " 'backbone.stage4.2.main_conv.conv': QuantizedConv2d(\n",
       "   384, 192, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "   (param_quantizers): ModuleDict(\n",
       "     (weight): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=True)\n",
       "   )\n",
       "   (input_quantizers): ModuleList(\n",
       "     (0): None\n",
       "   )\n",
       "   (output_quantizers): ModuleList(\n",
       "     (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "   )\n",
       " ),\n",
       " 'backbone.stage4.2.main_conv.conv.param_quantizers': ModuleDict(\n",
       "   (weight): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=True)\n",
       " ),\n",
       " 'backbone.stage4.2.main_conv.conv.param_quantizers.weight': QuantizeDequantize(shape=[1], bitwidth=8, symmetric=True),\n",
       " 'backbone.stage4.2.main_conv.conv.input_quantizers': ModuleList(\n",
       "   (0): None\n",
       " ),\n",
       " 'backbone.stage4.2.main_conv.conv.output_quantizers': ModuleList(\n",
       "   (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       " ),\n",
       " 'backbone.stage4.2.main_conv.conv.output_quantizers.0': QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False),\n",
       " 'backbone.stage4.2.main_conv.bn': Module(\n",
       "   (module_batch_norm_28): FakeQuantizedBatchNorm(\n",
       "     (param_quantizers): ModuleDict()\n",
       "     (input_quantizers): ModuleList(\n",
       "       (0-4): 5 x QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "     )\n",
       "     (output_quantizers): ModuleList(\n",
       "       (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "     )\n",
       "   )\n",
       " ),\n",
       " 'backbone.stage4.2.main_conv.bn.module_batch_norm_28': FakeQuantizedBatchNorm(\n",
       "   (param_quantizers): ModuleDict()\n",
       "   (input_quantizers): ModuleList(\n",
       "     (0-4): 5 x QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "   )\n",
       "   (output_quantizers): ModuleList(\n",
       "     (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "   )\n",
       " ),\n",
       " 'backbone.stage4.2.main_conv.bn.module_batch_norm_28.param_quantizers': ModuleDict(),\n",
       " 'backbone.stage4.2.main_conv.bn.module_batch_norm_28.input_quantizers': ModuleList(\n",
       "   (0-4): 5 x QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       " ),\n",
       " 'backbone.stage4.2.main_conv.bn.module_batch_norm_28.input_quantizers.0': QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False),\n",
       " 'backbone.stage4.2.main_conv.bn.module_batch_norm_28.input_quantizers.1': QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False),\n",
       " 'backbone.stage4.2.main_conv.bn.module_batch_norm_28.input_quantizers.2': QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False),\n",
       " 'backbone.stage4.2.main_conv.bn.module_batch_norm_28.input_quantizers.3': QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False),\n",
       " 'backbone.stage4.2.main_conv.bn.module_batch_norm_28.input_quantizers.4': QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False),\n",
       " 'backbone.stage4.2.main_conv.bn.module_batch_norm_28.output_quantizers': ModuleList(\n",
       "   (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       " ),\n",
       " 'backbone.stage4.2.main_conv.bn.module_batch_norm_28.output_quantizers.0': QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False),\n",
       " 'backbone.stage4.2.main_conv.activate': CustomSiLU(\n",
       "   (sigmoid): QuantizedSigmoid(\n",
       "     (param_quantizers): ModuleDict()\n",
       "     (input_quantizers): ModuleList(\n",
       "       (0): None\n",
       "     )\n",
       "     (output_quantizers): ModuleList(\n",
       "       (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "     )\n",
       "   )\n",
       "   (mul): QuantizedMultiply(\n",
       "     (param_quantizers): ModuleDict()\n",
       "     (input_quantizers): ModuleList(\n",
       "       (0-1): 2 x None\n",
       "     )\n",
       "     (output_quantizers): ModuleList(\n",
       "       (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "     )\n",
       "   )\n",
       " ),\n",
       " 'backbone.stage4.2.main_conv.activate.sigmoid': QuantizedSigmoid(\n",
       "   (param_quantizers): ModuleDict()\n",
       "   (input_quantizers): ModuleList(\n",
       "     (0): None\n",
       "   )\n",
       "   (output_quantizers): ModuleList(\n",
       "     (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "   )\n",
       " ),\n",
       " 'backbone.stage4.2.main_conv.activate.sigmoid.param_quantizers': ModuleDict(),\n",
       " 'backbone.stage4.2.main_conv.activate.sigmoid.input_quantizers': ModuleList(\n",
       "   (0): None\n",
       " ),\n",
       " 'backbone.stage4.2.main_conv.activate.sigmoid.output_quantizers': ModuleList(\n",
       "   (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       " ),\n",
       " 'backbone.stage4.2.main_conv.activate.sigmoid.output_quantizers.0': QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False),\n",
       " 'backbone.stage4.2.main_conv.activate.mul': QuantizedMultiply(\n",
       "   (param_quantizers): ModuleDict()\n",
       "   (input_quantizers): ModuleList(\n",
       "     (0-1): 2 x None\n",
       "   )\n",
       "   (output_quantizers): ModuleList(\n",
       "     (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "   )\n",
       " ),\n",
       " 'backbone.stage4.2.main_conv.activate.mul.param_quantizers': ModuleDict(),\n",
       " 'backbone.stage4.2.main_conv.activate.mul.input_quantizers': ModuleList(\n",
       "   (0-1): 2 x None\n",
       " ),\n",
       " 'backbone.stage4.2.main_conv.activate.mul.output_quantizers': ModuleList(\n",
       "   (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       " ),\n",
       " 'backbone.stage4.2.main_conv.activate.mul.output_quantizers.0': QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False),\n",
       " 'backbone.stage4.2.blocks': Module(\n",
       "   (0): Module(\n",
       "     (conv1): Module(\n",
       "       (conv): QuantizedConv2d(\n",
       "         192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "         (param_quantizers): ModuleDict(\n",
       "           (weight): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=True)\n",
       "         )\n",
       "         (input_quantizers): ModuleList(\n",
       "           (0): None\n",
       "         )\n",
       "         (output_quantizers): ModuleList(\n",
       "           (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "         )\n",
       "       )\n",
       "       (bn): Module(\n",
       "         (module_batch_norm_29): FakeQuantizedBatchNorm(\n",
       "           (param_quantizers): ModuleDict()\n",
       "           (input_quantizers): ModuleList(\n",
       "             (0-4): 5 x QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "           )\n",
       "           (output_quantizers): ModuleList(\n",
       "             (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "           )\n",
       "         )\n",
       "       )\n",
       "       (activate): CustomSiLU(\n",
       "         (sigmoid): QuantizedSigmoid(\n",
       "           (param_quantizers): ModuleDict()\n",
       "           (input_quantizers): ModuleList(\n",
       "             (0): None\n",
       "           )\n",
       "           (output_quantizers): ModuleList(\n",
       "             (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "           )\n",
       "         )\n",
       "         (mul): QuantizedMultiply(\n",
       "           (param_quantizers): ModuleDict()\n",
       "           (input_quantizers): ModuleList(\n",
       "             (0-1): 2 x None\n",
       "           )\n",
       "           (output_quantizers): ModuleList(\n",
       "             (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "           )\n",
       "         )\n",
       "       )\n",
       "     )\n",
       "     (conv2): Module(\n",
       "       (depthwise_conv): Module(\n",
       "         (conv): QuantizedConv2d(\n",
       "           192, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=192, bias=False\n",
       "           (param_quantizers): ModuleDict(\n",
       "             (weight): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=True)\n",
       "           )\n",
       "           (input_quantizers): ModuleList(\n",
       "             (0): None\n",
       "           )\n",
       "           (output_quantizers): ModuleList(\n",
       "             (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "           )\n",
       "         )\n",
       "         (bn): Module(\n",
       "           (module_batch_norm_30): BatchNorm()\n",
       "         )\n",
       "         (activate): CustomSiLU(\n",
       "           (sigmoid): QuantizedSigmoid(\n",
       "             (param_quantizers): ModuleDict()\n",
       "             (input_quantizers): ModuleList(\n",
       "               (0): None\n",
       "             )\n",
       "             (output_quantizers): ModuleList(\n",
       "               (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "             )\n",
       "           )\n",
       "           (mul): QuantizedMultiply(\n",
       "             (param_quantizers): ModuleDict()\n",
       "             (input_quantizers): ModuleList(\n",
       "               (0-1): 2 x None\n",
       "             )\n",
       "             (output_quantizers): ModuleList(\n",
       "               (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "             )\n",
       "           )\n",
       "         )\n",
       "       )\n",
       "       (pointwise_conv): Module(\n",
       "         (conv): QuantizedConv2d(\n",
       "           192, 192, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "           (param_quantizers): ModuleDict(\n",
       "             (weight): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=True)\n",
       "           )\n",
       "           (input_quantizers): ModuleList(\n",
       "             (0): None\n",
       "           )\n",
       "           (output_quantizers): ModuleList(\n",
       "             (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "           )\n",
       "         )\n",
       "         (bn): Module(\n",
       "           (module_batch_norm_31): FakeQuantizedBatchNorm(\n",
       "             (param_quantizers): ModuleDict()\n",
       "             (input_quantizers): ModuleList(\n",
       "               (0-4): 5 x QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "             )\n",
       "             (output_quantizers): ModuleList(\n",
       "               (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "             )\n",
       "           )\n",
       "         )\n",
       "         (activate): CustomSiLU(\n",
       "           (sigmoid): QuantizedSigmoid(\n",
       "             (param_quantizers): ModuleDict()\n",
       "             (input_quantizers): ModuleList(\n",
       "               (0): None\n",
       "             )\n",
       "             (output_quantizers): ModuleList(\n",
       "               (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "             )\n",
       "           )\n",
       "           (mul): QuantizedMultiply(\n",
       "             (param_quantizers): ModuleDict()\n",
       "             (input_quantizers): ModuleList(\n",
       "               (0-1): 2 x None\n",
       "             )\n",
       "             (output_quantizers): ModuleList(\n",
       "               (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "             )\n",
       "           )\n",
       "         )\n",
       "       )\n",
       "     )\n",
       "   )\n",
       " ),\n",
       " 'backbone.stage4.2.blocks.0': Module(\n",
       "   (conv1): Module(\n",
       "     (conv): QuantizedConv2d(\n",
       "       192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "       (param_quantizers): ModuleDict(\n",
       "         (weight): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=True)\n",
       "       )\n",
       "       (input_quantizers): ModuleList(\n",
       "         (0): None\n",
       "       )\n",
       "       (output_quantizers): ModuleList(\n",
       "         (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "       )\n",
       "     )\n",
       "     (bn): Module(\n",
       "       (module_batch_norm_29): FakeQuantizedBatchNorm(\n",
       "         (param_quantizers): ModuleDict()\n",
       "         (input_quantizers): ModuleList(\n",
       "           (0-4): 5 x QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "         )\n",
       "         (output_quantizers): ModuleList(\n",
       "           (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "         )\n",
       "       )\n",
       "     )\n",
       "     (activate): CustomSiLU(\n",
       "       (sigmoid): QuantizedSigmoid(\n",
       "         (param_quantizers): ModuleDict()\n",
       "         (input_quantizers): ModuleList(\n",
       "           (0): None\n",
       "         )\n",
       "         (output_quantizers): ModuleList(\n",
       "           (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "         )\n",
       "       )\n",
       "       (mul): QuantizedMultiply(\n",
       "         (param_quantizers): ModuleDict()\n",
       "         (input_quantizers): ModuleList(\n",
       "           (0-1): 2 x None\n",
       "         )\n",
       "         (output_quantizers): ModuleList(\n",
       "           (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "         )\n",
       "       )\n",
       "     )\n",
       "   )\n",
       "   (conv2): Module(\n",
       "     (depthwise_conv): Module(\n",
       "       (conv): QuantizedConv2d(\n",
       "         192, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=192, bias=False\n",
       "         (param_quantizers): ModuleDict(\n",
       "           (weight): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=True)\n",
       "         )\n",
       "         (input_quantizers): ModuleList(\n",
       "           (0): None\n",
       "         )\n",
       "         (output_quantizers): ModuleList(\n",
       "           (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "         )\n",
       "       )\n",
       "       (bn): Module(\n",
       "         (module_batch_norm_30): BatchNorm()\n",
       "       )\n",
       "       (activate): CustomSiLU(\n",
       "         (sigmoid): QuantizedSigmoid(\n",
       "           (param_quantizers): ModuleDict()\n",
       "           (input_quantizers): ModuleList(\n",
       "             (0): None\n",
       "           )\n",
       "           (output_quantizers): ModuleList(\n",
       "             (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "           )\n",
       "         )\n",
       "         (mul): QuantizedMultiply(\n",
       "           (param_quantizers): ModuleDict()\n",
       "           (input_quantizers): ModuleList(\n",
       "             (0-1): 2 x None\n",
       "           )\n",
       "           (output_quantizers): ModuleList(\n",
       "             (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "           )\n",
       "         )\n",
       "       )\n",
       "     )\n",
       "     (pointwise_conv): Module(\n",
       "       (conv): QuantizedConv2d(\n",
       "         192, 192, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "         (param_quantizers): ModuleDict(\n",
       "           (weight): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=True)\n",
       "         )\n",
       "         (input_quantizers): ModuleList(\n",
       "           (0): None\n",
       "         )\n",
       "         (output_quantizers): ModuleList(\n",
       "           (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "         )\n",
       "       )\n",
       "       (bn): Module(\n",
       "         (module_batch_norm_31): FakeQuantizedBatchNorm(\n",
       "           (param_quantizers): ModuleDict()\n",
       "           (input_quantizers): ModuleList(\n",
       "             (0-4): 5 x QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "           )\n",
       "           (output_quantizers): ModuleList(\n",
       "             (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "           )\n",
       "         )\n",
       "       )\n",
       "       (activate): CustomSiLU(\n",
       "         (sigmoid): QuantizedSigmoid(\n",
       "           (param_quantizers): ModuleDict()\n",
       "           (input_quantizers): ModuleList(\n",
       "             (0): None\n",
       "           )\n",
       "           (output_quantizers): ModuleList(\n",
       "             (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "           )\n",
       "         )\n",
       "         (mul): QuantizedMultiply(\n",
       "           (param_quantizers): ModuleDict()\n",
       "           (input_quantizers): ModuleList(\n",
       "             (0-1): 2 x None\n",
       "           )\n",
       "           (output_quantizers): ModuleList(\n",
       "             (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "           )\n",
       "         )\n",
       "       )\n",
       "     )\n",
       "   )\n",
       " ),\n",
       " 'backbone.stage4.2.blocks.0.conv1': Module(\n",
       "   (conv): QuantizedConv2d(\n",
       "     192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "     (param_quantizers): ModuleDict(\n",
       "       (weight): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=True)\n",
       "     )\n",
       "     (input_quantizers): ModuleList(\n",
       "       (0): None\n",
       "     )\n",
       "     (output_quantizers): ModuleList(\n",
       "       (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "     )\n",
       "   )\n",
       "   (bn): Module(\n",
       "     (module_batch_norm_29): FakeQuantizedBatchNorm(\n",
       "       (param_quantizers): ModuleDict()\n",
       "       (input_quantizers): ModuleList(\n",
       "         (0-4): 5 x QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "       )\n",
       "       (output_quantizers): ModuleList(\n",
       "         (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "       )\n",
       "     )\n",
       "   )\n",
       "   (activate): CustomSiLU(\n",
       "     (sigmoid): QuantizedSigmoid(\n",
       "       (param_quantizers): ModuleDict()\n",
       "       (input_quantizers): ModuleList(\n",
       "         (0): None\n",
       "       )\n",
       "       (output_quantizers): ModuleList(\n",
       "         (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "       )\n",
       "     )\n",
       "     (mul): QuantizedMultiply(\n",
       "       (param_quantizers): ModuleDict()\n",
       "       (input_quantizers): ModuleList(\n",
       "         (0-1): 2 x None\n",
       "       )\n",
       "       (output_quantizers): ModuleList(\n",
       "         (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "       )\n",
       "     )\n",
       "   )\n",
       " ),\n",
       " 'backbone.stage4.2.blocks.0.conv1.conv': QuantizedConv2d(\n",
       "   192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "   (param_quantizers): ModuleDict(\n",
       "     (weight): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=True)\n",
       "   )\n",
       "   (input_quantizers): ModuleList(\n",
       "     (0): None\n",
       "   )\n",
       "   (output_quantizers): ModuleList(\n",
       "     (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "   )\n",
       " ),\n",
       " 'backbone.stage4.2.blocks.0.conv1.conv.param_quantizers': ModuleDict(\n",
       "   (weight): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=True)\n",
       " ),\n",
       " 'backbone.stage4.2.blocks.0.conv1.conv.param_quantizers.weight': QuantizeDequantize(shape=[1], bitwidth=8, symmetric=True),\n",
       " 'backbone.stage4.2.blocks.0.conv1.conv.input_quantizers': ModuleList(\n",
       "   (0): None\n",
       " ),\n",
       " 'backbone.stage4.2.blocks.0.conv1.conv.output_quantizers': ModuleList(\n",
       "   (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       " ),\n",
       " 'backbone.stage4.2.blocks.0.conv1.conv.output_quantizers.0': QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False),\n",
       " 'backbone.stage4.2.blocks.0.conv1.bn': Module(\n",
       "   (module_batch_norm_29): FakeQuantizedBatchNorm(\n",
       "     (param_quantizers): ModuleDict()\n",
       "     (input_quantizers): ModuleList(\n",
       "       (0-4): 5 x QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "     )\n",
       "     (output_quantizers): ModuleList(\n",
       "       (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "     )\n",
       "   )\n",
       " ),\n",
       " 'backbone.stage4.2.blocks.0.conv1.bn.module_batch_norm_29': FakeQuantizedBatchNorm(\n",
       "   (param_quantizers): ModuleDict()\n",
       "   (input_quantizers): ModuleList(\n",
       "     (0-4): 5 x QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "   )\n",
       "   (output_quantizers): ModuleList(\n",
       "     (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "   )\n",
       " ),\n",
       " 'backbone.stage4.2.blocks.0.conv1.bn.module_batch_norm_29.param_quantizers': ModuleDict(),\n",
       " 'backbone.stage4.2.blocks.0.conv1.bn.module_batch_norm_29.input_quantizers': ModuleList(\n",
       "   (0-4): 5 x QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       " ),\n",
       " 'backbone.stage4.2.blocks.0.conv1.bn.module_batch_norm_29.input_quantizers.0': QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False),\n",
       " 'backbone.stage4.2.blocks.0.conv1.bn.module_batch_norm_29.input_quantizers.1': QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False),\n",
       " 'backbone.stage4.2.blocks.0.conv1.bn.module_batch_norm_29.input_quantizers.2': QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False),\n",
       " 'backbone.stage4.2.blocks.0.conv1.bn.module_batch_norm_29.input_quantizers.3': QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False),\n",
       " 'backbone.stage4.2.blocks.0.conv1.bn.module_batch_norm_29.input_quantizers.4': QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False),\n",
       " 'backbone.stage4.2.blocks.0.conv1.bn.module_batch_norm_29.output_quantizers': ModuleList(\n",
       "   (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       " ),\n",
       " 'backbone.stage4.2.blocks.0.conv1.bn.module_batch_norm_29.output_quantizers.0': QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False),\n",
       " 'backbone.stage4.2.blocks.0.conv1.activate': CustomSiLU(\n",
       "   (sigmoid): QuantizedSigmoid(\n",
       "     (param_quantizers): ModuleDict()\n",
       "     (input_quantizers): ModuleList(\n",
       "       (0): None\n",
       "     )\n",
       "     (output_quantizers): ModuleList(\n",
       "       (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "     )\n",
       "   )\n",
       "   (mul): QuantizedMultiply(\n",
       "     (param_quantizers): ModuleDict()\n",
       "     (input_quantizers): ModuleList(\n",
       "       (0-1): 2 x None\n",
       "     )\n",
       "     (output_quantizers): ModuleList(\n",
       "       (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "     )\n",
       "   )\n",
       " ),\n",
       " 'backbone.stage4.2.blocks.0.conv1.activate.sigmoid': QuantizedSigmoid(\n",
       "   (param_quantizers): ModuleDict()\n",
       "   (input_quantizers): ModuleList(\n",
       "     (0): None\n",
       "   )\n",
       "   (output_quantizers): ModuleList(\n",
       "     (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "   )\n",
       " ),\n",
       " 'backbone.stage4.2.blocks.0.conv1.activate.sigmoid.param_quantizers': ModuleDict(),\n",
       " 'backbone.stage4.2.blocks.0.conv1.activate.sigmoid.input_quantizers': ModuleList(\n",
       "   (0): None\n",
       " ),\n",
       " 'backbone.stage4.2.blocks.0.conv1.activate.sigmoid.output_quantizers': ModuleList(\n",
       "   (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       " ),\n",
       " 'backbone.stage4.2.blocks.0.conv1.activate.sigmoid.output_quantizers.0': QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False),\n",
       " 'backbone.stage4.2.blocks.0.conv1.activate.mul': QuantizedMultiply(\n",
       "   (param_quantizers): ModuleDict()\n",
       "   (input_quantizers): ModuleList(\n",
       "     (0-1): 2 x None\n",
       "   )\n",
       "   (output_quantizers): ModuleList(\n",
       "     (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "   )\n",
       " ),\n",
       " 'backbone.stage4.2.blocks.0.conv1.activate.mul.param_quantizers': ModuleDict(),\n",
       " 'backbone.stage4.2.blocks.0.conv1.activate.mul.input_quantizers': ModuleList(\n",
       "   (0-1): 2 x None\n",
       " ),\n",
       " 'backbone.stage4.2.blocks.0.conv1.activate.mul.output_quantizers': ModuleList(\n",
       "   (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       " ),\n",
       " 'backbone.stage4.2.blocks.0.conv1.activate.mul.output_quantizers.0': QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False),\n",
       " 'backbone.stage4.2.blocks.0.conv2': Module(\n",
       "   (depthwise_conv): Module(\n",
       "     (conv): QuantizedConv2d(\n",
       "       192, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=192, bias=False\n",
       "       (param_quantizers): ModuleDict(\n",
       "         (weight): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=True)\n",
       "       )\n",
       "       (input_quantizers): ModuleList(\n",
       "         (0): None\n",
       "       )\n",
       "       (output_quantizers): ModuleList(\n",
       "         (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "       )\n",
       "     )\n",
       "     (bn): Module(\n",
       "       (module_batch_norm_30): BatchNorm()\n",
       "     )\n",
       "     (activate): CustomSiLU(\n",
       "       (sigmoid): QuantizedSigmoid(\n",
       "         (param_quantizers): ModuleDict()\n",
       "         (input_quantizers): ModuleList(\n",
       "           (0): None\n",
       "         )\n",
       "         (output_quantizers): ModuleList(\n",
       "           (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "         )\n",
       "       )\n",
       "       (mul): QuantizedMultiply(\n",
       "         (param_quantizers): ModuleDict()\n",
       "         (input_quantizers): ModuleList(\n",
       "           (0-1): 2 x None\n",
       "         )\n",
       "         (output_quantizers): ModuleList(\n",
       "           (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "         )\n",
       "       )\n",
       "     )\n",
       "   )\n",
       "   (pointwise_conv): Module(\n",
       "     (conv): QuantizedConv2d(\n",
       "       192, 192, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "       (param_quantizers): ModuleDict(\n",
       "         (weight): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=True)\n",
       "       )\n",
       "       (input_quantizers): ModuleList(\n",
       "         (0): None\n",
       "       )\n",
       "       (output_quantizers): ModuleList(\n",
       "         (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "       )\n",
       "     )\n",
       "     (bn): Module(\n",
       "       (module_batch_norm_31): FakeQuantizedBatchNorm(\n",
       "         (param_quantizers): ModuleDict()\n",
       "         (input_quantizers): ModuleList(\n",
       "           (0-4): 5 x QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "         )\n",
       "         (output_quantizers): ModuleList(\n",
       "           (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "         )\n",
       "       )\n",
       "     )\n",
       "     (activate): CustomSiLU(\n",
       "       (sigmoid): QuantizedSigmoid(\n",
       "         (param_quantizers): ModuleDict()\n",
       "         (input_quantizers): ModuleList(\n",
       "           (0): None\n",
       "         )\n",
       "         (output_quantizers): ModuleList(\n",
       "           (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "         )\n",
       "       )\n",
       "       (mul): QuantizedMultiply(\n",
       "         (param_quantizers): ModuleDict()\n",
       "         (input_quantizers): ModuleList(\n",
       "           (0-1): 2 x None\n",
       "         )\n",
       "         (output_quantizers): ModuleList(\n",
       "           (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "         )\n",
       "       )\n",
       "     )\n",
       "   )\n",
       " ),\n",
       " 'backbone.stage4.2.blocks.0.conv2.depthwise_conv': Module(\n",
       "   (conv): QuantizedConv2d(\n",
       "     192, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=192, bias=False\n",
       "     (param_quantizers): ModuleDict(\n",
       "       (weight): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=True)\n",
       "     )\n",
       "     (input_quantizers): ModuleList(\n",
       "       (0): None\n",
       "     )\n",
       "     (output_quantizers): ModuleList(\n",
       "       (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "     )\n",
       "   )\n",
       "   (bn): Module(\n",
       "     (module_batch_norm_30): BatchNorm()\n",
       "   )\n",
       "   (activate): CustomSiLU(\n",
       "     (sigmoid): QuantizedSigmoid(\n",
       "       (param_quantizers): ModuleDict()\n",
       "       (input_quantizers): ModuleList(\n",
       "         (0): None\n",
       "       )\n",
       "       (output_quantizers): ModuleList(\n",
       "         (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "       )\n",
       "     )\n",
       "     (mul): QuantizedMultiply(\n",
       "       (param_quantizers): ModuleDict()\n",
       "       (input_quantizers): ModuleList(\n",
       "         (0-1): 2 x None\n",
       "       )\n",
       "       (output_quantizers): ModuleList(\n",
       "         (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "       )\n",
       "     )\n",
       "   )\n",
       " ),\n",
       " 'backbone.stage4.2.blocks.0.conv2.depthwise_conv.conv': QuantizedConv2d(\n",
       "   192, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=192, bias=False\n",
       "   (param_quantizers): ModuleDict(\n",
       "     (weight): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=True)\n",
       "   )\n",
       "   (input_quantizers): ModuleList(\n",
       "     (0): None\n",
       "   )\n",
       "   (output_quantizers): ModuleList(\n",
       "     (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "   )\n",
       " ),\n",
       " 'backbone.stage4.2.blocks.0.conv2.depthwise_conv.conv.param_quantizers': ModuleDict(\n",
       "   (weight): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=True)\n",
       " ),\n",
       " 'backbone.stage4.2.blocks.0.conv2.depthwise_conv.conv.param_quantizers.weight': QuantizeDequantize(shape=[1], bitwidth=8, symmetric=True),\n",
       " 'backbone.stage4.2.blocks.0.conv2.depthwise_conv.conv.input_quantizers': ModuleList(\n",
       "   (0): None\n",
       " ),\n",
       " 'backbone.stage4.2.blocks.0.conv2.depthwise_conv.conv.output_quantizers': ModuleList(\n",
       "   (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       " ),\n",
       " 'backbone.stage4.2.blocks.0.conv2.depthwise_conv.conv.output_quantizers.0': QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False),\n",
       " 'backbone.stage4.2.blocks.0.conv2.depthwise_conv.bn': Module(\n",
       "   (module_batch_norm_30): BatchNorm()\n",
       " ),\n",
       " 'backbone.stage4.2.blocks.0.conv2.depthwise_conv.bn.module_batch_norm_30': BatchNorm(),\n",
       " 'backbone.stage4.2.blocks.0.conv2.depthwise_conv.activate': CustomSiLU(\n",
       "   (sigmoid): QuantizedSigmoid(\n",
       "     (param_quantizers): ModuleDict()\n",
       "     (input_quantizers): ModuleList(\n",
       "       (0): None\n",
       "     )\n",
       "     (output_quantizers): ModuleList(\n",
       "       (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "     )\n",
       "   )\n",
       "   (mul): QuantizedMultiply(\n",
       "     (param_quantizers): ModuleDict()\n",
       "     (input_quantizers): ModuleList(\n",
       "       (0-1): 2 x None\n",
       "     )\n",
       "     (output_quantizers): ModuleList(\n",
       "       (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "     )\n",
       "   )\n",
       " ),\n",
       " 'backbone.stage4.2.blocks.0.conv2.depthwise_conv.activate.sigmoid': QuantizedSigmoid(\n",
       "   (param_quantizers): ModuleDict()\n",
       "   (input_quantizers): ModuleList(\n",
       "     (0): None\n",
       "   )\n",
       "   (output_quantizers): ModuleList(\n",
       "     (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "   )\n",
       " ),\n",
       " 'backbone.stage4.2.blocks.0.conv2.depthwise_conv.activate.sigmoid.param_quantizers': ModuleDict(),\n",
       " 'backbone.stage4.2.blocks.0.conv2.depthwise_conv.activate.sigmoid.input_quantizers': ModuleList(\n",
       "   (0): None\n",
       " ),\n",
       " 'backbone.stage4.2.blocks.0.conv2.depthwise_conv.activate.sigmoid.output_quantizers': ModuleList(\n",
       "   (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       " ),\n",
       " 'backbone.stage4.2.blocks.0.conv2.depthwise_conv.activate.sigmoid.output_quantizers.0': QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False),\n",
       " 'backbone.stage4.2.blocks.0.conv2.depthwise_conv.activate.mul': QuantizedMultiply(\n",
       "   (param_quantizers): ModuleDict()\n",
       "   (input_quantizers): ModuleList(\n",
       "     (0-1): 2 x None\n",
       "   )\n",
       "   (output_quantizers): ModuleList(\n",
       "     (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       "   )\n",
       " ),\n",
       " 'backbone.stage4.2.blocks.0.conv2.depthwise_conv.activate.mul.param_quantizers': ModuleDict(),\n",
       " 'backbone.stage4.2.blocks.0.conv2.depthwise_conv.activate.mul.input_quantizers': ModuleList(\n",
       "   (0-1): 2 x None\n",
       " ),\n",
       " 'backbone.stage4.2.blocks.0.conv2.depthwise_conv.activate.mul.output_quantizers': ModuleList(\n",
       "   (0): QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False)\n",
       " ),\n",
       " 'backbone.stage4.2.blocks.0.conv2.depthwise_conv.activate.mul.output_quantizers.0': QuantizeDequantize(shape=[1], bitwidth=8, symmetric=False),\n",
       " ...}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict(sim.model.named_modules())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim.export(path=\"/teamspace/studios/this_studio/aimet/Examples/torch/quantization/sim_model_excluded_modules_embedded_encodings\",\n",
    "           filename_prefix=\"rtm_det\",\n",
    "           dummy_input=dummy_input.cpu(),\n",
    "           use_embedded_encodings=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loads checkpoint by local backend from path: /teamspace/studios/this_studio/mmdetection/rtmdet_tiny_8xb32-300e_coco_20220902_112414-78e30dcc.pth\n",
      "The model and loaded state dict do not match exactly\n",
      "\n",
      "unexpected key in source state_dict: data_preprocessor.mean, data_preprocessor.std\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/mmengine/visualization/visualizer.py:196: UserWarning: Failed to add <class 'mmengine.visualization.vis_backend.LocalVisBackend'>, please provide the `save_dir` argument.\n",
      "  warnings.warn(f'Failed to add {vis_backend.__class__}, '\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from mmdet.apis import DetInferencer\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Resize([640, 640]),  # Resize\n",
    "])\n",
    "\n",
    "DEVICE = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "CONFIG_PATH = '/teamspace/studios/this_studio/mmdetection/rtmdet_tiny_8xb32-300e_coco.py'\n",
    "WEIGHTS_PATH = '/teamspace/studios/this_studio/mmdetection/rtmdet_tiny_8xb32-300e_coco_20220902_112414-78e30dcc.pth'\n",
    "EVAL_DATASET_SIZE = 5000\n",
    "CALIBRATION_DATASET_SIZE = 1000\n",
    "BATCH_SIZE = 80\n",
    "\n",
    "ROOT_DATASET_DIR = '/teamspace/studios/this_studio/COCO'\n",
    "IMAGES_DIR = os.path.join(ROOT_DATASET_DIR, 'images')\n",
    "ANNOTATIONS_JSON_PATH = os.path.join(ROOT_DATASET_DIR, 'annotations/instances_val2017.json')\n",
    "# ANNOTATIONS_JSON_PATH = \"/home/shayaan/Desktop/aimet/my_mmdet/temp.json\"\n",
    "\n",
    "model = DetInferencer(model=CONFIG_PATH, weights=WEIGHTS_PATH, device=DEVICE)\n",
    "evalDataset = CustomImageDataset(images_dir=IMAGES_DIR, annotations_json_path=ANNOTATIONS_JSON_PATH, transform=transform)\n",
    "eval_data_loader = DataLoader(evalDataset, batch_size=BATCH_SIZE)\n",
    "\n",
    "DEVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pass_calibration_data(model: torch.nn.Module, samples: int):\n",
    "    data_loader = eval_data_loader\n",
    "    batch_size = data_loader.batch_size\n",
    "    model.eval()\n",
    "    batch_ctr = 0\n",
    "    with torch.no_grad():\n",
    "        for image_id, image_path, _ in tqdm(data_loader):\n",
    "            pre_processed = collate_preprocessor(inputs=image_path, batch_size=BATCH_SIZE)\n",
    "            _, data = list(pre_processed)[0]\n",
    "            data = preprocessor(data, False)\n",
    "            \n",
    "            preds = model(data['inputs'].to(DEVICE))\n",
    "\n",
    "            batch_ctr += 1\n",
    "            if (batch_ctr * batch_size) > samples:\n",
    "                break  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-08-21 09:32:05,078 - ModelPreparer - INFO - Functional         : Adding new module for node: {backbone.stem.0.bn.module_batch_norm} \n",
      "2024-08-21 09:32:05,079 - ModelPreparer - INFO - Functional         : Adding new module for node: {backbone.stem.1.bn.module_batch_norm_1} \n",
      "2024-08-21 09:32:05,080 - ModelPreparer - INFO - Functional         : Adding new module for node: {backbone.stem.2.bn.module_batch_norm_2} \n",
      "2024-08-21 09:32:05,081 - ModelPreparer - INFO - Functional         : Adding new module for node: {backbone.stage1.0.bn.module_batch_norm_3} \n",
      "2024-08-21 09:32:05,082 - ModelPreparer - INFO - Functional         : Adding new module for node: {backbone.stage1.1.short_conv.bn.module_batch_norm_4} \n",
      "2024-08-21 09:32:05,082 - ModelPreparer - INFO - Functional         : Adding new module for node: {backbone.stage1.1.main_conv.bn.module_batch_norm_5} \n",
      "2024-08-21 09:32:05,083 - ModelPreparer - INFO - Functional         : Adding new module for node: {backbone.stage1.1.blocks.0.conv1.bn.module_batch_norm_6} \n",
      "2024-08-21 09:32:05,084 - ModelPreparer - INFO - Functional         : Adding new module for node: {backbone.stage1.1.blocks.0.conv2.depthwise_conv.bn.module_batch_norm_7} \n",
      "2024-08-21 09:32:05,085 - ModelPreparer - INFO - Functional         : Adding new module for node: {backbone.stage1.1.blocks.0.conv2.pointwise_conv.bn.module_batch_norm_8} \n",
      "2024-08-21 09:32:05,086 - ModelPreparer - INFO - Functional         : Adding new module for node: {backbone.stage1.1.blocks.0.module_add} \n",
      "2024-08-21 09:32:05,087 - ModelPreparer - INFO - Functional         : Adding new module for node: {backbone.stage1.1.module_cat} \n",
      "2024-08-21 09:32:05,088 - ModelPreparer - INFO - Functional         : Adding new module for node: {backbone.stage1.1.attention.module_mul} \n",
      "2024-08-21 09:32:05,089 - ModelPreparer - INFO - Functional         : Adding new module for node: {backbone.stage1.1.final_conv.bn.module_batch_norm_9} \n",
      "2024-08-21 09:32:05,089 - ModelPreparer - INFO - Functional         : Adding new module for node: {backbone.stage2.0.bn.module_batch_norm_10} \n",
      "2024-08-21 09:32:05,090 - ModelPreparer - INFO - Functional         : Adding new module for node: {backbone.stage2.1.short_conv.bn.module_batch_norm_11} \n",
      "2024-08-21 09:32:05,091 - ModelPreparer - INFO - Functional         : Adding new module for node: {backbone.stage2.1.main_conv.bn.module_batch_norm_12} \n",
      "2024-08-21 09:32:05,092 - ModelPreparer - INFO - Functional         : Adding new module for node: {backbone.stage2.1.blocks.0.conv1.bn.module_batch_norm_13} \n",
      "2024-08-21 09:32:05,092 - ModelPreparer - INFO - Functional         : Adding new module for node: {backbone.stage2.1.blocks.0.conv2.depthwise_conv.bn.module_batch_norm_14} \n",
      "2024-08-21 09:32:05,093 - ModelPreparer - INFO - Functional         : Adding new module for node: {backbone.stage2.1.blocks.0.conv2.pointwise_conv.bn.module_batch_norm_15} \n",
      "2024-08-21 09:32:05,094 - ModelPreparer - INFO - Functional         : Adding new module for node: {backbone.stage2.1.blocks.0.module_add_1} \n",
      "2024-08-21 09:32:05,095 - ModelPreparer - INFO - Functional         : Adding new module for node: {backbone.stage2.1.module_cat_1} \n",
      "2024-08-21 09:32:05,096 - ModelPreparer - INFO - Functional         : Adding new module for node: {backbone.stage2.1.attention.module_mul_1} \n",
      "2024-08-21 09:32:05,096 - ModelPreparer - INFO - Functional         : Adding new module for node: {backbone.stage2.1.final_conv.bn.module_batch_norm_16} \n",
      "2024-08-21 09:32:05,097 - ModelPreparer - INFO - Functional         : Adding new module for node: {backbone.stage3.0.bn.module_batch_norm_17} \n",
      "2024-08-21 09:32:05,098 - ModelPreparer - INFO - Functional         : Adding new module for node: {backbone.stage3.1.short_conv.bn.module_batch_norm_18} \n",
      "2024-08-21 09:32:05,099 - ModelPreparer - INFO - Functional         : Adding new module for node: {backbone.stage3.1.main_conv.bn.module_batch_norm_19} \n",
      "2024-08-21 09:32:05,100 - ModelPreparer - INFO - Functional         : Adding new module for node: {backbone.stage3.1.blocks.0.conv1.bn.module_batch_norm_20} \n",
      "2024-08-21 09:32:05,101 - ModelPreparer - INFO - Functional         : Adding new module for node: {backbone.stage3.1.blocks.0.conv2.depthwise_conv.bn.module_batch_norm_21} \n",
      "2024-08-21 09:32:05,101 - ModelPreparer - INFO - Functional         : Adding new module for node: {backbone.stage3.1.blocks.0.conv2.pointwise_conv.bn.module_batch_norm_22} \n",
      "2024-08-21 09:32:05,102 - ModelPreparer - INFO - Functional         : Adding new module for node: {backbone.stage3.1.blocks.0.module_add_2} \n",
      "2024-08-21 09:32:05,103 - ModelPreparer - INFO - Functional         : Adding new module for node: {backbone.stage3.1.module_cat_2} \n",
      "2024-08-21 09:32:05,104 - ModelPreparer - INFO - Functional         : Adding new module for node: {backbone.stage3.1.attention.module_mul_2} \n",
      "2024-08-21 09:32:05,105 - ModelPreparer - INFO - Functional         : Adding new module for node: {backbone.stage3.1.final_conv.bn.module_batch_norm_23} \n",
      "2024-08-21 09:32:05,106 - ModelPreparer - INFO - Functional         : Adding new module for node: {backbone.stage4.0.bn.module_batch_norm_24} \n",
      "2024-08-21 09:32:05,106 - ModelPreparer - INFO - Functional         : Adding new module for node: {backbone.stage4.1.conv1.bn.module_batch_norm_25} \n",
      "2024-08-21 09:32:05,107 - ModelPreparer - INFO - Functional         : Adding new module for node: {backbone.stage4.1.module_cat_3} \n",
      "2024-08-21 09:32:05,108 - ModelPreparer - INFO - Functional         : Adding new module for node: {backbone.stage4.1.conv2.bn.module_batch_norm_26} \n",
      "2024-08-21 09:32:05,109 - ModelPreparer - INFO - Functional         : Adding new module for node: {backbone.stage4.2.short_conv.bn.module_batch_norm_27} \n",
      "2024-08-21 09:32:05,110 - ModelPreparer - INFO - Functional         : Adding new module for node: {backbone.stage4.2.main_conv.bn.module_batch_norm_28} \n",
      "2024-08-21 09:32:05,111 - ModelPreparer - INFO - Functional         : Adding new module for node: {backbone.stage4.2.blocks.0.conv1.bn.module_batch_norm_29} \n",
      "2024-08-21 09:32:05,111 - ModelPreparer - INFO - Functional         : Adding new module for node: {backbone.stage4.2.blocks.0.conv2.depthwise_conv.bn.module_batch_norm_30} \n",
      "2024-08-21 09:32:05,112 - ModelPreparer - INFO - Functional         : Adding new module for node: {backbone.stage4.2.blocks.0.conv2.pointwise_conv.bn.module_batch_norm_31} \n",
      "2024-08-21 09:32:05,113 - ModelPreparer - INFO - Functional         : Adding new module for node: {backbone.stage4.2.module_cat_4} \n",
      "2024-08-21 09:32:05,114 - ModelPreparer - INFO - Functional         : Adding new module for node: {backbone.stage4.2.attention.module_mul_3} \n",
      "2024-08-21 09:32:05,115 - ModelPreparer - INFO - Functional         : Adding new module for node: {backbone.stage4.2.final_conv.bn.module_batch_norm_32} \n",
      "2024-08-21 09:32:05,117 - ModelPreparer - INFO - Functional         : Adding new module for node: {neck.reduce_layers.0.bn.module_batch_norm_33} \n",
      "2024-08-21 09:32:05,118 - ModelPreparer - INFO - Functional         : Adding new module for node: {neck.module_cat_5} \n",
      "2024-08-21 09:32:05,119 - ModelPreparer - INFO - Functional         : Adding new module for node: {neck.top_down_blocks.0.short_conv.bn.module_batch_norm_34} \n",
      "2024-08-21 09:32:05,119 - ModelPreparer - INFO - Functional         : Adding new module for node: {neck.top_down_blocks.0.main_conv.bn.module_batch_norm_35} \n",
      "2024-08-21 09:32:05,121 - ModelPreparer - INFO - Functional         : Adding new module for node: {neck.top_down_blocks.0.blocks.0.conv1.bn.module_batch_norm_36} \n",
      "2024-08-21 09:32:05,121 - ModelPreparer - INFO - Functional         : Adding new module for node: {neck.top_down_blocks.0.blocks.0.conv2.depthwise_conv.bn.module_batch_norm_37} \n",
      "2024-08-21 09:32:05,122 - ModelPreparer - INFO - Functional         : Adding new module for node: {neck.top_down_blocks.0.blocks.0.conv2.pointwise_conv.bn.module_batch_norm_38} \n",
      "2024-08-21 09:32:05,123 - ModelPreparer - INFO - Functional         : Adding new module for node: {neck.top_down_blocks.0.module_cat_6} \n",
      "2024-08-21 09:32:05,124 - ModelPreparer - INFO - Functional         : Adding new module for node: {neck.top_down_blocks.0.final_conv.bn.module_batch_norm_39} \n",
      "2024-08-21 09:32:05,125 - ModelPreparer - INFO - Functional         : Adding new module for node: {neck.reduce_layers.1.bn.module_batch_norm_40} \n",
      "2024-08-21 09:32:05,125 - ModelPreparer - INFO - Reused/Duplicate   : Adding new module for node: {neck.module_upsample_1} \n",
      "2024-08-21 09:32:05,126 - ModelPreparer - INFO - Functional         : Adding new module for node: {neck.module_cat_7} \n",
      "2024-08-21 09:32:05,127 - ModelPreparer - INFO - Functional         : Adding new module for node: {neck.top_down_blocks.1.short_conv.bn.module_batch_norm_41} \n",
      "2024-08-21 09:32:05,128 - ModelPreparer - INFO - Functional         : Adding new module for node: {neck.top_down_blocks.1.main_conv.bn.module_batch_norm_42} \n",
      "2024-08-21 09:32:05,129 - ModelPreparer - INFO - Functional         : Adding new module for node: {neck.top_down_blocks.1.blocks.0.conv1.bn.module_batch_norm_43} \n",
      "2024-08-21 09:32:05,130 - ModelPreparer - INFO - Functional         : Adding new module for node: {neck.top_down_blocks.1.blocks.0.conv2.depthwise_conv.bn.module_batch_norm_44} \n",
      "2024-08-21 09:32:05,130 - ModelPreparer - INFO - Functional         : Adding new module for node: {neck.top_down_blocks.1.blocks.0.conv2.pointwise_conv.bn.module_batch_norm_45} \n",
      "2024-08-21 09:32:05,131 - ModelPreparer - INFO - Functional         : Adding new module for node: {neck.top_down_blocks.1.module_cat_8} \n",
      "2024-08-21 09:32:05,132 - ModelPreparer - INFO - Functional         : Adding new module for node: {neck.top_down_blocks.1.final_conv.bn.module_batch_norm_46} \n",
      "2024-08-21 09:32:05,133 - ModelPreparer - INFO - Functional         : Adding new module for node: {neck.downsamples.0.bn.module_batch_norm_47} \n",
      "2024-08-21 09:32:05,134 - ModelPreparer - INFO - Functional         : Adding new module for node: {neck.module_cat_9} \n",
      "2024-08-21 09:32:05,134 - ModelPreparer - INFO - Functional         : Adding new module for node: {neck.bottom_up_blocks.0.short_conv.bn.module_batch_norm_48} \n",
      "2024-08-21 09:32:05,135 - ModelPreparer - INFO - Functional         : Adding new module for node: {neck.bottom_up_blocks.0.main_conv.bn.module_batch_norm_49} \n",
      "2024-08-21 09:32:05,136 - ModelPreparer - INFO - Functional         : Adding new module for node: {neck.bottom_up_blocks.0.blocks.0.conv1.bn.module_batch_norm_50} \n",
      "2024-08-21 09:32:05,137 - ModelPreparer - INFO - Functional         : Adding new module for node: {neck.bottom_up_blocks.0.blocks.0.conv2.depthwise_conv.bn.module_batch_norm_51} \n",
      "2024-08-21 09:32:05,138 - ModelPreparer - INFO - Functional         : Adding new module for node: {neck.bottom_up_blocks.0.blocks.0.conv2.pointwise_conv.bn.module_batch_norm_52} \n",
      "2024-08-21 09:32:05,139 - ModelPreparer - INFO - Functional         : Adding new module for node: {neck.bottom_up_blocks.0.module_cat_10} \n",
      "2024-08-21 09:32:05,140 - ModelPreparer - INFO - Functional         : Adding new module for node: {neck.bottom_up_blocks.0.final_conv.bn.module_batch_norm_53} \n",
      "2024-08-21 09:32:05,140 - ModelPreparer - INFO - Functional         : Adding new module for node: {neck.downsamples.1.bn.module_batch_norm_54} \n",
      "2024-08-21 09:32:05,141 - ModelPreparer - INFO - Functional         : Adding new module for node: {neck.module_cat_11} \n",
      "2024-08-21 09:32:05,142 - ModelPreparer - INFO - Functional         : Adding new module for node: {neck.bottom_up_blocks.1.short_conv.bn.module_batch_norm_55} \n",
      "2024-08-21 09:32:05,143 - ModelPreparer - INFO - Functional         : Adding new module for node: {neck.bottom_up_blocks.1.main_conv.bn.module_batch_norm_56} \n",
      "2024-08-21 09:32:05,144 - ModelPreparer - INFO - Functional         : Adding new module for node: {neck.bottom_up_blocks.1.blocks.0.conv1.bn.module_batch_norm_57} \n",
      "2024-08-21 09:32:05,145 - ModelPreparer - INFO - Functional         : Adding new module for node: {neck.bottom_up_blocks.1.blocks.0.conv2.depthwise_conv.bn.module_batch_norm_58} \n",
      "2024-08-21 09:32:05,147 - ModelPreparer - INFO - Functional         : Adding new module for node: {neck.bottom_up_blocks.1.blocks.0.conv2.pointwise_conv.bn.module_batch_norm_59} \n",
      "2024-08-21 09:32:05,149 - ModelPreparer - INFO - Functional         : Adding new module for node: {neck.bottom_up_blocks.1.module_cat_12} \n",
      "2024-08-21 09:32:05,150 - ModelPreparer - INFO - Functional         : Adding new module for node: {neck.bottom_up_blocks.1.final_conv.bn.module_batch_norm_60} \n",
      "2024-08-21 09:32:05,150 - ModelPreparer - INFO - Functional         : Adding new module for node: {neck.out_convs.0.bn.module_batch_norm_61} \n",
      "2024-08-21 09:32:05,151 - ModelPreparer - INFO - Functional         : Adding new module for node: {neck.out_convs.1.bn.module_batch_norm_62} \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-08-21 09:32:05,152 - ModelPreparer - INFO - Functional         : Adding new module for node: {neck.out_convs.2.bn.module_batch_norm_63} \n",
      "2024-08-21 09:32:05,155 - ModelPreparer - INFO - Functional         : Adding new module for node: {bbox_head.cls_convs.0.0.bn.module_batch_norm_64} \n",
      "2024-08-21 09:32:05,156 - ModelPreparer - INFO - Functional         : Adding new module for node: {bbox_head.cls_convs.0.1.bn.module_batch_norm_65} \n",
      "2024-08-21 09:32:05,157 - ModelPreparer - INFO - Functional         : Adding new module for node: {bbox_head.reg_convs.0.0.bn.module_batch_norm_66} \n",
      "2024-08-21 09:32:05,158 - ModelPreparer - INFO - Functional         : Adding new module for node: {bbox_head.reg_convs.0.1.bn.module_batch_norm_67} \n",
      "2024-08-21 09:32:05,158 - ModelPreparer - INFO - Functional         : Adding new module for node: {bbox_head.module_mul_4} \n",
      "2024-08-21 09:32:05,159 - ModelPreparer - INFO - Reused/Duplicate   : Adding new module for node: {bbox_head.cls_convs.0.0.module_conv_1} \n",
      "2024-08-21 09:32:05,160 - ModelPreparer - INFO - Functional         : Adding new module for node: {bbox_head.cls_convs.1.0.bn.module_batch_norm_68} \n",
      "2024-08-21 09:32:05,161 - ModelPreparer - INFO - Reused/Duplicate   : Adding new module for node: {bbox_head.cls_convs.0.1.module_conv_1} \n",
      "2024-08-21 09:32:05,162 - ModelPreparer - INFO - Functional         : Adding new module for node: {bbox_head.cls_convs.1.1.bn.module_batch_norm_69} \n",
      "2024-08-21 09:32:05,163 - ModelPreparer - INFO - Reused/Duplicate   : Adding new module for node: {bbox_head.reg_convs.0.0.module_conv_1} \n",
      "2024-08-21 09:32:05,163 - ModelPreparer - INFO - Functional         : Adding new module for node: {bbox_head.reg_convs.1.0.bn.module_batch_norm_70} \n",
      "2024-08-21 09:32:05,164 - ModelPreparer - INFO - Reused/Duplicate   : Adding new module for node: {bbox_head.reg_convs.0.1.module_conv_1} \n",
      "2024-08-21 09:32:05,166 - ModelPreparer - INFO - Functional         : Adding new module for node: {bbox_head.reg_convs.1.1.bn.module_batch_norm_71} \n",
      "2024-08-21 09:32:05,166 - ModelPreparer - INFO - Functional         : Adding new module for node: {bbox_head.module_mul_5} \n",
      "2024-08-21 09:32:05,167 - ModelPreparer - INFO - Reused/Duplicate   : Adding new module for node: {bbox_head.cls_convs.0.0.module_conv_2} \n",
      "2024-08-21 09:32:05,168 - ModelPreparer - INFO - Functional         : Adding new module for node: {bbox_head.cls_convs.2.0.bn.module_batch_norm_72} \n",
      "2024-08-21 09:32:05,169 - ModelPreparer - INFO - Reused/Duplicate   : Adding new module for node: {bbox_head.cls_convs.0.1.module_conv_2} \n",
      "2024-08-21 09:32:05,169 - ModelPreparer - INFO - Functional         : Adding new module for node: {bbox_head.cls_convs.2.1.bn.module_batch_norm_73} \n",
      "2024-08-21 09:32:05,170 - ModelPreparer - INFO - Reused/Duplicate   : Adding new module for node: {bbox_head.reg_convs.0.0.module_conv_2} \n",
      "2024-08-21 09:32:05,171 - ModelPreparer - INFO - Functional         : Adding new module for node: {bbox_head.reg_convs.2.0.bn.module_batch_norm_74} \n",
      "2024-08-21 09:32:05,172 - ModelPreparer - INFO - Reused/Duplicate   : Adding new module for node: {bbox_head.reg_convs.0.1.module_conv_2} \n",
      "2024-08-21 09:32:05,173 - ModelPreparer - INFO - Functional         : Adding new module for node: {bbox_head.reg_convs.2.1.bn.module_batch_norm_75} \n",
      "2024-08-21 09:32:05,173 - ModelPreparer - INFO - Functional         : Adding new module for node: {bbox_head.module_mul_6} \n",
      "2024-08-21 09:32:10,279 - Quant - INFO - No config file provided, defaulting to config file at /usr/local/lib/python3.10/dist-packages/aimet_common/quantsim_config/default_config.json\n",
      "2024-08-21 09:32:10,318 - Quant - INFO - Unsupported op type Squeeze\n",
      "2024-08-21 09:32:10,319 - Quant - INFO - Unsupported op type Mean\n",
      "2024-08-21 09:32:10,337 - Quant - INFO - Selecting DefaultOpInstanceConfigGenerator to compute the specialized config. hw_version:default\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/63 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'collate_preprocessor' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 18\u001b[0m\n\u001b[1;32m     15\u001b[0m quant_sim\u001b[38;5;241m.\u001b[39mload_encodings(encodings\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/teamspace/studios/this_studio/aimet/Examples/torch/quantization/sim_model_excluded_modules/rtm_det_torch.encodings\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m### else compute encodings\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m \u001b[43mquant_sim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_encodings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpass_calibration_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mCALIBRATION_DATASET_SIZE\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/aimet_torch/quantsim.py:445\u001b[0m, in \u001b[0;36mQuantizationSimModel.compute_encodings\u001b[0;34m(self, forward_pass_callback, forward_pass_callback_args)\u001b[0m\n\u001b[1;32m    443\u001b[0m \u001b[38;5;66;03m# Run forward iterations so we can collect statistics to compute the appropriate encodings\u001b[39;00m\n\u001b[1;32m    444\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m utils\u001b[38;5;241m.\u001b[39min_eval_mode(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel), torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 445\u001b[0m     _ \u001b[38;5;241m=\u001b[39m \u001b[43mforward_pass_callback\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforward_pass_callback_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    447\u001b[0m QuantizationSimModel\u001b[38;5;241m.\u001b[39mcompute_layer_encodings_for_sim(\u001b[38;5;28mself\u001b[39m)\n",
      "Cell \u001b[0;32mIn[22], line 8\u001b[0m, in \u001b[0;36mpass_calibration_data\u001b[0;34m(model, samples)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m image_id, image_path, _ \u001b[38;5;129;01min\u001b[39;00m tqdm(data_loader):\n\u001b[0;32m----> 8\u001b[0m         pre_processed \u001b[38;5;241m=\u001b[39m \u001b[43mcollate_preprocessor\u001b[49m(inputs\u001b[38;5;241m=\u001b[39mimage_path, batch_size\u001b[38;5;241m=\u001b[39mBATCH_SIZE)\n\u001b[1;32m      9\u001b[0m         _, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(pre_processed)[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     10\u001b[0m         data \u001b[38;5;241m=\u001b[39m preprocessor(data, \u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'collate_preprocessor' is not defined"
     ]
    }
   ],
   "source": [
    "from aimet_torch.quantsim import QuantizationSimModel\n",
    "from aimet_common.defs import QuantScheme\n",
    "from aimet_torch.model_preparer import prepare_model\n",
    "\n",
    "dummy_input = torch.rand(1, 3, 640, 640).to(DEVICE)  # Shape for each ImageNet sample is (3 channels) x (224 height) x (224 width)\n",
    "model = prepare_model(model.model)\n",
    "quant_sim = QuantizationSimModel(model=model,\n",
    "                                quant_scheme=QuantScheme.post_training_tf_enhanced,\n",
    "                                default_param_bw=8,\n",
    "                                default_output_bw=8,\n",
    "                                config_file=None,\n",
    "                                dummy_input=dummy_input)\n",
    "\n",
    "### if load encodings\n",
    "quant_sim.load_encodings(encodings=\"/teamspace/studios/this_studio/aimet/Examples/torch/quantization/sim_model_excluded_modules/rtm_det_torch.encodings\")\n",
    "\n",
    "### else compute encodings\n",
    "quant_sim.compute_encodings(pass_calibration_data, CALIBRATION_DATASET_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[BatchNorm(),\n",
       " BatchNorm(),\n",
       " Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False),\n",
       " BatchNorm(),\n",
       " BatchNorm(),\n",
       " BatchNorm(),\n",
       " BatchNorm(),\n",
       " BatchNorm(),\n",
       " BatchNorm()]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "module_names = dict(model.named_modules())\n",
    "modules_to_ignore = ['backbone.stage2.1.blocks.0.conv2.depthwise_conv.bn.module_batch_norm_14', 'backbone.stage1.1.blocks.0.conv2.depthwise_conv.bn.module_batch_norm_7', 'backbone.stage2.1.blocks.0.conv2.pointwise_conv.conv', 'backbone.stage3.1.blocks.0.conv2.depthwise_conv.bn.module_batch_norm_21', 'backbone.stage4.2.blocks.0.conv2.depthwise_conv.bn.module_batch_norm_30', 'neck.top_down_blocks.0.blocks.0.conv2.depthwise_conv.bn.module_batch_norm_37', 'neck.top_down_blocks.1.blocks.0.conv2.depthwise_conv.bn.module_batch_norm_44', 'neck.bottom_up_blocks.0.blocks.0.conv2.depthwise_conv.bn.module_batch_norm_51', 'neck.bottom_up_blocks.1.blocks.0.conv2.depthwise_conv.bn.module_batch_norm_58']\n",
    "modules_to_ignore = [module_names[m] for m in modules_to_ignore]\n",
    "\n",
    "modules_to_ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Module(\n",
      "  (conv): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "  (bn): Module(\n",
      "    (module_batch_norm_6): BatchNorm()\n",
      "  )\n",
      "  (activate): CustomSiLU(\n",
      "    (sigmoid): Sigmoid()\n",
      "    (mul): Multiply()\n",
      "  )\n",
      ")\n",
      "[BatchNorm(), BatchNorm(), Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False), BatchNorm(), BatchNorm(), BatchNorm(), BatchNorm(), BatchNorm(), BatchNorm()]\n"
     ]
    }
   ],
   "source": [
    "def exclude_modules_from_quant(model, sim, modules_to_ignore):\n",
    "    name_to_quant_wrapper_dict = {}\n",
    "    for name, module in sim.model.named_modules():\n",
    "        name_to_quant_wrapper_dict[name] = module\n",
    "\n",
    "    module_to_name_dict = {}\n",
    "    for name, module in model.named_modules():\n",
    "        module_to_name_dict[module] = name\n",
    "    print(list(module_to_name_dict.keys())[49])\n",
    "    print(modules_to_ignore)\n",
    "    quant_wrappers_to_ignore = []\n",
    "    for module in modules_to_ignore:\n",
    "        \n",
    "        name = module_to_name_dict[module]\n",
    "        quant_wrapper = name_to_quant_wrapper_dict[name]\n",
    "        quant_wrappers_to_ignore.append(quant_wrapper)\n",
    "\n",
    "    sim.exclude_layers_from_quantization(quant_wrappers_to_ignore)\n",
    "\n",
    "exclude_modules_from_quant(model, quant_sim, modules_to_ignore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GraphModule(\n",
       "  (backbone): Module(\n",
       "    (stem): Module(\n",
       "      (0): Module(\n",
       "        (conv): StaticGridQuantWrapper(\n",
       "          (_module_to_wrap): Conv2d(3, 12, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (bn): Module(\n",
       "          (module_batch_norm): StaticGridQuantWrapper(\n",
       "            (_module_to_wrap): BatchNorm()\n",
       "          )\n",
       "        )\n",
       "        (activate): CustomSiLU(\n",
       "          (sigmoid): StaticGridQuantWrapper(\n",
       "            (_module_to_wrap): Sigmoid()\n",
       "          )\n",
       "          (mul): StaticGridQuantWrapper(\n",
       "            (_module_to_wrap): Multiply()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1): Module(\n",
       "        (conv): StaticGridQuantWrapper(\n",
       "          (_module_to_wrap): Conv2d(12, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (bn): Module(\n",
       "          (module_batch_norm_1): StaticGridQuantWrapper(\n",
       "            (_module_to_wrap): BatchNorm()\n",
       "          )\n",
       "        )\n",
       "        (activate): CustomSiLU(\n",
       "          (sigmoid): StaticGridQuantWrapper(\n",
       "            (_module_to_wrap): Sigmoid()\n",
       "          )\n",
       "          (mul): StaticGridQuantWrapper(\n",
       "            (_module_to_wrap): Multiply()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (2): Module(\n",
       "        (conv): StaticGridQuantWrapper(\n",
       "          (_module_to_wrap): Conv2d(12, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (bn): Module(\n",
       "          (module_batch_norm_2): StaticGridQuantWrapper(\n",
       "            (_module_to_wrap): BatchNorm()\n",
       "          )\n",
       "        )\n",
       "        (activate): CustomSiLU(\n",
       "          (sigmoid): StaticGridQuantWrapper(\n",
       "            (_module_to_wrap): Sigmoid()\n",
       "          )\n",
       "          (mul): StaticGridQuantWrapper(\n",
       "            (_module_to_wrap): Multiply()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (stage1): Module(\n",
       "      (0): Module(\n",
       "        (conv): StaticGridQuantWrapper(\n",
       "          (_module_to_wrap): Conv2d(24, 48, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (bn): Module(\n",
       "          (module_batch_norm_3): StaticGridQuantWrapper(\n",
       "            (_module_to_wrap): BatchNorm()\n",
       "          )\n",
       "        )\n",
       "        (activate): CustomSiLU(\n",
       "          (sigmoid): StaticGridQuantWrapper(\n",
       "            (_module_to_wrap): Sigmoid()\n",
       "          )\n",
       "          (mul): StaticGridQuantWrapper(\n",
       "            (_module_to_wrap): Multiply()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1): Module(\n",
       "        (short_conv): Module(\n",
       "          (conv): StaticGridQuantWrapper(\n",
       "            (_module_to_wrap): Conv2d(48, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          )\n",
       "          (bn): Module(\n",
       "            (module_batch_norm_4): StaticGridQuantWrapper(\n",
       "              (_module_to_wrap): BatchNorm()\n",
       "            )\n",
       "          )\n",
       "          (activate): CustomSiLU(\n",
       "            (sigmoid): StaticGridQuantWrapper(\n",
       "              (_module_to_wrap): Sigmoid()\n",
       "            )\n",
       "            (mul): StaticGridQuantWrapper(\n",
       "              (_module_to_wrap): Multiply()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (main_conv): Module(\n",
       "          (conv): StaticGridQuantWrapper(\n",
       "            (_module_to_wrap): Conv2d(48, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          )\n",
       "          (bn): Module(\n",
       "            (module_batch_norm_5): StaticGridQuantWrapper(\n",
       "              (_module_to_wrap): BatchNorm()\n",
       "            )\n",
       "          )\n",
       "          (activate): CustomSiLU(\n",
       "            (sigmoid): StaticGridQuantWrapper(\n",
       "              (_module_to_wrap): Sigmoid()\n",
       "            )\n",
       "            (mul): StaticGridQuantWrapper(\n",
       "              (_module_to_wrap): Multiply()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (blocks): Module(\n",
       "          (0): Module(\n",
       "            (conv1): Module(\n",
       "              (conv): StaticGridQuantWrapper(\n",
       "                (_module_to_wrap): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              )\n",
       "              (bn): Module(\n",
       "                (module_batch_norm_6): StaticGridQuantWrapper(\n",
       "                  (_module_to_wrap): BatchNorm()\n",
       "                )\n",
       "              )\n",
       "              (activate): CustomSiLU(\n",
       "                (sigmoid): StaticGridQuantWrapper(\n",
       "                  (_module_to_wrap): Sigmoid()\n",
       "                )\n",
       "                (mul): StaticGridQuantWrapper(\n",
       "                  (_module_to_wrap): Multiply()\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (conv2): Module(\n",
       "              (depthwise_conv): Module(\n",
       "                (conv): StaticGridQuantWrapper(\n",
       "                  (_module_to_wrap): Conv2d(24, 24, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=24, bias=False)\n",
       "                )\n",
       "                (bn): Module(\n",
       "                  (module_batch_norm_7): BatchNorm()\n",
       "                )\n",
       "                (activate): CustomSiLU(\n",
       "                  (sigmoid): StaticGridQuantWrapper(\n",
       "                    (_module_to_wrap): Sigmoid()\n",
       "                  )\n",
       "                  (mul): StaticGridQuantWrapper(\n",
       "                    (_module_to_wrap): Multiply()\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (pointwise_conv): Module(\n",
       "                (conv): StaticGridQuantWrapper(\n",
       "                  (_module_to_wrap): Conv2d(24, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                )\n",
       "                (bn): Module(\n",
       "                  (module_batch_norm_8): StaticGridQuantWrapper(\n",
       "                    (_module_to_wrap): BatchNorm()\n",
       "                  )\n",
       "                )\n",
       "                (activate): CustomSiLU(\n",
       "                  (sigmoid): StaticGridQuantWrapper(\n",
       "                    (_module_to_wrap): Sigmoid()\n",
       "                  )\n",
       "                  (mul): StaticGridQuantWrapper(\n",
       "                    (_module_to_wrap): Multiply()\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (module_add): StaticGridQuantWrapper(\n",
       "              (_module_to_wrap): Add()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (attention): Module(\n",
       "          (global_avgpool): StaticGridQuantWrapper(\n",
       "            (_module_to_wrap): AdaptiveAvgPool2d(output_size=1)\n",
       "          )\n",
       "          (fc): StaticGridQuantWrapper(\n",
       "            (_module_to_wrap): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (act): StaticGridQuantWrapper(\n",
       "            (_module_to_wrap): Hardsigmoid()\n",
       "          )\n",
       "          (module_mul): StaticGridQuantWrapper(\n",
       "            (_module_to_wrap): Multiply()\n",
       "          )\n",
       "        )\n",
       "        (final_conv): Module(\n",
       "          (conv): StaticGridQuantWrapper(\n",
       "            (_module_to_wrap): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          )\n",
       "          (bn): Module(\n",
       "            (module_batch_norm_9): StaticGridQuantWrapper(\n",
       "              (_module_to_wrap): BatchNorm()\n",
       "            )\n",
       "          )\n",
       "          (activate): CustomSiLU(\n",
       "            (sigmoid): StaticGridQuantWrapper(\n",
       "              (_module_to_wrap): Sigmoid()\n",
       "            )\n",
       "            (mul): StaticGridQuantWrapper(\n",
       "              (_module_to_wrap): Multiply()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (module_cat): StaticGridQuantWrapper(\n",
       "          (_module_to_wrap): Concat()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (stage2): Module(\n",
       "      (0): Module(\n",
       "        (conv): StaticGridQuantWrapper(\n",
       "          (_module_to_wrap): Conv2d(48, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (bn): Module(\n",
       "          (module_batch_norm_10): StaticGridQuantWrapper(\n",
       "            (_module_to_wrap): BatchNorm()\n",
       "          )\n",
       "        )\n",
       "        (activate): CustomSiLU(\n",
       "          (sigmoid): StaticGridQuantWrapper(\n",
       "            (_module_to_wrap): Sigmoid()\n",
       "          )\n",
       "          (mul): StaticGridQuantWrapper(\n",
       "            (_module_to_wrap): Multiply()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1): Module(\n",
       "        (short_conv): Module(\n",
       "          (conv): StaticGridQuantWrapper(\n",
       "            (_module_to_wrap): Conv2d(96, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          )\n",
       "          (bn): Module(\n",
       "            (module_batch_norm_11): StaticGridQuantWrapper(\n",
       "              (_module_to_wrap): BatchNorm()\n",
       "            )\n",
       "          )\n",
       "          (activate): CustomSiLU(\n",
       "            (sigmoid): StaticGridQuantWrapper(\n",
       "              (_module_to_wrap): Sigmoid()\n",
       "            )\n",
       "            (mul): StaticGridQuantWrapper(\n",
       "              (_module_to_wrap): Multiply()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (main_conv): Module(\n",
       "          (conv): StaticGridQuantWrapper(\n",
       "            (_module_to_wrap): Conv2d(96, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          )\n",
       "          (bn): Module(\n",
       "            (module_batch_norm_12): StaticGridQuantWrapper(\n",
       "              (_module_to_wrap): BatchNorm()\n",
       "            )\n",
       "          )\n",
       "          (activate): CustomSiLU(\n",
       "            (sigmoid): StaticGridQuantWrapper(\n",
       "              (_module_to_wrap): Sigmoid()\n",
       "            )\n",
       "            (mul): StaticGridQuantWrapper(\n",
       "              (_module_to_wrap): Multiply()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (blocks): Module(\n",
       "          (0): Module(\n",
       "            (conv1): Module(\n",
       "              (conv): StaticGridQuantWrapper(\n",
       "                (_module_to_wrap): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              )\n",
       "              (bn): Module(\n",
       "                (module_batch_norm_13): StaticGridQuantWrapper(\n",
       "                  (_module_to_wrap): BatchNorm()\n",
       "                )\n",
       "              )\n",
       "              (activate): CustomSiLU(\n",
       "                (sigmoid): StaticGridQuantWrapper(\n",
       "                  (_module_to_wrap): Sigmoid()\n",
       "                )\n",
       "                (mul): StaticGridQuantWrapper(\n",
       "                  (_module_to_wrap): Multiply()\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (conv2): Module(\n",
       "              (depthwise_conv): Module(\n",
       "                (conv): StaticGridQuantWrapper(\n",
       "                  (_module_to_wrap): Conv2d(48, 48, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=48, bias=False)\n",
       "                )\n",
       "                (bn): Module(\n",
       "                  (module_batch_norm_14): BatchNorm()\n",
       "                )\n",
       "                (activate): CustomSiLU(\n",
       "                  (sigmoid): StaticGridQuantWrapper(\n",
       "                    (_module_to_wrap): Sigmoid()\n",
       "                  )\n",
       "                  (mul): StaticGridQuantWrapper(\n",
       "                    (_module_to_wrap): Multiply()\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (pointwise_conv): Module(\n",
       "                (conv): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                (bn): Module(\n",
       "                  (module_batch_norm_15): StaticGridQuantWrapper(\n",
       "                    (_module_to_wrap): BatchNorm()\n",
       "                  )\n",
       "                )\n",
       "                (activate): CustomSiLU(\n",
       "                  (sigmoid): StaticGridQuantWrapper(\n",
       "                    (_module_to_wrap): Sigmoid()\n",
       "                  )\n",
       "                  (mul): StaticGridQuantWrapper(\n",
       "                    (_module_to_wrap): Multiply()\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (module_add_1): StaticGridQuantWrapper(\n",
       "              (_module_to_wrap): Add()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (attention): Module(\n",
       "          (global_avgpool): StaticGridQuantWrapper(\n",
       "            (_module_to_wrap): AdaptiveAvgPool2d(output_size=1)\n",
       "          )\n",
       "          (fc): StaticGridQuantWrapper(\n",
       "            (_module_to_wrap): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (act): StaticGridQuantWrapper(\n",
       "            (_module_to_wrap): Hardsigmoid()\n",
       "          )\n",
       "          (module_mul_1): StaticGridQuantWrapper(\n",
       "            (_module_to_wrap): Multiply()\n",
       "          )\n",
       "        )\n",
       "        (final_conv): Module(\n",
       "          (conv): StaticGridQuantWrapper(\n",
       "            (_module_to_wrap): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          )\n",
       "          (bn): Module(\n",
       "            (module_batch_norm_16): StaticGridQuantWrapper(\n",
       "              (_module_to_wrap): BatchNorm()\n",
       "            )\n",
       "          )\n",
       "          (activate): CustomSiLU(\n",
       "            (sigmoid): StaticGridQuantWrapper(\n",
       "              (_module_to_wrap): Sigmoid()\n",
       "            )\n",
       "            (mul): StaticGridQuantWrapper(\n",
       "              (_module_to_wrap): Multiply()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (module_cat_1): StaticGridQuantWrapper(\n",
       "          (_module_to_wrap): Concat()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (stage3): Module(\n",
       "      (0): Module(\n",
       "        (conv): StaticGridQuantWrapper(\n",
       "          (_module_to_wrap): Conv2d(96, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (bn): Module(\n",
       "          (module_batch_norm_17): StaticGridQuantWrapper(\n",
       "            (_module_to_wrap): BatchNorm()\n",
       "          )\n",
       "        )\n",
       "        (activate): CustomSiLU(\n",
       "          (sigmoid): StaticGridQuantWrapper(\n",
       "            (_module_to_wrap): Sigmoid()\n",
       "          )\n",
       "          (mul): StaticGridQuantWrapper(\n",
       "            (_module_to_wrap): Multiply()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1): Module(\n",
       "        (short_conv): Module(\n",
       "          (conv): StaticGridQuantWrapper(\n",
       "            (_module_to_wrap): Conv2d(192, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          )\n",
       "          (bn): Module(\n",
       "            (module_batch_norm_18): StaticGridQuantWrapper(\n",
       "              (_module_to_wrap): BatchNorm()\n",
       "            )\n",
       "          )\n",
       "          (activate): CustomSiLU(\n",
       "            (sigmoid): StaticGridQuantWrapper(\n",
       "              (_module_to_wrap): Sigmoid()\n",
       "            )\n",
       "            (mul): StaticGridQuantWrapper(\n",
       "              (_module_to_wrap): Multiply()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (main_conv): Module(\n",
       "          (conv): StaticGridQuantWrapper(\n",
       "            (_module_to_wrap): Conv2d(192, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          )\n",
       "          (bn): Module(\n",
       "            (module_batch_norm_19): StaticGridQuantWrapper(\n",
       "              (_module_to_wrap): BatchNorm()\n",
       "            )\n",
       "          )\n",
       "          (activate): CustomSiLU(\n",
       "            (sigmoid): StaticGridQuantWrapper(\n",
       "              (_module_to_wrap): Sigmoid()\n",
       "            )\n",
       "            (mul): StaticGridQuantWrapper(\n",
       "              (_module_to_wrap): Multiply()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (blocks): Module(\n",
       "          (0): Module(\n",
       "            (conv1): Module(\n",
       "              (conv): StaticGridQuantWrapper(\n",
       "                (_module_to_wrap): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              )\n",
       "              (bn): Module(\n",
       "                (module_batch_norm_20): StaticGridQuantWrapper(\n",
       "                  (_module_to_wrap): BatchNorm()\n",
       "                )\n",
       "              )\n",
       "              (activate): CustomSiLU(\n",
       "                (sigmoid): StaticGridQuantWrapper(\n",
       "                  (_module_to_wrap): Sigmoid()\n",
       "                )\n",
       "                (mul): StaticGridQuantWrapper(\n",
       "                  (_module_to_wrap): Multiply()\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (conv2): Module(\n",
       "              (depthwise_conv): Module(\n",
       "                (conv): StaticGridQuantWrapper(\n",
       "                  (_module_to_wrap): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96, bias=False)\n",
       "                )\n",
       "                (bn): Module(\n",
       "                  (module_batch_norm_21): BatchNorm()\n",
       "                )\n",
       "                (activate): CustomSiLU(\n",
       "                  (sigmoid): StaticGridQuantWrapper(\n",
       "                    (_module_to_wrap): Sigmoid()\n",
       "                  )\n",
       "                  (mul): StaticGridQuantWrapper(\n",
       "                    (_module_to_wrap): Multiply()\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (pointwise_conv): Module(\n",
       "                (conv): StaticGridQuantWrapper(\n",
       "                  (_module_to_wrap): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                )\n",
       "                (bn): Module(\n",
       "                  (module_batch_norm_22): StaticGridQuantWrapper(\n",
       "                    (_module_to_wrap): BatchNorm()\n",
       "                  )\n",
       "                )\n",
       "                (activate): CustomSiLU(\n",
       "                  (sigmoid): StaticGridQuantWrapper(\n",
       "                    (_module_to_wrap): Sigmoid()\n",
       "                  )\n",
       "                  (mul): StaticGridQuantWrapper(\n",
       "                    (_module_to_wrap): Multiply()\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (module_add_2): StaticGridQuantWrapper(\n",
       "              (_module_to_wrap): Add()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (attention): Module(\n",
       "          (global_avgpool): StaticGridQuantWrapper(\n",
       "            (_module_to_wrap): AdaptiveAvgPool2d(output_size=1)\n",
       "          )\n",
       "          (fc): StaticGridQuantWrapper(\n",
       "            (_module_to_wrap): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (act): StaticGridQuantWrapper(\n",
       "            (_module_to_wrap): Hardsigmoid()\n",
       "          )\n",
       "          (module_mul_2): StaticGridQuantWrapper(\n",
       "            (_module_to_wrap): Multiply()\n",
       "          )\n",
       "        )\n",
       "        (final_conv): Module(\n",
       "          (conv): StaticGridQuantWrapper(\n",
       "            (_module_to_wrap): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          )\n",
       "          (bn): Module(\n",
       "            (module_batch_norm_23): StaticGridQuantWrapper(\n",
       "              (_module_to_wrap): BatchNorm()\n",
       "            )\n",
       "          )\n",
       "          (activate): CustomSiLU(\n",
       "            (sigmoid): StaticGridQuantWrapper(\n",
       "              (_module_to_wrap): Sigmoid()\n",
       "            )\n",
       "            (mul): StaticGridQuantWrapper(\n",
       "              (_module_to_wrap): Multiply()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (module_cat_2): StaticGridQuantWrapper(\n",
       "          (_module_to_wrap): Concat()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (stage4): Module(\n",
       "      (0): Module(\n",
       "        (conv): StaticGridQuantWrapper(\n",
       "          (_module_to_wrap): Conv2d(192, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (bn): Module(\n",
       "          (module_batch_norm_24): StaticGridQuantWrapper(\n",
       "            (_module_to_wrap): BatchNorm()\n",
       "          )\n",
       "        )\n",
       "        (activate): CustomSiLU(\n",
       "          (sigmoid): StaticGridQuantWrapper(\n",
       "            (_module_to_wrap): Sigmoid()\n",
       "          )\n",
       "          (mul): StaticGridQuantWrapper(\n",
       "            (_module_to_wrap): Multiply()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1): Module(\n",
       "        (conv1): Module(\n",
       "          (conv): StaticGridQuantWrapper(\n",
       "            (_module_to_wrap): Conv2d(384, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          )\n",
       "          (bn): Module(\n",
       "            (module_batch_norm_25): StaticGridQuantWrapper(\n",
       "              (_module_to_wrap): BatchNorm()\n",
       "            )\n",
       "          )\n",
       "          (activate): CustomSiLU(\n",
       "            (sigmoid): StaticGridQuantWrapper(\n",
       "              (_module_to_wrap): Sigmoid()\n",
       "            )\n",
       "            (mul): StaticGridQuantWrapper(\n",
       "              (_module_to_wrap): Multiply()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (poolings): Module(\n",
       "          (0): StaticGridQuantWrapper(\n",
       "            (_module_to_wrap): MaxPool2d(kernel_size=5, stride=1, padding=2, dilation=1, ceil_mode=False)\n",
       "          )\n",
       "          (1): StaticGridQuantWrapper(\n",
       "            (_module_to_wrap): MaxPool2d(kernel_size=9, stride=1, padding=4, dilation=1, ceil_mode=False)\n",
       "          )\n",
       "          (2): StaticGridQuantWrapper(\n",
       "            (_module_to_wrap): MaxPool2d(kernel_size=13, stride=1, padding=6, dilation=1, ceil_mode=False)\n",
       "          )\n",
       "        )\n",
       "        (conv2): Module(\n",
       "          (conv): StaticGridQuantWrapper(\n",
       "            (_module_to_wrap): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          )\n",
       "          (bn): Module(\n",
       "            (module_batch_norm_26): StaticGridQuantWrapper(\n",
       "              (_module_to_wrap): BatchNorm()\n",
       "            )\n",
       "          )\n",
       "          (activate): CustomSiLU(\n",
       "            (sigmoid): StaticGridQuantWrapper(\n",
       "              (_module_to_wrap): Sigmoid()\n",
       "            )\n",
       "            (mul): StaticGridQuantWrapper(\n",
       "              (_module_to_wrap): Multiply()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (module_cat_3): StaticGridQuantWrapper(\n",
       "          (_module_to_wrap): Concat()\n",
       "        )\n",
       "      )\n",
       "      (2): Module(\n",
       "        (short_conv): Module(\n",
       "          (conv): StaticGridQuantWrapper(\n",
       "            (_module_to_wrap): Conv2d(384, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          )\n",
       "          (bn): Module(\n",
       "            (module_batch_norm_27): StaticGridQuantWrapper(\n",
       "              (_module_to_wrap): BatchNorm()\n",
       "            )\n",
       "          )\n",
       "          (activate): CustomSiLU(\n",
       "            (sigmoid): StaticGridQuantWrapper(\n",
       "              (_module_to_wrap): Sigmoid()\n",
       "            )\n",
       "            (mul): StaticGridQuantWrapper(\n",
       "              (_module_to_wrap): Multiply()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (main_conv): Module(\n",
       "          (conv): StaticGridQuantWrapper(\n",
       "            (_module_to_wrap): Conv2d(384, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          )\n",
       "          (bn): Module(\n",
       "            (module_batch_norm_28): StaticGridQuantWrapper(\n",
       "              (_module_to_wrap): BatchNorm()\n",
       "            )\n",
       "          )\n",
       "          (activate): CustomSiLU(\n",
       "            (sigmoid): StaticGridQuantWrapper(\n",
       "              (_module_to_wrap): Sigmoid()\n",
       "            )\n",
       "            (mul): StaticGridQuantWrapper(\n",
       "              (_module_to_wrap): Multiply()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (blocks): Module(\n",
       "          (0): Module(\n",
       "            (conv1): Module(\n",
       "              (conv): StaticGridQuantWrapper(\n",
       "                (_module_to_wrap): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              )\n",
       "              (bn): Module(\n",
       "                (module_batch_norm_29): StaticGridQuantWrapper(\n",
       "                  (_module_to_wrap): BatchNorm()\n",
       "                )\n",
       "              )\n",
       "              (activate): CustomSiLU(\n",
       "                (sigmoid): StaticGridQuantWrapper(\n",
       "                  (_module_to_wrap): Sigmoid()\n",
       "                )\n",
       "                (mul): StaticGridQuantWrapper(\n",
       "                  (_module_to_wrap): Multiply()\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (conv2): Module(\n",
       "              (depthwise_conv): Module(\n",
       "                (conv): StaticGridQuantWrapper(\n",
       "                  (_module_to_wrap): Conv2d(192, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=192, bias=False)\n",
       "                )\n",
       "                (bn): Module(\n",
       "                  (module_batch_norm_30): BatchNorm()\n",
       "                )\n",
       "                (activate): CustomSiLU(\n",
       "                  (sigmoid): StaticGridQuantWrapper(\n",
       "                    (_module_to_wrap): Sigmoid()\n",
       "                  )\n",
       "                  (mul): StaticGridQuantWrapper(\n",
       "                    (_module_to_wrap): Multiply()\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (pointwise_conv): Module(\n",
       "                (conv): StaticGridQuantWrapper(\n",
       "                  (_module_to_wrap): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                )\n",
       "                (bn): Module(\n",
       "                  (module_batch_norm_31): StaticGridQuantWrapper(\n",
       "                    (_module_to_wrap): BatchNorm()\n",
       "                  )\n",
       "                )\n",
       "                (activate): CustomSiLU(\n",
       "                  (sigmoid): StaticGridQuantWrapper(\n",
       "                    (_module_to_wrap): Sigmoid()\n",
       "                  )\n",
       "                  (mul): StaticGridQuantWrapper(\n",
       "                    (_module_to_wrap): Multiply()\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (attention): Module(\n",
       "          (global_avgpool): StaticGridQuantWrapper(\n",
       "            (_module_to_wrap): AdaptiveAvgPool2d(output_size=1)\n",
       "          )\n",
       "          (fc): StaticGridQuantWrapper(\n",
       "            (_module_to_wrap): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (act): StaticGridQuantWrapper(\n",
       "            (_module_to_wrap): Hardsigmoid()\n",
       "          )\n",
       "          (module_mul_3): StaticGridQuantWrapper(\n",
       "            (_module_to_wrap): Multiply()\n",
       "          )\n",
       "        )\n",
       "        (final_conv): Module(\n",
       "          (conv): StaticGridQuantWrapper(\n",
       "            (_module_to_wrap): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          )\n",
       "          (bn): Module(\n",
       "            (module_batch_norm_32): StaticGridQuantWrapper(\n",
       "              (_module_to_wrap): BatchNorm()\n",
       "            )\n",
       "          )\n",
       "          (activate): CustomSiLU(\n",
       "            (sigmoid): StaticGridQuantWrapper(\n",
       "              (_module_to_wrap): Sigmoid()\n",
       "            )\n",
       "            (mul): StaticGridQuantWrapper(\n",
       "              (_module_to_wrap): Multiply()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (module_cat_4): StaticGridQuantWrapper(\n",
       "          (_module_to_wrap): Concat()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (neck): Module(\n",
       "    (reduce_layers): Module(\n",
       "      (0): Module(\n",
       "        (conv): StaticGridQuantWrapper(\n",
       "          (_module_to_wrap): Conv2d(384, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        )\n",
       "        (bn): Module(\n",
       "          (module_batch_norm_33): StaticGridQuantWrapper(\n",
       "            (_module_to_wrap): BatchNorm()\n",
       "          )\n",
       "        )\n",
       "        (activate): CustomSiLU(\n",
       "          (sigmoid): StaticGridQuantWrapper(\n",
       "            (_module_to_wrap): Sigmoid()\n",
       "          )\n",
       "          (mul): StaticGridQuantWrapper(\n",
       "            (_module_to_wrap): Multiply()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1): Module(\n",
       "        (conv): StaticGridQuantWrapper(\n",
       "          (_module_to_wrap): Conv2d(192, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        )\n",
       "        (bn): Module(\n",
       "          (module_batch_norm_40): StaticGridQuantWrapper(\n",
       "            (_module_to_wrap): BatchNorm()\n",
       "          )\n",
       "        )\n",
       "        (activate): CustomSiLU(\n",
       "          (sigmoid): StaticGridQuantWrapper(\n",
       "            (_module_to_wrap): Sigmoid()\n",
       "          )\n",
       "          (mul): StaticGridQuantWrapper(\n",
       "            (_module_to_wrap): Multiply()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (upsample): StaticGridQuantWrapper(\n",
       "      (_module_to_wrap): Upsample(scale_factor=2.0, mode='nearest')\n",
       "    )\n",
       "    (top_down_blocks): Module(\n",
       "      (0): Module(\n",
       "        (short_conv): Module(\n",
       "          (conv): StaticGridQuantWrapper(\n",
       "            (_module_to_wrap): Conv2d(384, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          )\n",
       "          (bn): Module(\n",
       "            (module_batch_norm_34): StaticGridQuantWrapper(\n",
       "              (_module_to_wrap): BatchNorm()\n",
       "            )\n",
       "          )\n",
       "          (activate): CustomSiLU(\n",
       "            (sigmoid): StaticGridQuantWrapper(\n",
       "              (_module_to_wrap): Sigmoid()\n",
       "            )\n",
       "            (mul): StaticGridQuantWrapper(\n",
       "              (_module_to_wrap): Multiply()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (main_conv): Module(\n",
       "          (conv): StaticGridQuantWrapper(\n",
       "            (_module_to_wrap): Conv2d(384, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          )\n",
       "          (bn): Module(\n",
       "            (module_batch_norm_35): StaticGridQuantWrapper(\n",
       "              (_module_to_wrap): BatchNorm()\n",
       "            )\n",
       "          )\n",
       "          (activate): CustomSiLU(\n",
       "            (sigmoid): StaticGridQuantWrapper(\n",
       "              (_module_to_wrap): Sigmoid()\n",
       "            )\n",
       "            (mul): StaticGridQuantWrapper(\n",
       "              (_module_to_wrap): Multiply()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (blocks): Module(\n",
       "          (0): Module(\n",
       "            (conv1): Module(\n",
       "              (conv): StaticGridQuantWrapper(\n",
       "                (_module_to_wrap): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              )\n",
       "              (bn): Module(\n",
       "                (module_batch_norm_36): StaticGridQuantWrapper(\n",
       "                  (_module_to_wrap): BatchNorm()\n",
       "                )\n",
       "              )\n",
       "              (activate): CustomSiLU(\n",
       "                (sigmoid): StaticGridQuantWrapper(\n",
       "                  (_module_to_wrap): Sigmoid()\n",
       "                )\n",
       "                (mul): StaticGridQuantWrapper(\n",
       "                  (_module_to_wrap): Multiply()\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (conv2): Module(\n",
       "              (depthwise_conv): Module(\n",
       "                (conv): StaticGridQuantWrapper(\n",
       "                  (_module_to_wrap): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96, bias=False)\n",
       "                )\n",
       "                (bn): Module(\n",
       "                  (module_batch_norm_37): BatchNorm()\n",
       "                )\n",
       "                (activate): CustomSiLU(\n",
       "                  (sigmoid): StaticGridQuantWrapper(\n",
       "                    (_module_to_wrap): Sigmoid()\n",
       "                  )\n",
       "                  (mul): StaticGridQuantWrapper(\n",
       "                    (_module_to_wrap): Multiply()\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (pointwise_conv): Module(\n",
       "                (conv): StaticGridQuantWrapper(\n",
       "                  (_module_to_wrap): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                )\n",
       "                (bn): Module(\n",
       "                  (module_batch_norm_38): StaticGridQuantWrapper(\n",
       "                    (_module_to_wrap): BatchNorm()\n",
       "                  )\n",
       "                )\n",
       "                (activate): CustomSiLU(\n",
       "                  (sigmoid): StaticGridQuantWrapper(\n",
       "                    (_module_to_wrap): Sigmoid()\n",
       "                  )\n",
       "                  (mul): StaticGridQuantWrapper(\n",
       "                    (_module_to_wrap): Multiply()\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (final_conv): Module(\n",
       "          (conv): StaticGridQuantWrapper(\n",
       "            (_module_to_wrap): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          )\n",
       "          (bn): Module(\n",
       "            (module_batch_norm_39): StaticGridQuantWrapper(\n",
       "              (_module_to_wrap): BatchNorm()\n",
       "            )\n",
       "          )\n",
       "          (activate): CustomSiLU(\n",
       "            (sigmoid): StaticGridQuantWrapper(\n",
       "              (_module_to_wrap): Sigmoid()\n",
       "            )\n",
       "            (mul): StaticGridQuantWrapper(\n",
       "              (_module_to_wrap): Multiply()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (module_cat_6): StaticGridQuantWrapper(\n",
       "          (_module_to_wrap): Concat()\n",
       "        )\n",
       "      )\n",
       "      (1): Module(\n",
       "        (short_conv): Module(\n",
       "          (conv): StaticGridQuantWrapper(\n",
       "            (_module_to_wrap): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          )\n",
       "          (bn): Module(\n",
       "            (module_batch_norm_41): StaticGridQuantWrapper(\n",
       "              (_module_to_wrap): BatchNorm()\n",
       "            )\n",
       "          )\n",
       "          (activate): CustomSiLU(\n",
       "            (sigmoid): StaticGridQuantWrapper(\n",
       "              (_module_to_wrap): Sigmoid()\n",
       "            )\n",
       "            (mul): StaticGridQuantWrapper(\n",
       "              (_module_to_wrap): Multiply()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (main_conv): Module(\n",
       "          (conv): StaticGridQuantWrapper(\n",
       "            (_module_to_wrap): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          )\n",
       "          (bn): Module(\n",
       "            (module_batch_norm_42): StaticGridQuantWrapper(\n",
       "              (_module_to_wrap): BatchNorm()\n",
       "            )\n",
       "          )\n",
       "          (activate): CustomSiLU(\n",
       "            (sigmoid): StaticGridQuantWrapper(\n",
       "              (_module_to_wrap): Sigmoid()\n",
       "            )\n",
       "            (mul): StaticGridQuantWrapper(\n",
       "              (_module_to_wrap): Multiply()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (blocks): Module(\n",
       "          (0): Module(\n",
       "            (conv1): Module(\n",
       "              (conv): StaticGridQuantWrapper(\n",
       "                (_module_to_wrap): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              )\n",
       "              (bn): Module(\n",
       "                (module_batch_norm_43): StaticGridQuantWrapper(\n",
       "                  (_module_to_wrap): BatchNorm()\n",
       "                )\n",
       "              )\n",
       "              (activate): CustomSiLU(\n",
       "                (sigmoid): StaticGridQuantWrapper(\n",
       "                  (_module_to_wrap): Sigmoid()\n",
       "                )\n",
       "                (mul): StaticGridQuantWrapper(\n",
       "                  (_module_to_wrap): Multiply()\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (conv2): Module(\n",
       "              (depthwise_conv): Module(\n",
       "                (conv): StaticGridQuantWrapper(\n",
       "                  (_module_to_wrap): Conv2d(48, 48, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=48, bias=False)\n",
       "                )\n",
       "                (bn): Module(\n",
       "                  (module_batch_norm_44): BatchNorm()\n",
       "                )\n",
       "                (activate): CustomSiLU(\n",
       "                  (sigmoid): StaticGridQuantWrapper(\n",
       "                    (_module_to_wrap): Sigmoid()\n",
       "                  )\n",
       "                  (mul): StaticGridQuantWrapper(\n",
       "                    (_module_to_wrap): Multiply()\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (pointwise_conv): Module(\n",
       "                (conv): StaticGridQuantWrapper(\n",
       "                  (_module_to_wrap): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                )\n",
       "                (bn): Module(\n",
       "                  (module_batch_norm_45): StaticGridQuantWrapper(\n",
       "                    (_module_to_wrap): BatchNorm()\n",
       "                  )\n",
       "                )\n",
       "                (activate): CustomSiLU(\n",
       "                  (sigmoid): StaticGridQuantWrapper(\n",
       "                    (_module_to_wrap): Sigmoid()\n",
       "                  )\n",
       "                  (mul): StaticGridQuantWrapper(\n",
       "                    (_module_to_wrap): Multiply()\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (final_conv): Module(\n",
       "          (conv): StaticGridQuantWrapper(\n",
       "            (_module_to_wrap): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          )\n",
       "          (bn): Module(\n",
       "            (module_batch_norm_46): StaticGridQuantWrapper(\n",
       "              (_module_to_wrap): BatchNorm()\n",
       "            )\n",
       "          )\n",
       "          (activate): CustomSiLU(\n",
       "            (sigmoid): StaticGridQuantWrapper(\n",
       "              (_module_to_wrap): Sigmoid()\n",
       "            )\n",
       "            (mul): StaticGridQuantWrapper(\n",
       "              (_module_to_wrap): Multiply()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (module_cat_8): StaticGridQuantWrapper(\n",
       "          (_module_to_wrap): Concat()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (downsamples): Module(\n",
       "      (0): Module(\n",
       "        (conv): StaticGridQuantWrapper(\n",
       "          (_module_to_wrap): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (bn): Module(\n",
       "          (module_batch_norm_47): StaticGridQuantWrapper(\n",
       "            (_module_to_wrap): BatchNorm()\n",
       "          )\n",
       "        )\n",
       "        (activate): CustomSiLU(\n",
       "          (sigmoid): StaticGridQuantWrapper(\n",
       "            (_module_to_wrap): Sigmoid()\n",
       "          )\n",
       "          (mul): StaticGridQuantWrapper(\n",
       "            (_module_to_wrap): Multiply()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1): Module(\n",
       "        (conv): StaticGridQuantWrapper(\n",
       "          (_module_to_wrap): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (bn): Module(\n",
       "          (module_batch_norm_54): StaticGridQuantWrapper(\n",
       "            (_module_to_wrap): BatchNorm()\n",
       "          )\n",
       "        )\n",
       "        (activate): CustomSiLU(\n",
       "          (sigmoid): StaticGridQuantWrapper(\n",
       "            (_module_to_wrap): Sigmoid()\n",
       "          )\n",
       "          (mul): StaticGridQuantWrapper(\n",
       "            (_module_to_wrap): Multiply()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (bottom_up_blocks): Module(\n",
       "      (0): Module(\n",
       "        (short_conv): Module(\n",
       "          (conv): StaticGridQuantWrapper(\n",
       "            (_module_to_wrap): Conv2d(192, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          )\n",
       "          (bn): Module(\n",
       "            (module_batch_norm_48): StaticGridQuantWrapper(\n",
       "              (_module_to_wrap): BatchNorm()\n",
       "            )\n",
       "          )\n",
       "          (activate): CustomSiLU(\n",
       "            (sigmoid): StaticGridQuantWrapper(\n",
       "              (_module_to_wrap): Sigmoid()\n",
       "            )\n",
       "            (mul): StaticGridQuantWrapper(\n",
       "              (_module_to_wrap): Multiply()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (main_conv): Module(\n",
       "          (conv): StaticGridQuantWrapper(\n",
       "            (_module_to_wrap): Conv2d(192, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          )\n",
       "          (bn): Module(\n",
       "            (module_batch_norm_49): StaticGridQuantWrapper(\n",
       "              (_module_to_wrap): BatchNorm()\n",
       "            )\n",
       "          )\n",
       "          (activate): CustomSiLU(\n",
       "            (sigmoid): StaticGridQuantWrapper(\n",
       "              (_module_to_wrap): Sigmoid()\n",
       "            )\n",
       "            (mul): StaticGridQuantWrapper(\n",
       "              (_module_to_wrap): Multiply()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (blocks): Module(\n",
       "          (0): Module(\n",
       "            (conv1): Module(\n",
       "              (conv): StaticGridQuantWrapper(\n",
       "                (_module_to_wrap): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              )\n",
       "              (bn): Module(\n",
       "                (module_batch_norm_50): StaticGridQuantWrapper(\n",
       "                  (_module_to_wrap): BatchNorm()\n",
       "                )\n",
       "              )\n",
       "              (activate): CustomSiLU(\n",
       "                (sigmoid): StaticGridQuantWrapper(\n",
       "                  (_module_to_wrap): Sigmoid()\n",
       "                )\n",
       "                (mul): StaticGridQuantWrapper(\n",
       "                  (_module_to_wrap): Multiply()\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (conv2): Module(\n",
       "              (depthwise_conv): Module(\n",
       "                (conv): StaticGridQuantWrapper(\n",
       "                  (_module_to_wrap): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96, bias=False)\n",
       "                )\n",
       "                (bn): Module(\n",
       "                  (module_batch_norm_51): BatchNorm()\n",
       "                )\n",
       "                (activate): CustomSiLU(\n",
       "                  (sigmoid): StaticGridQuantWrapper(\n",
       "                    (_module_to_wrap): Sigmoid()\n",
       "                  )\n",
       "                  (mul): StaticGridQuantWrapper(\n",
       "                    (_module_to_wrap): Multiply()\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (pointwise_conv): Module(\n",
       "                (conv): StaticGridQuantWrapper(\n",
       "                  (_module_to_wrap): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                )\n",
       "                (bn): Module(\n",
       "                  (module_batch_norm_52): StaticGridQuantWrapper(\n",
       "                    (_module_to_wrap): BatchNorm()\n",
       "                  )\n",
       "                )\n",
       "                (activate): CustomSiLU(\n",
       "                  (sigmoid): StaticGridQuantWrapper(\n",
       "                    (_module_to_wrap): Sigmoid()\n",
       "                  )\n",
       "                  (mul): StaticGridQuantWrapper(\n",
       "                    (_module_to_wrap): Multiply()\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (final_conv): Module(\n",
       "          (conv): StaticGridQuantWrapper(\n",
       "            (_module_to_wrap): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          )\n",
       "          (bn): Module(\n",
       "            (module_batch_norm_53): StaticGridQuantWrapper(\n",
       "              (_module_to_wrap): BatchNorm()\n",
       "            )\n",
       "          )\n",
       "          (activate): CustomSiLU(\n",
       "            (sigmoid): StaticGridQuantWrapper(\n",
       "              (_module_to_wrap): Sigmoid()\n",
       "            )\n",
       "            (mul): StaticGridQuantWrapper(\n",
       "              (_module_to_wrap): Multiply()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (module_cat_10): StaticGridQuantWrapper(\n",
       "          (_module_to_wrap): Concat()\n",
       "        )\n",
       "      )\n",
       "      (1): Module(\n",
       "        (short_conv): Module(\n",
       "          (conv): StaticGridQuantWrapper(\n",
       "            (_module_to_wrap): Conv2d(384, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          )\n",
       "          (bn): Module(\n",
       "            (module_batch_norm_55): StaticGridQuantWrapper(\n",
       "              (_module_to_wrap): BatchNorm()\n",
       "            )\n",
       "          )\n",
       "          (activate): CustomSiLU(\n",
       "            (sigmoid): StaticGridQuantWrapper(\n",
       "              (_module_to_wrap): Sigmoid()\n",
       "            )\n",
       "            (mul): StaticGridQuantWrapper(\n",
       "              (_module_to_wrap): Multiply()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (main_conv): Module(\n",
       "          (conv): StaticGridQuantWrapper(\n",
       "            (_module_to_wrap): Conv2d(384, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          )\n",
       "          (bn): Module(\n",
       "            (module_batch_norm_56): StaticGridQuantWrapper(\n",
       "              (_module_to_wrap): BatchNorm()\n",
       "            )\n",
       "          )\n",
       "          (activate): CustomSiLU(\n",
       "            (sigmoid): StaticGridQuantWrapper(\n",
       "              (_module_to_wrap): Sigmoid()\n",
       "            )\n",
       "            (mul): StaticGridQuantWrapper(\n",
       "              (_module_to_wrap): Multiply()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (blocks): Module(\n",
       "          (0): Module(\n",
       "            (conv1): Module(\n",
       "              (conv): StaticGridQuantWrapper(\n",
       "                (_module_to_wrap): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              )\n",
       "              (bn): Module(\n",
       "                (module_batch_norm_57): StaticGridQuantWrapper(\n",
       "                  (_module_to_wrap): BatchNorm()\n",
       "                )\n",
       "              )\n",
       "              (activate): CustomSiLU(\n",
       "                (sigmoid): StaticGridQuantWrapper(\n",
       "                  (_module_to_wrap): Sigmoid()\n",
       "                )\n",
       "                (mul): StaticGridQuantWrapper(\n",
       "                  (_module_to_wrap): Multiply()\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (conv2): Module(\n",
       "              (depthwise_conv): Module(\n",
       "                (conv): StaticGridQuantWrapper(\n",
       "                  (_module_to_wrap): Conv2d(192, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=192, bias=False)\n",
       "                )\n",
       "                (bn): Module(\n",
       "                  (module_batch_norm_58): BatchNorm()\n",
       "                )\n",
       "                (activate): CustomSiLU(\n",
       "                  (sigmoid): StaticGridQuantWrapper(\n",
       "                    (_module_to_wrap): Sigmoid()\n",
       "                  )\n",
       "                  (mul): StaticGridQuantWrapper(\n",
       "                    (_module_to_wrap): Multiply()\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (pointwise_conv): Module(\n",
       "                (conv): StaticGridQuantWrapper(\n",
       "                  (_module_to_wrap): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                )\n",
       "                (bn): Module(\n",
       "                  (module_batch_norm_59): StaticGridQuantWrapper(\n",
       "                    (_module_to_wrap): BatchNorm()\n",
       "                  )\n",
       "                )\n",
       "                (activate): CustomSiLU(\n",
       "                  (sigmoid): StaticGridQuantWrapper(\n",
       "                    (_module_to_wrap): Sigmoid()\n",
       "                  )\n",
       "                  (mul): StaticGridQuantWrapper(\n",
       "                    (_module_to_wrap): Multiply()\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (final_conv): Module(\n",
       "          (conv): StaticGridQuantWrapper(\n",
       "            (_module_to_wrap): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          )\n",
       "          (bn): Module(\n",
       "            (module_batch_norm_60): StaticGridQuantWrapper(\n",
       "              (_module_to_wrap): BatchNorm()\n",
       "            )\n",
       "          )\n",
       "          (activate): CustomSiLU(\n",
       "            (sigmoid): StaticGridQuantWrapper(\n",
       "              (_module_to_wrap): Sigmoid()\n",
       "            )\n",
       "            (mul): StaticGridQuantWrapper(\n",
       "              (_module_to_wrap): Multiply()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (module_cat_12): StaticGridQuantWrapper(\n",
       "          (_module_to_wrap): Concat()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (out_convs): Module(\n",
       "      (0): Module(\n",
       "        (conv): StaticGridQuantWrapper(\n",
       "          (_module_to_wrap): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (bn): Module(\n",
       "          (module_batch_norm_61): StaticGridQuantWrapper(\n",
       "            (_module_to_wrap): BatchNorm()\n",
       "          )\n",
       "        )\n",
       "        (activate): CustomSiLU(\n",
       "          (sigmoid): StaticGridQuantWrapper(\n",
       "            (_module_to_wrap): Sigmoid()\n",
       "          )\n",
       "          (mul): StaticGridQuantWrapper(\n",
       "            (_module_to_wrap): Multiply()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1): Module(\n",
       "        (conv): StaticGridQuantWrapper(\n",
       "          (_module_to_wrap): Conv2d(192, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (bn): Module(\n",
       "          (module_batch_norm_62): StaticGridQuantWrapper(\n",
       "            (_module_to_wrap): BatchNorm()\n",
       "          )\n",
       "        )\n",
       "        (activate): CustomSiLU(\n",
       "          (sigmoid): StaticGridQuantWrapper(\n",
       "            (_module_to_wrap): Sigmoid()\n",
       "          )\n",
       "          (mul): StaticGridQuantWrapper(\n",
       "            (_module_to_wrap): Multiply()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (2): Module(\n",
       "        (conv): StaticGridQuantWrapper(\n",
       "          (_module_to_wrap): Conv2d(384, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (bn): Module(\n",
       "          (module_batch_norm_63): StaticGridQuantWrapper(\n",
       "            (_module_to_wrap): BatchNorm()\n",
       "          )\n",
       "        )\n",
       "        (activate): CustomSiLU(\n",
       "          (sigmoid): StaticGridQuantWrapper(\n",
       "            (_module_to_wrap): Sigmoid()\n",
       "          )\n",
       "          (mul): StaticGridQuantWrapper(\n",
       "            (_module_to_wrap): Multiply()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (module_cat_5): StaticGridQuantWrapper(\n",
       "      (_module_to_wrap): Concat()\n",
       "    )\n",
       "    (module_upsample_1): StaticGridQuantWrapper(\n",
       "      (_module_to_wrap): Upsample(scale_factor=2.0, mode='nearest')\n",
       "    )\n",
       "    (module_cat_7): StaticGridQuantWrapper(\n",
       "      (_module_to_wrap): Concat()\n",
       "    )\n",
       "    (module_cat_9): StaticGridQuantWrapper(\n",
       "      (_module_to_wrap): Concat()\n",
       "    )\n",
       "    (module_cat_11): StaticGridQuantWrapper(\n",
       "      (_module_to_wrap): Concat()\n",
       "    )\n",
       "  )\n",
       "  (bbox_head): Module(\n",
       "    (cls_convs): Module(\n",
       "      (0): Module(\n",
       "        (0): Module(\n",
       "          (conv): StaticGridQuantWrapper(\n",
       "            (_module_to_wrap): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          )\n",
       "          (bn): Module(\n",
       "            (module_batch_norm_64): StaticGridQuantWrapper(\n",
       "              (_module_to_wrap): BatchNorm()\n",
       "            )\n",
       "          )\n",
       "          (activate): CustomSiLU(\n",
       "            (sigmoid): StaticGridQuantWrapper(\n",
       "              (_module_to_wrap): Sigmoid()\n",
       "            )\n",
       "            (mul): StaticGridQuantWrapper(\n",
       "              (_module_to_wrap): Multiply()\n",
       "            )\n",
       "          )\n",
       "          (module_conv_1): StaticGridQuantWrapper(\n",
       "            (_module_to_wrap): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          )\n",
       "          (module_conv_2): StaticGridQuantWrapper(\n",
       "            (_module_to_wrap): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          )\n",
       "        )\n",
       "        (1): Module(\n",
       "          (conv): StaticGridQuantWrapper(\n",
       "            (_module_to_wrap): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          )\n",
       "          (bn): Module(\n",
       "            (module_batch_norm_65): StaticGridQuantWrapper(\n",
       "              (_module_to_wrap): BatchNorm()\n",
       "            )\n",
       "          )\n",
       "          (activate): CustomSiLU(\n",
       "            (sigmoid): StaticGridQuantWrapper(\n",
       "              (_module_to_wrap): Sigmoid()\n",
       "            )\n",
       "            (mul): StaticGridQuantWrapper(\n",
       "              (_module_to_wrap): Multiply()\n",
       "            )\n",
       "          )\n",
       "          (module_conv_1): StaticGridQuantWrapper(\n",
       "            (_module_to_wrap): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          )\n",
       "          (module_conv_2): StaticGridQuantWrapper(\n",
       "            (_module_to_wrap): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1): Module(\n",
       "        (0): Module(\n",
       "          (bn): Module(\n",
       "            (module_batch_norm_68): StaticGridQuantWrapper(\n",
       "              (_module_to_wrap): BatchNorm()\n",
       "            )\n",
       "          )\n",
       "          (activate): CustomSiLU(\n",
       "            (sigmoid): StaticGridQuantWrapper(\n",
       "              (_module_to_wrap): Sigmoid()\n",
       "            )\n",
       "            (mul): StaticGridQuantWrapper(\n",
       "              (_module_to_wrap): Multiply()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (1): Module(\n",
       "          (bn): Module(\n",
       "            (module_batch_norm_69): StaticGridQuantWrapper(\n",
       "              (_module_to_wrap): BatchNorm()\n",
       "            )\n",
       "          )\n",
       "          (activate): CustomSiLU(\n",
       "            (sigmoid): StaticGridQuantWrapper(\n",
       "              (_module_to_wrap): Sigmoid()\n",
       "            )\n",
       "            (mul): StaticGridQuantWrapper(\n",
       "              (_module_to_wrap): Multiply()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (2): Module(\n",
       "        (0): Module(\n",
       "          (bn): Module(\n",
       "            (module_batch_norm_72): StaticGridQuantWrapper(\n",
       "              (_module_to_wrap): BatchNorm()\n",
       "            )\n",
       "          )\n",
       "          (activate): CustomSiLU(\n",
       "            (sigmoid): StaticGridQuantWrapper(\n",
       "              (_module_to_wrap): Sigmoid()\n",
       "            )\n",
       "            (mul): StaticGridQuantWrapper(\n",
       "              (_module_to_wrap): Multiply()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (1): Module(\n",
       "          (bn): Module(\n",
       "            (module_batch_norm_73): StaticGridQuantWrapper(\n",
       "              (_module_to_wrap): BatchNorm()\n",
       "            )\n",
       "          )\n",
       "          (activate): CustomSiLU(\n",
       "            (sigmoid): StaticGridQuantWrapper(\n",
       "              (_module_to_wrap): Sigmoid()\n",
       "            )\n",
       "            (mul): StaticGridQuantWrapper(\n",
       "              (_module_to_wrap): Multiply()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (rtm_cls): Module(\n",
       "      (0): StaticGridQuantWrapper(\n",
       "        (_module_to_wrap): Conv2d(96, 80, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "      (1): StaticGridQuantWrapper(\n",
       "        (_module_to_wrap): Conv2d(96, 80, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "      (2): StaticGridQuantWrapper(\n",
       "        (_module_to_wrap): Conv2d(96, 80, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "    )\n",
       "    (reg_convs): Module(\n",
       "      (0): Module(\n",
       "        (0): Module(\n",
       "          (conv): StaticGridQuantWrapper(\n",
       "            (_module_to_wrap): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          )\n",
       "          (bn): Module(\n",
       "            (module_batch_norm_66): StaticGridQuantWrapper(\n",
       "              (_module_to_wrap): BatchNorm()\n",
       "            )\n",
       "          )\n",
       "          (activate): CustomSiLU(\n",
       "            (sigmoid): StaticGridQuantWrapper(\n",
       "              (_module_to_wrap): Sigmoid()\n",
       "            )\n",
       "            (mul): StaticGridQuantWrapper(\n",
       "              (_module_to_wrap): Multiply()\n",
       "            )\n",
       "          )\n",
       "          (module_conv_1): StaticGridQuantWrapper(\n",
       "            (_module_to_wrap): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          )\n",
       "          (module_conv_2): StaticGridQuantWrapper(\n",
       "            (_module_to_wrap): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          )\n",
       "        )\n",
       "        (1): Module(\n",
       "          (conv): StaticGridQuantWrapper(\n",
       "            (_module_to_wrap): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          )\n",
       "          (bn): Module(\n",
       "            (module_batch_norm_67): StaticGridQuantWrapper(\n",
       "              (_module_to_wrap): BatchNorm()\n",
       "            )\n",
       "          )\n",
       "          (activate): CustomSiLU(\n",
       "            (sigmoid): StaticGridQuantWrapper(\n",
       "              (_module_to_wrap): Sigmoid()\n",
       "            )\n",
       "            (mul): StaticGridQuantWrapper(\n",
       "              (_module_to_wrap): Multiply()\n",
       "            )\n",
       "          )\n",
       "          (module_conv_1): StaticGridQuantWrapper(\n",
       "            (_module_to_wrap): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          )\n",
       "          (module_conv_2): StaticGridQuantWrapper(\n",
       "            (_module_to_wrap): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1): Module(\n",
       "        (0): Module(\n",
       "          (bn): Module(\n",
       "            (module_batch_norm_70): StaticGridQuantWrapper(\n",
       "              (_module_to_wrap): BatchNorm()\n",
       "            )\n",
       "          )\n",
       "          (activate): CustomSiLU(\n",
       "            (sigmoid): StaticGridQuantWrapper(\n",
       "              (_module_to_wrap): Sigmoid()\n",
       "            )\n",
       "            (mul): StaticGridQuantWrapper(\n",
       "              (_module_to_wrap): Multiply()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (1): Module(\n",
       "          (bn): Module(\n",
       "            (module_batch_norm_71): StaticGridQuantWrapper(\n",
       "              (_module_to_wrap): BatchNorm()\n",
       "            )\n",
       "          )\n",
       "          (activate): CustomSiLU(\n",
       "            (sigmoid): StaticGridQuantWrapper(\n",
       "              (_module_to_wrap): Sigmoid()\n",
       "            )\n",
       "            (mul): StaticGridQuantWrapper(\n",
       "              (_module_to_wrap): Multiply()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (2): Module(\n",
       "        (0): Module(\n",
       "          (bn): Module(\n",
       "            (module_batch_norm_74): StaticGridQuantWrapper(\n",
       "              (_module_to_wrap): BatchNorm()\n",
       "            )\n",
       "          )\n",
       "          (activate): CustomSiLU(\n",
       "            (sigmoid): StaticGridQuantWrapper(\n",
       "              (_module_to_wrap): Sigmoid()\n",
       "            )\n",
       "            (mul): StaticGridQuantWrapper(\n",
       "              (_module_to_wrap): Multiply()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (1): Module(\n",
       "          (bn): Module(\n",
       "            (module_batch_norm_75): StaticGridQuantWrapper(\n",
       "              (_module_to_wrap): BatchNorm()\n",
       "            )\n",
       "          )\n",
       "          (activate): CustomSiLU(\n",
       "            (sigmoid): StaticGridQuantWrapper(\n",
       "              (_module_to_wrap): Sigmoid()\n",
       "            )\n",
       "            (mul): StaticGridQuantWrapper(\n",
       "              (_module_to_wrap): Multiply()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (rtm_reg): Module(\n",
       "      (0): StaticGridQuantWrapper(\n",
       "        (_module_to_wrap): Conv2d(96, 4, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "      (1): StaticGridQuantWrapper(\n",
       "        (_module_to_wrap): Conv2d(96, 4, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "      (2): StaticGridQuantWrapper(\n",
       "        (_module_to_wrap): Conv2d(96, 4, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "    )\n",
       "    (module_mul_4): StaticGridQuantWrapper(\n",
       "      (_module_to_wrap): Multiply()\n",
       "    )\n",
       "    (module_mul_5): StaticGridQuantWrapper(\n",
       "      (_module_to_wrap): Multiply()\n",
       "    )\n",
       "    (module_mul_6): StaticGridQuantWrapper(\n",
       "      (_module_to_wrap): Multiply()\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quant_sim.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
